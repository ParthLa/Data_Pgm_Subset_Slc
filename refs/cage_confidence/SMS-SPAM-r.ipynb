{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df=pd.read_csv('./SMSSpamCollection',sep='\\t', header=None,na_filter=False,names=['class','text'])\n",
    "\n",
    "# df['class'] = df[\"class\"].astype('category')\n",
    "\n",
    "# df['true_label'] = df['class'].cat.codes\n",
    "\n",
    "# np.random.seed(1234)\n",
    "# msk = np.random.rand(len(df)) < 0.66\n",
    "\n",
    "# train_df = df[msk]\n",
    "# test_df = df[~msk]\n",
    "\n",
    "# true_labels = test_df['true_label'].tolist()\n",
    "\n",
    "# train_df.to_pickle(\"train_df\")\n",
    "# test_df.to_pickle(\"test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avglen</th>\n",
       "      <th>maxlen</th>\n",
       "      <th>minlen</th>\n",
       "      <th>caplen</th>\n",
       "      <th>capmax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>70.206076</td>\n",
       "      <td>910</td>\n",
       "      <td>2</td>\n",
       "      <td>4.115252</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>140.305720</td>\n",
       "      <td>223</td>\n",
       "      <td>13</td>\n",
       "      <td>15.114398</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text                                \n",
       "           avglen maxlen minlen     caplen capmax\n",
       "class                                            \n",
       "ham     70.206076    910      2   4.115252    129\n",
       "spam   140.305720    223     13  15.114398    128"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avglen(x):\n",
    "    s = 0\n",
    "    for i,y in x.iteritems():\n",
    "        s = s+ len(y)\n",
    "    a = s/x.size    \n",
    "    return a\n",
    "\n",
    "def caplen(x):\n",
    "    s = 0\n",
    "    for i,y in x.iteritems():\n",
    "        s = s+ sum(1 for c in y if c.isupper())\n",
    "    a = s/x.size    \n",
    "    return a\n",
    "\n",
    "def capmax(x):\n",
    "    s = 0\n",
    "    for i,y in x.iteritems():\n",
    "        s = max(s, sum(1 for c in y if c.isupper())) \n",
    "    return s\n",
    "\n",
    "def maxlen(x):\n",
    "    l = 0\n",
    "    for i,y in x.iteritems():\n",
    "        l = max(l,len(y))\n",
    "    return l\n",
    "\n",
    "def minlen(x):\n",
    "    l = 5000\n",
    "    for i,y in x.iteritems():\n",
    "        l = min(l,len(y))\n",
    "    return l\n",
    "\n",
    "train_df.groupby(\"class\").agg({\"text\": [avglen,maxlen,minlen,caplen,capmax]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 431\n"
     ]
    }
   ],
   "source": [
    "sc = 0\n",
    "hc = 0\n",
    "lt = 6\n",
    "# for v in df.iterrows():\n",
    "# #     print(v[0])\n",
    "#     if(v[1]['class']=='spam' and len(v[1]['text'])>lt):\n",
    "#         sc = sc+1\n",
    "#     if(v[1]['class']=='ham' and len(v[1]['text'])>lt):\n",
    "#         hc = hc+1\n",
    "        \n",
    "for v in df.iterrows():\n",
    "#     print(v[0])\n",
    "    if(v[1]['class']=='spam' and sum(1 for c in v[1]['text'] if c.isupper())>lt):\n",
    "        sc = sc+1\n",
    "    if(v[1]['class']=='ham' and sum(1 for c in v[1]['text'] if c.isupper())>lt):\n",
    "        hc = hc+1\n",
    "    \n",
    "print(sc,hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "class      \n",
       "ham    3193\n",
       "spam    507"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_pickle(\"train_df\")\n",
    "# train_df.head()\n",
    "train_df.groupby(\"class\").agg({\"text\": np.count_nonzero})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "class      \n",
       "ham    1632\n",
       "spam    240"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle(\"test_df\")\n",
    "\n",
    "true_labels = test_df['true_label'].tolist()\n",
    "\n",
    "test_df.groupby(\"class\").agg({\"text\": np.count_nonzero})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "1\n",
      "Ok lar... Joking wif u oni...\n",
      "6\n",
      "Even my brother is not like to speak with me. They treat me like aids patent.\n",
      "10\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "14\n",
      "I HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "16\n",
      "Oh k...i'm watching here:)\n",
      "17\n",
      "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "20\n",
      "Is that seriously how you spell his name?\n",
      "21\n",
      "I‘m going to try for 2 months ha ha only joking\n",
      "22\n",
      "So ü pay first lar... Then when is da stock comin...\n",
      "23\n",
      "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?\n",
      "25\n",
      "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\n",
      "26\n",
      "Lol your always so convincing.\n",
      "28\n",
      "I'm back &amp; we're packing the car now, I'll let you know if there's room\n",
      "29\n",
      "Ahhh. Work. I vaguely remember that! What does it feel like? Lol\n",
      "31\n",
      "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? \n",
      "33\n",
      "For fear of fainting with the of all that housework you just did? Quick have a cuppa\n",
      "36\n",
      "Oops, I'll let you know when my roommate's done\n",
      "38\n",
      "Anything lor... U decide...\n",
      "40\n",
      "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola\n",
      "41\n",
      "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy\n",
      "44\n",
      "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches...\n",
      "45\n",
      "No calls..messages..missed calls\n",
      "46\n",
      "Didn't you get hep b immunisation in nigeria.\n",
      "47\n",
      "Fair enough, anything going on?\n",
      "48\n",
      "Yeah hopefully, if tyler can't do it I could maybe ask around a bit\n",
      "49\n",
      "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers.\n",
      "50\n",
      "What you thinked about me. First time you saw me in class.\n",
      "51\n",
      "A gram usually runs like  &lt;#&gt; , a half eighth is smarter though and gets you almost a whole second gram for  &lt;#&gt;\n",
      "52\n",
      "K fyi x has a ride early tomorrow morning but he's crashing at our place tonight\n",
      "53\n",
      "Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always seemed so happy about \"the cave\". I'm sorry I didn't and don't have more to give. I'm sorry i offered. I'm sorry your room was so embarassing.\n",
      "60\n",
      "Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\n",
      "61\n",
      "Ha ha ha good joke. Girls are situation seekers.\n",
      "62\n",
      "Its a part of checking IQ\n",
      "63\n",
      "Sorry my roommates took forever, it ok if I come by now?\n",
      "64\n",
      "Ok lar i double check wif da hair dresser already he said wun cut v short. He said will cut until i look nice.\n",
      "66\n",
      "Today is \"song dedicated day..\" Which song will u dedicate for me? Send this to all ur valuable frnds but first rply me...\n",
      "69\n",
      "I plane to give on this month end.\n",
      "71\n",
      "Finished class where are you.\n",
      "72\n",
      "HI BABE IM AT HOME NOW WANNA DO SOMETHING? XX\n",
      "73\n",
      "K..k:)where are you?how did you performed?\n",
      "74\n",
      "U can call me now...\n",
      "76\n",
      "Thats cool. i am a gentleman and will treat you with dignity and respect.\n",
      "77\n",
      "I like you peoples very much:) but am very shy pa.\n",
      "78\n",
      "Does not operate after  &lt;#&gt;  or what\n",
      "79\n",
      "Its not the same here. Still looking for a job. How much do Ta's earn there.\n",
      "80\n",
      "Sorry, I'll call later\n",
      "81\n",
      "K. Did you call me just now ah? \n",
      "83\n",
      "You will be in the place of that man\n",
      "85\n",
      "I call you later, don't have network. If urgnt, sms me.\n",
      "88\n",
      "I'm really not up to it still tonight babe\n",
      "89\n",
      "Ela kano.,il download, come wen ur free..\n",
      "90\n",
      "Yeah do! Don‘t stand to close tho- you‘ll catch something!\n",
      "91\n",
      "Sorry to be a pain. Is it ok if we meet another night? I spent late afternoon in casualty and that means i haven't done any of y stuff42moro and that includes all my time sheets and that. Sorry. \n",
      "92\n",
      "Smile in Pleasure Smile in Pain Smile when trouble pours like Rain Smile when sum1 Hurts U Smile becoz SOMEONE still Loves to see u Smiling!!\n",
      "94\n",
      "Havent planning to buy later. I check already lido only got 530 show in e afternoon. U finish work already?\n",
      "96\n",
      "Watching telugu movie..wat abt u?\n",
      "98\n",
      "Hi. Wk been ok - on hols now! Yes on for a bit of a run. Forgot that i have hairdressers appointment at four so need to get home n shower beforehand. Does that cause prob for u?\"\n",
      "103\n",
      "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "105\n",
      "Umma my life and vava umma love you lot dear\n",
      "106\n",
      "Thanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\n",
      "107\n",
      "Aight, I'll hit you up when I get some cash\n",
      "108\n",
      "How would my ip address test that considering my computer isn't a minecraft server\n",
      "110\n",
      "Dont worry. I guess he's busy.\n",
      "111\n",
      "What is the plural of the noun research?\n",
      "112\n",
      "Going for dinner.msg you after.\n",
      "115\n",
      "Wa, ur openin sentence very formal... Anyway, i'm fine too, juz tt i'm eatin too much n puttin on weight...Haha... So anythin special happened?\n",
      "116\n",
      "As I entered my cabin my PA said, '' Happy B'day Boss !!''. I felt special. She askd me 4 lunch. After lunch she invited me to her apartment. We went there.\n",
      "124\n",
      "I am going to sao mu today. Will be done only at 12 \n",
      "125\n",
      "Ü predict wat time ü'll finish buying?\n",
      "126\n",
      "Good stuff, will do.\n",
      "127\n",
      "Just so that you know,yetunde hasn't sent money yet. I just sent her a text not to bother sending. So its over, you dont have to involve yourself in anything. I shouldn't have imposed anything on you in the first place so for that, i apologise.\n",
      "128\n",
      "Are you there in room.\n",
      "129\n",
      "HEY GIRL. HOW R U? HOPE U R WELL ME AN DEL R BAK! AGAIN LONG TIME NO C! GIVE ME A CALL SUM TIME FROM LUCYxx\n",
      "130\n",
      "K..k:)how much does it cost?\n",
      "131\n",
      "I'm home.\n",
      "133\n",
      "First answer my question.\n",
      "137\n",
      "He is there. You call and meet him\n",
      "138\n",
      "No no. I will check all rooms befor activities\n",
      "140\n",
      "Got c... I lazy to type... I forgot ü in lect... I saw a pouch but like not v nice...\n",
      "143\n",
      "A swt thought: \"Nver get tired of doing little things 4 lovable persons..\" Coz..somtimes those little things occupy d biggest part in their Hearts.. Gud ni8\n",
      "144\n",
      "I know you are. Can you pls open the back?\n",
      "145\n",
      "Yes see ya not on the dot\n",
      "148\n",
      "Ummma.will call after check in.our life will begin from qatar so pls pray very hard.\n",
      "149\n",
      "K..i deleted my contact that why?\n",
      "152\n",
      "Yup i thk cine is better cos no need 2 go down 2 plaza mah.\n",
      "153\n",
      "Ok... Ur typical reply...\n",
      "158\n",
      "Hello, my love. What are you doing? Did you get to that interview today? Are you you happy? Are you being a good boy? Do you think of me?Are you missing me ?\n",
      "166\n",
      "I place all ur points on e cultures module already.\n",
      "170\n",
      "Yes :)it completely in out of form:)clark also utter waste.\n",
      "171\n",
      "Sir, I need AXIS BANK account no and bank address.\n",
      "172\n",
      "Hmmm.. Thk sure got time to hop ard... Ya, can go 4 free abt... Muz call u to discuss liao... \n",
      "174\n",
      "Bloody hell, cant believe you forgot my surname Mr . Ill give u a clue, its spanish and begins with m... \n",
      "176\n",
      "Let me know when you've got the money so carlos can make the call\n",
      "180\n",
      "Hi! You just spoke to MANEESHA V. We'd like to know if you were satisfied with the experience. Reply Toll Free with Yes or No.\n",
      "181\n",
      "You lifted my hopes with the offer of money. I am in need. Especially when the end of the month approaches and it hurts my studying. Anyways have a gr8 weekend\n",
      "184\n",
      "He will, you guys close?\n",
      "185\n",
      "Going on nothing great.bye\n",
      "186\n",
      "Hello handsome ! Are you finding that job ? Not being lazy ? Working towards getting back that net for mummy ? Where's my boytoy now ? Does he miss me ?\n",
      "189\n",
      "Have you got Xmas radio times. If not i will get it now\n",
      "190\n",
      "I jus reached home. I go bathe first. But my sis using net tell u when she finishes k...\n",
      "192\n",
      "I'm sorry. I've joined the league of people that dont keep in touch. You mean a great deal to me. You have been a friend at all times even at great personal cost. Do have a great week.|\n",
      "194\n",
      "It will stop on itself. I however suggest she stays with someone that will be able to give ors for every stool.\n",
      "195\n",
      "How are you doing? Hope you've settled in for the new school year. Just wishin you a gr8 day\n",
      "196\n",
      "Gud mrng dear hav a nice day\n",
      "197\n",
      "Did u got that persons story\n",
      "202\n",
      "Hello darlin ive finished college now so txt me when u finish if u can love Kate xxx\n",
      "206\n",
      "Ü say until like dat i dun buy ericsson oso cannot oredi lar...\n",
      "207\n",
      "As I entered my cabin my PA said, '' Happy B'day Boss !!''. I felt special. She askd me 4 lunch. After lunch she invited me to her apartment. We went there.\n",
      "208\n",
      "Aight yo, dats straight dogg\n",
      "209\n",
      "You please give us connection today itself before  &lt;DECIMAL&gt;  or refund the bill\n",
      "210\n",
      "Both :) i shoot big loads so get ready!\n",
      "212\n",
      "Home so we can always chat\n",
      "214\n",
      "Yup... How ü noe leh...\n",
      "215\n",
      "Sounds great! Are you home now?\n",
      "216\n",
      "Finally the match heading towards draw as your prediction.\n",
      "217\n",
      "Tired. I haven't slept well the past few nights.\n",
      "218\n",
      "Easy ah?sen got selected means its good..\n",
      "220\n",
      "Yeah you should. I think you can use your gt atm now to register. Not sure but if there's anyway i can help let me know. But when you do be sure you are ready.\n",
      "221\n",
      "Ok no prob. Take ur time.\n",
      "224\n",
      "U say leh... Of course nothing happen lar. Not say v romantic jus a bit only lor. I thk e nite scenery not so nice leh.\n",
      "226\n",
      "Would really appreciate if you call me. Just need someone to talk to.\n",
      "228\n",
      "Hey company elama po mudyadhu.\n",
      "229\n",
      "Life is more strict than teacher... Bcoz Teacher teaches lesson &amp; then conducts exam, But Life first conducts Exam &amp; then teaches Lessons. Happy morning. . .\n",
      "230\n",
      "Dear good morning now only i am up\n",
      "231\n",
      "Get down in gandhipuram and walk to cross cut road. Right side &lt;#&gt; street road and turn at first right.\n",
      "234\n",
      "Yes:)here tv is always available in work place..\n",
      "236\n",
      "I have printed it oh. So  &lt;#&gt;  come upstairs\n",
      "239\n",
      "New Theory: Argument wins d SITUATION, but loses the PERSON. So dont argue with ur friends just.. . . . kick them &amp; say, I'm always correct.!\n",
      "241\n",
      "Tomarrow final hearing on my laptop case so i cant.\n",
      "242\n",
      "PLEASSSSSSSEEEEEE TEL ME V AVENT DONE SPORTSx\n",
      "243\n",
      "Okay. No no, just shining on. That was meant to be signing, but that sounds better.\n",
      "244\n",
      "Although i told u dat i'm into baig face watches now but i really like e watch u gave cos it's fr u. Thanx 4 everything dat u've done today, i'm touched...\n",
      "245\n",
      "U don't remember that old commercial?\n",
      "246\n",
      "Too late. I said i have the website. I didn't i have or dont have the slippers\n",
      "247\n",
      "I asked you to call him now ok\n",
      "248\n",
      "Kallis wont bat in 2nd innings.\n",
      "252\n",
      "Wen ur lovable bcums angry wid u, dnt take it seriously.. Coz being angry is d most childish n true way of showing deep affection, care n luv!.. kettoda manda... Have nice day da.\n",
      "253\n",
      "What you doing?how are you?\n",
      "254\n",
      "Ups which is 3days also, and the shipping company that takes 2wks. The other way is usps which takes a week but when it gets to lag you may have to bribe nipost to get your stuff.\n",
      "255\n",
      "I'm back, lemme know when you're ready\n",
      "256\n",
      "Don't necessarily expect it to be done before you get back though because I'm just now headin out\n",
      "258\n",
      "Where are you lover ? I need you ...\n",
      "260\n",
      "I‘m parked next to a MINI!!!! When are you coming in today do you think?\n",
      "262\n",
      "Anyway i'm going shopping on my own now. Cos my sis not done yet. Dun disturb u liao.\n",
      "267\n",
      "Not sure yet, still trying to get a hold of him\n",
      "271\n",
      "Come to mu, we're sorting out our narcotics situation\n",
      "272\n",
      "Night has ended for another day, morning has come in a special way. May you smile like the sunny rays and leaves your worries at the blue blue bay.\n",
      "274\n",
      "Usf I guess, might as well take 1 car\n",
      "278\n",
      "Awesome, I'll see you in a bit\n",
      "279\n",
      "Just sent it. So what type of food do you like?\n",
      "280\n",
      "All done? All handed in? Celebrations in full swing yet?\n",
      "281\n",
      "You got called a tool?\n",
      "282\n",
      "Wen u miss someone, the person is definitely special for u..... But if the person is so special, why to miss them, just Keep-in-touch gdeve..\n",
      "283\n",
      "Ok. I asked for money how far\n",
      "284\n",
      "Okie...\n",
      "285\n",
      "Yeah I think my usual guy's still passed out from last night, if you get ahold of anybody let me know and I'll throw down\n",
      "289\n",
      "My life Means a lot to me, Not because I love my life, But because I love the people in my life, The world calls them friends, I call them my World:-).. Ge:-)..\n",
      "290\n",
      "Dear,shall mail tonite.busy in the street,shall update you tonite.things are looking ok.varunnathu edukkukayee raksha ollu.but a good one in real sense.\n",
      "294\n",
      "Are you this much buzy\n",
      "295\n",
      "I accidentally deleted the message. Resend please.\n",
      "298\n",
      "Hurt me... Tease me... Make me cry... But in the end of my life when i die plz keep one rose on my grave and say STUPID I MISS U.. HAVE A NICE DAY BSLVYL\n",
      "299\n",
      "I cant pick the phone right now. Pls send a message\n",
      "300\n",
      "Need a coffee run tomo?Can't believe it's that time of week already\n",
      "302\n",
      "Shit that is really shocking and scary, cant imagine for a second. Def up for night out. Do u think there is somewhere i could crash for night, save on taxi?\n",
      "303\n",
      "Oh and by the way you do have more food in your fridge! Want to go out for a meal tonight? \n",
      "304\n",
      "He is a womdarfull actor\n",
      "306\n",
      "Yup... From what i remb... I think should be can book... \n",
      "307\n",
      "Jos ask if u wana meet up?\n",
      "311\n",
      "Today is ACCEPT DAY..U Accept me as? Brother Sister Lover Dear1 Best1 Clos1 Lvblefrnd Jstfrnd Cutefrnd Lifpartnr Belovd Swtheart Bstfrnd No rply means enemy\n",
      "313\n",
      "He says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week\n",
      "314\n",
      "Hi the way I was with u 2day, is the normal way&this is the real me. UR unique&I hope I know u 4 the rest of mylife. Hope u find wot was lost.\n",
      "315\n",
      "You made my day. Do have a great day too.\n",
      "316\n",
      "K.k:)advance happy pongal.\n",
      "321\n",
      "Merry Christmas to you too babe, i love ya *kisses*\n",
      "323\n",
      "cud u tell ppl im gona b a bit l8 cos 2 buses hav gon past cos they were full & im still waitin 4 1. Pete x\n",
      "325\n",
      "No problem. How are you doing?\n",
      "327\n",
      "Hi da:)how is the todays class?\n",
      "328\n",
      "I'd say that's a good sign but, well, you know my track record at reading women\n",
      "329\n",
      "Cool, text me when you're parked\n",
      "330\n",
      "I'm reading the text i just sent you. Its meant to be a joke. So read it in that light\n",
      "331\n",
      "K.k:)apo k.good movie.\n",
      "332\n",
      "Maybe i could get book out tomo then return it immediately ..? Or something.\n",
      "334\n",
      "Any chance you might have had with me evaporated as soon as you violated my privacy by stealing my phone number from your employer's paperwork. Not cool at all. Please do not contact me again or I will report you to your supervisor.\n",
      "337\n",
      "Cool. So how come you havent been wined and dined before?\n",
      "339\n",
      "Sorry, I'll call later\n",
      "343\n",
      "Where u been hiding stranger?\n",
      "345\n",
      "My sister cleared two round in birla soft yesterday.\n",
      "346\n",
      "Gudnite....tc...practice going on\n",
      "348\n",
      "One small prestige problem now.\n",
      "352\n",
      "If you're not in my car in an hour and a half I'm going apeshit\n",
      "353\n",
      "TODAY is Sorry day.! If ever i was angry with you, if ever i misbehaved or hurt you? plz plz JUST SLAP URSELF Bcoz, Its ur fault, I'm basically GOOD\n",
      "354\n",
      "Yo you guys ever figure out how much we need for alcohol? Jay and I are trying to figure out how much we can safely spend on weed\n",
      "355\n",
      "&lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "359\n",
      "I'm an actor. When i work, i work in the evening and sleep late. Since i'm unemployed at the moment, i ALWAYS sleep late. When you're unemployed, every day is saturday.\n",
      "360\n",
      "Hello! Just got here, st andrews-boy its a long way! Its cold. I will keep you posted\n",
      "361\n",
      "Ha ha cool cool chikku chikku:-):-DB-)\n",
      "362\n",
      "Oh ok no prob..\n",
      "363\n",
      "Check audrey's status right now\n",
      "366\n",
      "Well i know Z will take care of me. So no worries.\n",
      "369\n",
      "Wat uniform? In where get?\n",
      "371\n",
      "Hello my boytoy ... Geeee I miss you already and I just woke up. I wish you were here in bed with me, cuddling me. I love you ...\n",
      "372\n",
      "I will spoil you in bed as well :)\n",
      "373\n",
      "I'm going for bath will msg you next  &lt;#&gt;  min..\n",
      "374\n",
      "I cant keep talking to people if am not sure i can pay them if they agree to price. So pls tell me what you want to really buy and how much you are willing to pay\n",
      "378\n",
      "Well there's not a lot of things happening in Lindsay on New years *sighs* Some bars in Ptbo and the blue heron has something going\n",
      "379\n",
      "Keep my payasam there if rinu brings\n",
      "380\n",
      "I taught that Ranjith sir called me. So only i sms like that. Becaus hes verifying about project. Prabu told today so only pa dont mistake me..\n",
      "381\n",
      "I guess that's why you re worried. You must know that there's a way the body repairs itself. And i'm quite sure you shouldn't worry. We'll take it slow. First the tests, they will guide when your ovulation is then just relax. Nothing you've said is a reason to worry but i.ll keep on followin you up.\n",
      "382\n",
      "Yeah sure, give me a couple minutes to track down my wallet\n",
      "384\n",
      "Hey i will be late ah... Meet you at 945+\n",
      "386\n",
      "It took Mr owl 3 licks\n",
      "391\n",
      "Huh so late... Fr dinner?\n",
      "392\n",
      "Hey so this sat are we going for the intro pilates only? Or the kickboxing too? \n",
      "393\n",
      "Morning only i can ok.\n",
      "396\n",
      "From here after The performance award is calculated every two month.not for current one month period..\n",
      "398\n",
      "You are always putting your business out there. You put pictures of your ass on facebook. You are one of the most open people i've ever met. Why would i think a picture of your room would hurt you, make you feel violated.\n",
      "399\n",
      "Good evening Sir, Al Salam Wahleykkum.sharing a happy news.By the grace of God, i got an offer from Tayseer,TISSCO and i joined.Hope you are fine.Inshah Allah,meet you sometime.Rakhesh,visitor from India.\n",
      "402\n",
      "Dear how is chechi. Did you talk to her\n",
      "403\n",
      "The hair cream has not been shipped.\n",
      "404\n",
      "None of that's happening til you get here though\n",
      "406\n",
      "Haha get used to driving to usf man, I know a lot of stoners\n",
      "407\n",
      "All was well until slightly disastrous class this pm with my fav darlings! Hope day off ok. Coffee wld be good as can't stay late tomorrow. Same time + place as always?\n",
      "408\n",
      "Hello! Good week? Fancy a drink or something later?\n",
      "410\n",
      "Message:some text missing* Sender:Name Missing* *Number Missing *Sent:Date missing *Missing U a lot thats y everything is missing sent via fullonsms.com\n",
      "414\n",
      "Bring home some Wendy =D\n",
      "419\n",
      "Alright, I'll head out in a few minutes, text me where to meet you\n",
      "423\n",
      "Siva is in hostel aha:-.\n",
      "426\n",
      "Ok. She'll be ok. I guess\n",
      "428\n",
      "Any pain on urination any thing else?\n",
      "430\n",
      "I wnt to buy a BMW car urgently..its vry urgent.but hv a shortage of  &lt;#&gt; Lacs.there is no source to arng dis amt. &lt;#&gt; lacs..thats my prob\n",
      "431\n",
      "At home watching tv lor.\n",
      "435\n",
      "You available now? I'm like right around hillsborough &amp;  &lt;#&gt; th\n",
      "436\n",
      "The message sent is askin for  &lt;#&gt; dollars. Shoul i pay  &lt;#&gt;  or  &lt;#&gt; ?\n",
      "437\n",
      "Ask g or iouri, I've told the story like ten times already\n",
      "441\n",
      "Yes..he is really great..bhaji told kallis best cricketer after sachin in world:).very tough to get out.\n",
      "442\n",
      "You were supposed to wake ME up &gt;:(\n",
      "443\n",
      "Oic... I saw him too but i tot he din c me... I found a group liao...\n",
      "445\n",
      "HEY HEY WERETHE MONKEESPEOPLE SAY WE MONKEYAROUND! HOWDY GORGEOUS, HOWU DOIN? FOUNDURSELF A JOBYET SAUSAGE?LOVE JEN XXX\n",
      "446\n",
      "Sorry, my battery died, I can come by but I'm only getting a gram for now, where's your place?\n",
      "447\n",
      "Well done, blimey, exercise, yeah, i kinda remember wot that is, hmm. \n",
      "448\n",
      "I wont get concentration dear you know you are my mind and everything :-)\n",
      "449\n",
      "LOL ... Have you made plans for new years?\n",
      "450\n",
      "10 min later k...\n",
      "453\n",
      "K:)k:)what are detail you want to transfer?acc no enough?\n",
      "456\n",
      "Si si. I think ill go make those oreo truffles.\n",
      "458\n",
      "I hope you that's the result of being consistently intelligent and kind. Start asking him about practicum links and keep your ears open and all the best. ttyl\n",
      "459\n",
      "1.20 that call cost. Which i guess isnt bad. Miss ya, need ya, want ya, love ya\n",
      "460\n",
      "Going thru a very different feeling.wavering decisions and coping up with the same is the same individual.time will heal everything i believe.\n",
      "461\n",
      "Where did u go? My phone is gonna die you have to stay in here\n",
      "462\n",
      "Great. Never been better. Each day gives even more reasons to thank God\n",
      "464\n",
      "Sorry, I'll call later ok bye\n",
      "466\n",
      "great princess! I love giving and receiving oral. Doggy style is my fave position. How about you? I enjoy making love  &lt;#&gt;  times per night :)\n",
      "467\n",
      "They don't put that stuff on the roads to keep it from getting slippery over there?\n",
      "468\n",
      "When are you going to ride your bike?\n",
      "469\n",
      "Yup, no need. I'll jus wait 4 e rain 2 stop.\n",
      "470\n",
      "There are many company. Tell me the language.\n",
      "472\n",
      "How long has it been since you screamed, princess?\n",
      "475\n",
      "Nice line said by a broken heart- Plz don't cum 1 more times infront of me... Other wise once again I ll trust U... Good 9t:)\n",
      "476\n",
      "Ok I'm gonna head up to usf in like fifteen minutes\n",
      "477\n",
      "Love you aathi..love u lot..\n",
      "478\n",
      "Tension ah?what machi?any problem?\n",
      "479\n",
      "K, can I pick up another 8th when you're done?\n",
      "480\n",
      "When're you guys getting back? G said you were thinking about not staying for mcr\n",
      "482\n",
      "Yo carlos, a few friends are already asking me about you, you working at all this weekend?\n",
      "484\n",
      "Thank you baby! I cant wait to taste the real thing...\n",
      "486\n",
      "If we win its really no 1 side for long time.\n",
      "488\n",
      "Dear reached railway. What happen to you\n",
      "490\n",
      "I think i've fixed it can you send a test message?\n",
      "493\n",
      "Sorry,in meeting I'll call later\n",
      "494\n",
      "What class of  &lt;#&gt;  reunion?\n",
      "495\n",
      "Are you free now?can i call now?\n",
      "498\n",
      "Some of them told accenture is not confirm. Is it true.\n",
      "499\n",
      "Kate jackson rec center before 7ish, right?\n",
      "502\n",
      "When can ü come out?\n",
      "503\n",
      "Check with nuerologist.\n",
      "504\n",
      "Lolnice. I went from a fish to ..water.?\n",
      "506\n",
      "No it's waiting in e car dat's bored wat. Cos wait outside got nothing 2 do. At home can do my stuff or watch tv wat.\n",
      "507\n",
      "Maybe westshore or hyde park village, the place near my house?\n",
      "509\n",
      "What's the significance?\n",
      "511\n",
      "8 at the latest, g's still there if you can scrounge up some ammo and want to give the new ak a try\n",
      "512\n",
      "Prabha..i'm soryda..realy..frm heart i'm sory\n",
      "513\n",
      "Lol ok your forgiven :)\n",
      "514\n",
      "No..jst change tat only..\n",
      "516\n",
      "S:)no competition for him.\n",
      "522\n",
      "Shall i come to get pickle\n",
      "523\n",
      "Were gonna go get some tacos\n",
      "526\n",
      "Hi i won't b ard 4 christmas. But do enjoy n merry x'mas.\n",
      "530\n",
      "Jay says that you're a double-faggot\n",
      "533\n",
      "Gudnite....tc...practice going on\n",
      "534\n",
      "I'll be late...\n",
      "536\n",
      "Good afternoon, my love! How goes that day ? I hope maybe you got some leads on a job. I think of you, boytoy and send you a passionate kiss from across the sea\n",
      "537\n",
      "Probably gonna be here for a while, see you later tonight &lt;)\n",
      "540\n",
      "I am in tirupur da, once you started from office call me.\n",
      "542\n",
      "A famous quote : when you develop the ability to listen to 'anything' unconditionally without losing your temper or self confidence, it means you are ......... 'MARRIED'\n",
      "543\n",
      "But am going to college pa. What to do. are else ill come there it self. Pa.\n",
      "544\n",
      "4 oclock at mine. Just to bash out a flat plan.\n",
      "545\n",
      "This girl does not stay in bed. This girl doesn't need recovery time. Id rather pass out while having fun then be cooped up in bed\n",
      "546\n",
      "Then any special there?\n",
      "547\n",
      "I know but you need to get hotel now. I just got my invitation but i had to apologise. Cali is to sweet for me to come to some english bloke's weddin\n",
      "550\n",
      "Ok give me 5 minutes I think I see her. BTW you're my alibi. You were cutting my hair the whole time.\n",
      "551\n",
      "Imagine you finally get to sink into that bath after I have put you through your paces, maybe even having you eat me for a while before I left ... But also imagine the feel of that cage on your cock surrounded by the bath water, reminding you always who owns you ... Enjoy, my cuck\n",
      "552\n",
      "Hurry up, I've been weed-deficient for like three days\n",
      "555\n",
      "I‘ll have a look at the frying pan in case it‘s cheap or a book perhaps. No that‘s silly a frying pan isn‘t likely to be a book\n",
      "558\n",
      "I know that my friend already told that.\n",
      "559\n",
      "Hi Princess! Thank you for the pics. You are very pretty. How are you?\n",
      "560\n",
      "Aiyo... U always c our ex one... I dunno abt mei, she haven reply... First time u reply so fast... Y so lucky not workin huh, got bao by ur sugardad ah...gee.. \n",
      "561\n",
      "Hi msg me:)i'm in office..\n",
      "563\n",
      "Geeeee ... I love you so much I can barely stand it\n",
      "565\n",
      "Fuck babe ... I miss you already, you know ? Can't you let me send you some money towards your net ? I need you ... I want you ... I crave you ...\n",
      "566\n",
      "Ill call u 2mrw at ninish, with my address that icky American freek wont stop callin me 2 bad Jen k eh?\n",
      "567\n",
      "Oooh bed ridden ey? What are YOU thinking of?\n",
      "568\n",
      "So anyways, you can just go to your gym or whatever, my love *smiles* I hope your ok and having a good day babe ... I miss you so much already\n",
      "569\n",
      "Love it! Daddy will make you scream with pleasure! I am going to slap your ass with my dick!\n",
      "570\n",
      "WOT U WANNA DO THEN MISSY?\n",
      "573\n",
      "Can you open the door?\n",
      "575\n",
      "Nope i waiting in sch 4 daddy... \n",
      "577\n",
      "I'm tired of arguing with you about this week after week. Do what you want and from now on, i'll do the same.\n",
      "578\n",
      "Ü wait 4 me in sch i finish ard 5..\n",
      "582\n",
      "Ok anyway no need to change with what you said\n",
      "584\n",
      "my ex-wife was not able to have kids. Do you want kids one day?\n",
      "585\n",
      "So how's scotland. Hope you are not over showing your JJC tendencies. Take care. Live the dream\n",
      "586\n",
      "Tell them u have a headache and just want to use 1 hour of sick time.\n",
      "587\n",
      "I dun thk i'll quit yet... Hmmm, can go jazz ? Yogasana oso can... We can go meet em after our lessons den... \n",
      "588\n",
      "Pete can you please ring meive hardly gotany credit\n",
      "589\n",
      "Ya srsly better than yi tho\n",
      "594\n",
      "You still at grand prix?\n",
      "595\n",
      "I met you as a stranger and choose you as my friend. As long as the world stands, our friendship never ends. Lets be Friends forever!!! Gud nitz...\n",
      "596\n",
      "I am great! How are you?\n",
      "597\n",
      "Gud mrng dear have a nice day\n",
      "599\n",
      "Will do. Was exhausted on train this morning. Too much wine and pie. You sleep well too\n",
      "602\n",
      "If u sending her home first it's ok lor. I'm not ready yet.\n",
      "604\n",
      "Be happy there. I will come after noon\n",
      "605\n",
      "Meet after lunch la...\n",
      "606\n",
      "TaKe CaRE n gET WeLL sOOn\n",
      "608\n",
      "what I meant to say is cant wait to see u again getting bored of this bridgwater banter\n",
      "609\n",
      "Neva mind it's ok..\n",
      "610\n",
      "It's fine, imma get a drink or somethin. Want me to come find you?\n",
      "613\n",
      "I have many dependents\n",
      "614\n",
      "THANX4 TODAY CER IT WAS NICE 2 CATCH UP BUT WE AVE 2 FIND MORE TIME MORE OFTEN OH WELL TAKE CARE C U SOON.C\n",
      "615\n",
      "I called and said all to him:)then he have to choose this future.\n",
      "616\n",
      "Happy valentines day I know its early but i have hundreds of handsomes and beauties to wish. So i thought to finish off aunties and uncles 1st...\n",
      "618\n",
      "For my family happiness..\n",
      "619\n",
      "I come n pick ü up... Come out immediately aft ur lesson...\n",
      "622\n",
      "Good words.... But words may leave u in dismay many times.\n",
      "623\n",
      "MAKE SURE ALEX KNOWS HIS BIRTHDAY IS OVER IN FIFTEEN MINUTES AS FAR AS YOU'RE CONCERNED\n",
      "625\n",
      "Nah it's straight, if you can just bring bud or drinks or something that's actually a little more useful than straight cash\n",
      "626\n",
      "Haha good to hear, I'm officially paid and on the market for an 8th\n",
      "627\n",
      "How many licks does it take to get to the center of a tootsie pop?\n",
      "628\n",
      "Yup i thk they r e teacher said that will make my face look longer. Darren ask me not 2 cut too short.\n",
      "631\n",
      "Please dont say like that. Hi hi hi\n",
      "632\n",
      "Thank u!\n",
      "633\n",
      "Oh that was a forwarded message. I thought you send that to me\n",
      "636\n",
      "Me n him so funny...\n",
      "637\n",
      "Sweetheart, hope you are not having that kind of day! Have one with loads of reasons to smile. Biola\n",
      "638\n",
      "When ü login dat time... Dad fetching ü home now?\n",
      "639\n",
      "What will we do in the shower, baby?\n",
      "640\n",
      "I had askd u a question some hours before. Its answer\n",
      "641\n",
      "Well imma definitely need to restock before thanksgiving, I'll let you know when I'm out\n",
      "644\n",
      "Ya very nice. . .be ready on thursday\n",
      "646\n",
      "Watching cartoon, listening music &amp; at eve had to go temple &amp; church.. What about u?\n",
      "647\n",
      "Do you mind if I ask what happened? You dont have to say if it is uncomfortable.\n",
      "649\n",
      "No prob. I will send to your email.\n",
      "651\n",
      "Thats cool! Sometimes slow and gentle. Sonetimes rough and hard :)\n",
      "654\n",
      "Fine i miss you very much.\n",
      "655\n",
      "Did u got that persons story\n",
      "657\n",
      "Sun cant come to earth but send luv as rays. cloud cant come to river but send luv as rain. I cant come to meet U, but can send my care as msg to U. Gud evng\n",
      "659\n",
      "It doesnt make sense to take it there unless its free. If you need to know more, wikipedia.com\n",
      "662\n",
      "Then mum's repent how?\n",
      "663\n",
      "Sorry me going home first... Daddy come fetch ü later...\n",
      "664\n",
      "Leave it de:-). Start Prepare for next:-)..\n",
      "665\n",
      "Yes baby! We can study all the positions of the kama sutra ;)\n",
      "666\n",
      "En chikku nange bakra msg kalstiya..then had tea/coffee?\n",
      "667\n",
      "Carlos'll be here in a minute if you still need to buy\n",
      "668\n",
      "This pay is  &lt;DECIMAL&gt;  lakhs:)\n",
      "669\n",
      "Have a good evening! Ttyl\n",
      "674\n",
      "Ditto. And you won't have to worry about me saying ANYTHING to you anymore. Like i said last night, you do whatever you want and i'll do the same. Peace.\n",
      "675\n",
      "I've got  &lt;#&gt; , any way I could pick up?\n",
      "676\n",
      "I dont knw pa, i just drink milk..\n",
      "678\n",
      "Piggy, r u awake? I bet u're still sleeping. I'm going 4 lunch now...\n",
      "679\n",
      "Cause I'm not freaky lol\n",
      "680\n",
      "Missed your call cause I was yelling at scrappy. Miss u. Can't wait for u to come home. I'm so lonely today.\n",
      "681\n",
      "What is this 'hex' place you talk of? Explain!\n",
      "682\n",
      "Ü log off 4 wat. It's sdryb8i\n",
      "685\n",
      "I wanted to ask ü to wait 4 me to finish lect. Cos my lect finishes in an hour anyway.\n",
      "687\n",
      "Every King Was Once A Crying Baby And Every Great Building Was Once A Map.. Not Imprtant Where U r TODAY, BUT Where U Wil Reach TOMORW. Gud ni8\n",
      "688\n",
      "Dear,Me at cherthala.in case u r coming cochin pls call bfore u start.i shall also reach accordingly.or tell me which day u r coming.tmorow i am engaged ans its holiday.\n",
      "691\n",
      "Was the farm open?\n",
      "692\n",
      "Sorry to trouble u again. Can buy 4d for my dad again? 1405, 1680, 1843. All 2 big 1 small, sat n sun. Thanx.\n",
      "693\n",
      "My sister in law, hope you are having a great month. Just saying hey. Abiola\n",
      "694\n",
      "Will purchase d stuff today and mail to you. Do you have a po box number?\n",
      "695\n",
      "Ah poop. Looks like ill prob have to send in my laptop to get fixed cuz it has a gpu problem\n",
      "696\n",
      "Good. Good job. I like entrepreneurs\n",
      "697\n",
      "Aight, you close by or still down around alex's place?\n",
      "698\n",
      "meet you in corporation st outside gap … you can see how my mind is working!\n",
      "699\n",
      "Mum ask ü to buy food home...\n",
      "701\n",
      "How much r ü willing to pay?\n",
      "702\n",
      "Sorry, I'll call later\n",
      "704\n",
      "Thats a bit weird, even ?- where is the do supposed to be happening? But good idea, sure they will be in pub!\n",
      "705\n",
      "True dear..i sat to pray evening and felt so.so i sms'd you in some time...\n",
      "706\n",
      "I don't think I can get away for a trek that long with family in town, sorry\n",
      "708\n",
      "Quite late lar... Ard 12 anyway i wun b drivin...\n",
      "714\n",
      "Save yourself the stress. If the person has a dorm account, just send your account details and the money will be sent to you.\n",
      "715\n",
      "He also knows about lunch menu only da. . I know\n",
      "716\n",
      "When i have stuff to sell i.ll tell you\n",
      "721\n",
      "Oh is it? Send me the address\n",
      "722\n",
      "S'fine. Anytime. All the best with it.\n",
      "723\n",
      "That is wondar full flim.\n",
      "724\n",
      "Ya even those cookies have jelly on them\n",
      "726\n",
      "Got it! It looks scrumptious... daddy wants to eat you all night long!\n",
      "729\n",
      "Exactly. Anyways how far. Is jide her to study or just visiting\n",
      "732\n",
      "No he didn't. Spring is coming early yay!\n",
      "733\n",
      "Lol you won't feel bad when I use her money to take you out to a steak dinner =D\n",
      "734\n",
      "Even u dont get in trouble while convincing..just tel him once or twice and just tel neglect his msgs dont c and read it..just dont reply\n",
      "735\n",
      "Leaving to qatar tonite in search of an opportunity.all went fast.pls add me in ur prayers dear.Rakhesh\n",
      "736\n",
      "Then why no one talking to me\n",
      "737\n",
      "Thanks for looking out for me. I really appreciate.\n",
      "739\n",
      "Wish i were with you now!\n",
      "741\n",
      "Yes i will be there. Glad you made it.\n",
      "742\n",
      "Do well :)all will for little time. Thing of good times ahead:\n",
      "743\n",
      "Just got up. have to be out of the room very soon. …. i hadn't put the clocks back til at 8 i shouted at everyone to get up and then realised it was 7. wahay. another hour in bed.\n",
      "744\n",
      "Ok. There may be a free gym about.\n",
      "749\n",
      "Is there a reason we've not spoken this year? Anyways have a great week and all the best in your exam\n",
      "753\n",
      "Dont gimme that lip caveboy\n",
      "754\n",
      "When did you get to the library\n",
      "755\n",
      "Realy sorry-i don't recognise this number and am now confused :) who r u please?! \n",
      "756\n",
      "So why didnt you holla?\n",
      "759\n",
      "U should have made an appointment\n",
      "760\n",
      "Call me when you/carlos is/are here, my phone's vibrate is acting up and I might not hear texts\n",
      "765\n",
      "Wishing you and your family Merry \"X\" mas and HAPPY NEW Year in advance..\n",
      "768\n",
      "Sorry, I'll call later\n",
      "770\n",
      "Lol I know! They're so dramatic. Schools already closed for tomorrow. Apparently we can't drive in the inch of snow were supposed to get.\n",
      "771\n",
      "Not getting anywhere with this damn job hunting over here!\n",
      "772\n",
      "Lol! U drunkard! Just doing my hair at d moment. Yeah still up 4 tonight. Wats the plan? \n",
      "776\n",
      "Thanks for picking up the trash.\n",
      "777\n",
      "Why don't you go tell your friend you're not sure you want to live with him because he smokes too much then spend hours begging him to come smoke\n",
      "778\n",
      "Hi its Kate it was lovely to see you tonight and ill phone you tomorrow. I got to sing and a guy gave me his card! xxx\n",
      "779\n",
      "Happy New year my dear brother. I really do miss you. Just got your number and decided to send you this text wishing you only happiness. Abiola\n",
      "781\n",
      "Your opinion about me? 1. Over 2. Jada 3. Kusruthi 4. Lovable 5. Silent 6. Spl character 7. Not matured 8. Stylish 9. Simple Pls reply..\n",
      "782\n",
      "Hmmm ... I thought we said 2 hours slave, not 3 ... You are late ... How should I punish you ?\n",
      "785\n",
      "Dont think so. It turns off like randomlly within 5min of opening\n",
      "787\n",
      "It does it on its own. Most of the time it fixes my spelling. But sometimes it gets a completely diff word. Go figure\n",
      "791\n",
      "This is hoping you enjoyed your game yesterday. Sorry i've not been in touch but pls know that you are fondly bein thot off. Have a great week. Abiola\n",
      "792\n",
      "All e best 4 ur driving tmr :-)\n",
      "793\n",
      "Y?WHERE U AT DOGBREATH? ITS JUST SOUNDING LIKE JAN C THATS AL!!!!!!!!!\n",
      "794\n",
      "Omg I want to scream. I weighed myself and I lost more weight! Woohoo!\n",
      "796\n",
      "it's really getting me down just hanging around.\n",
      "799\n",
      "Ok i msg u b4 i leave my house.\n",
      "800\n",
      "Gimme a few was  &lt;#&gt;  minutes ago\n",
      "805\n",
      "K I'll be there before 4.\n",
      "807\n",
      "sure, but make sure he knows we ain't smokin yet\n",
      "808\n",
      "Boooo you always work. Just quit.\n",
      "809\n",
      "I am taking half day leave bec i am not well\n",
      "810\n",
      "Ugh I don't wanna get out of bed. It's so warm.\n",
      "811\n",
      "S:)s.nervous  &lt;#&gt; :)\n",
      "816\n",
      "Where's my boytoy? I miss you ... What happened?\n",
      "818\n",
      "Also are you bringing galileo or dobby\n",
      "819\n",
      "Then why you not responding\n",
      "822\n",
      "On the road so cant txt\n",
      "826\n",
      "Hmm .. Bits and pieces lol ... *sighs* ...\n",
      "827\n",
      "Hahaha..use your brain dear\n",
      "828\n",
      "Hey. You got any mail?\n",
      "829\n",
      "Sorry light turned green, I meant another friend wanted  &lt;#&gt;  worth but he may not be around\n",
      "830\n",
      "Thanks for yesterday sir. You have been wonderful. Hope you enjoyed the burial. MojiBiola\n",
      "832\n",
      "Hi mate its RV did u hav a nice hol just a message 3 say hello coz havent sent u 1 in ages started driving so stay off roads!RVx\n",
      "835\n",
      "Surely result will offer:)\n",
      "841\n",
      "I luv u soo much u dont understand how special u r 2 me ring u 2morrow luv u xxx\n",
      "842\n",
      "Pls send me a comprehensive mail about who i'm paying, when and how much.\n",
      "843\n",
      "Our Prashanthettan's mother passed away last night. pray for her and family.\n",
      "845\n",
      "K.k:)when are you going?\n",
      "846\n",
      "Meanwhile in the shit suite: xavier decided to give us  &lt;#&gt;  seconds of warning that samantha was coming over and is playing jay's guitar to impress her or some shit. Also I don't think doug realizes I don't live here anymore\n",
      "847\n",
      "My stomach has been thru so much trauma I swear I just can't eat. I better lose weight.\n",
      "848\n",
      "I am in office:)whats the matter..msg me now.i will call you at break:).\n",
      "849\n",
      "Yeah there's barely enough room for the two of us, x has too many fucking shoes. Sorry man, see you later\n",
      "851\n",
      "U reach orchard already? U wan 2 go buy tickets first?\n",
      "852\n",
      "I am real, baby! I want to bring out your inner tigress...\n",
      "853\n",
      "No da if you run that it activate the full version da.\n",
      "855\n",
      "Stop the story. I've told him i've returned it and he's saying i should not re order it.\n",
      "859\n",
      "Spoons it is then okay?\n",
      "860\n",
      "Did he just say somebody is named tampa\n",
      "862\n",
      "Your brother is a genius\n",
      "864\n",
      "Did u find out what time the bus is at coz i need to sort some stuff out.\n",
      "865\n",
      "Dude ive been seeing a lotta corvettes lately\n",
      "873\n",
      "I'll text you when I drop x off\n",
      "875\n",
      "Talk With Yourself Atleast Once In A Day...!!! Otherwise You Will Miss Your Best FRIEND In This WORLD...!!! -Shakespeare- SHESIL  &lt;#&gt;\n",
      "877\n",
      "Are you in castor? You need to see something\n",
      "883\n",
      "I love to give massages. I use lots of baby oil... What is your fave position?\n",
      "884\n",
      "Dude we should go sup again\n",
      "885\n",
      "Yoyyooo u know how to change permissions for a drive in mac. My usb flash drive\n",
      "888\n",
      "Y dun cut too short leh. U dun like ah? She failed. She's quite sad.\n",
      "889\n",
      "You unbelievable faglord\n",
      "890\n",
      "Wife.how she knew the time of murder exactly\n",
      "892\n",
      "I am great princess! What are you thinking about me? :)\n",
      "895\n",
      "Doesn't g have class early tomorrow and thus shouldn't be trying to smoke at  &lt;#&gt;\n",
      "899\n",
      "Thursday night? Yeah, sure thing, we'll work it out then\n",
      "901\n",
      "Probably money worries. Things are coming due and i have several outstanding invoices for work i did two and three months ago.\n",
      "902\n",
      "How is it possible to teach you. And where.\n",
      "905\n",
      "We're all getting worried over here, derek and taylor have already assumed the worst\n",
      "906\n",
      "Hey what's up charles sorry about the late reply.\n",
      "908\n",
      "I.ll give her once i have it. Plus she said grinule greet you whenever we speak\n",
      "909\n",
      "WHITE FUDGE OREOS ARE IN STORES\n",
      "911\n",
      "My love ! How come it took you so long to leave for Zaher's? I got your words on ym and was happy to see them but was sad you had left. I miss you\n",
      "912\n",
      "I am sorry it hurt you.\n",
      "913\n",
      "Can't. I feel nauseous. I'm so pissed. I didn't eat any sweets all week cause today I was planning to pig out. I was dieting all week. And now I'm not hungry :/\n",
      "914\n",
      "Ok lor but not too early. Me still having project meeting now.\n",
      "915\n",
      "Call me da, i am waiting for your call.\n",
      "919\n",
      "Hey you gave them your photo when you registered for driving ah? Tmr wanna meet at yck? \n",
      "920\n",
      "Dont talk to him ever ok its my word.\n",
      "922\n",
      "On ma way to school. Can you pls send me ashley's number\n",
      "926\n",
      "K, wait chikku..il send aftr  &lt;#&gt; mins\n",
      "927\n",
      "But I'm on a diet. And I ate 1 too many slices of pizza yesterday. Ugh I'm ALWAYS on a diet.\n",
      "928\n",
      "K:)i will give my kvb acc details:)\n",
      "929\n",
      "Oh all have to come ah?\n",
      "932\n",
      "Congratulations ore mo owo re wa. Enjoy it and i wish you many happy moments to and fro wherever you go\n",
      "933\n",
      "So do you have samus shoulders yet\n",
      "936\n",
      "Then dun wear jeans lor...\n",
      "937\n",
      "Since when, which side, any fever, any vomitin.\n",
      "938\n",
      "K:)k.are you in college?\n",
      "940\n",
      "Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel bleh. But at least its not writhing pain kind of bleh.\n",
      "941\n",
      "No we sell it all so we'll have tons if coins. Then sell our coins to someone thru paypal. Voila! Money back in life pockets:)\n",
      "948\n",
      "Hey i booked the kb on sat already... what other lessons are we going for ah? Keep your sat night free we need to meet and confirm our lodging \n",
      "949\n",
      "Chk in ur belovd ms dict\n",
      "951\n",
      "Awesome, lemme know whenever you're around\n",
      "952\n",
      "Shb b ok lor... Thanx...\n",
      "953\n",
      "Beautiful Truth against Gravity.. Read carefully: \"Our heart feels light when someone is in it.. But it feels very heavy when someone leaves it..\" GOOD NIGHT\n",
      "954\n",
      "Also remember to get dobby's bowl from your car\n",
      "956\n",
      "Sorry i now then c ur msg... Yar lor so poor thing... But only 4 one night... Tmr u'll have a brand new room 2 sleep in...\n",
      "957\n",
      "Love isn't a decision, it's a feeling. If we could decide who to love, then, life would be much simpler, but then less magical\n",
      "958\n",
      "Welp apparently he retired\n",
      "960\n",
      "Where @\n",
      "965\n",
      "Are you this much buzy\n",
      "966\n",
      "Or better still can you catch her and let ask her if she can sell  &lt;#&gt;  for me.\n",
      "969\n",
      "Are you willing to go for aptitude class.\n",
      "970\n",
      "It wont b until 2.15 as trying 2 sort house out, is that ok?\n",
      "971\n",
      "Yar lor he wan 2 go c horse racing today mah, so eat earlier lor. I ate chicken rice. U?\n",
      "972\n",
      "Haha awesome, omw back now then\n",
      "974\n",
      "what is your account number?\n",
      "975\n",
      "Eh u send wrongly lar...\n",
      "976\n",
      "Hey no I ad a crap nite was borin without ya 2 boggy with me u boring biatch! Thanx but u wait til nxt time il ave ya \n",
      "979\n",
      "Hey you can pay. With salary de. Only  &lt;#&gt; .\n",
      "981\n",
      "If he started searching he will get job in few days.he have great potential and talent.\n",
      "984\n",
      "LOOK AT THE FUCKIN TIME. WHAT THE FUCK YOU THINK IS UP\n",
      "986\n",
      "Carlos says he'll be at mu in  &lt;#&gt;  minutes\n",
      "987\n",
      "I'm in office now . I will call you  &lt;#&gt;  min:)\n",
      "988\n",
      "Geeee ... I miss you already, you know ? Your all I can think about. Fuck, I can't wait till next year when we will be together ... *loving kiss*\n",
      "989\n",
      "Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\n",
      "990\n",
      "Ugh. Gotta drive back to sd from la. My butt is sore.\n",
      "991\n",
      "26th OF JULY\n",
      "992\n",
      "Hi im having the most relaxing time ever! we have to get up at 7am every day! was the party good the other night? I get home tomorrow at 5ish.\n",
      "993\n",
      "Up to ü... Ü wan come then come lor... But i din c any stripes skirt...\n",
      "994\n",
      "The Xmas story is peace.. The Xmas msg is love.. The Xmas miracle is jesus.. Hav a blessed month ahead &amp; wish U Merry Xmas...\n",
      "997\n",
      "Yetunde i'm in class can you not run water on it to make it ok. Pls now.\n",
      "998\n",
      "Not a lot has happened here. Feels very quiet. Beth is at her aunts and charlie is working lots. Just me and helen in at the mo. How have you been? \n",
      "999\n",
      "Then ü wait 4 me at bus stop aft ur lect lar. If i dun c ü then i go get my car then come back n pick ü.\n",
      "1000\n",
      "Aight will do, thanks again for comin out\n",
      "1006\n",
      "Give me a sec to think think about it\n",
      "1008\n",
      "I don't quite know what to do. I still can't get hold of anyone. I cud pick you up bout 7.30pm and we can see if they're in the pub?\n",
      "1009\n",
      "Poyyarikatur,kolathupalayam,unjalur post,erode dis, &lt;#&gt; .\n",
      "1010\n",
      "Dear Hero,i am leaving to qatar tonite for an apt opportunity.pls do keep in touch at  &lt;EMAIL&gt; ,kerala\n",
      "1011\n",
      "Lol I would but my mom would have a fit and tell the whole family how crazy and terrible I am\n",
      "1012\n",
      "I just got home babe, are you still awake ?\n",
      "1014\n",
      "Just buy a pizza. Meat lovers or supreme. U get to pick.\n",
      "1015\n",
      "Ya, told..she was asking wats matter?\n",
      "1018\n",
      "Shall i send that exe to your mail id.\n",
      "1019\n",
      "Nope watching tv at home... Not going out. V bored...\n",
      "1020\n",
      "Don know..wait i will check it.\n",
      "1021\n",
      "Good afternoon on this glorious anniversary day, my sweet J !! I hope this finds you happy and content, my Prey. I think of you and send a teasing kiss from across the sea coaxing images of fond souveniers ... You Cougar-Pen\n",
      "1023\n",
      "We still on for tonight?\n",
      "1026\n",
      "I have a sore throat. It's scratches when I talk\n",
      "1028\n",
      "Are you not around or just still asleep? :V\n",
      "1029\n",
      "Lol you forgot it eh ? Yes, I'll bring it in babe\n",
      "1030\n",
      "Its good, we'll find a way\n",
      "1031\n",
      "Can not use foreign stamps in this country. Good lecture .\n",
      "1032\n",
      "Yup bathe liao...\n",
      "1033\n",
      "HAPPY NEW YEAR MY NO.1 MAN\n",
      "1034\n",
      "OH MR SHEFFIELD! You wanna play THAT game, okay. You're the boss and I'm the nanny. You give me a raise and I'll give YOU one!!\n",
      "1035\n",
      "ZOE IT JUST HIT ME 2 IM FUCKING SHITIN MYSELF IL DEFO TRY MY HARDEST 2 CUM 2MOROW LUV U MILLIONS LEKDOG\n",
      "1037\n",
      "No my blankets are sufficient, thx\n",
      "1038\n",
      "naughty little thought: 'its better to flirt, flirt n flirt, rather than loving someone n gettin hurt, hurt n hurt...:-) Gud nyt\n",
      "1039\n",
      "Edison has rightly said, \"A fool can ask more questions than a wise man can answer\" Now you know why all of us are speechless during ViVa.. GM,GN,GE,GNT:-)\n",
      "1040\n",
      "They just talking thats it de. They wont any other.\n",
      "1042\n",
      "I'm in class. Will holla later\n",
      "1044\n",
      "Mmm thats better now i got a roast down me! id b better if i had a few drinks down me 2! Good indian?\n",
      "1047\n",
      "Do 1 thing! Change that sentence into: \"Because i want 2 concentrate in my educational career im leaving here..\"\n",
      "1049\n",
      "I walked an hour 2 c u! doesnt that show I care y wont u believe im serious?\n",
      "1052\n",
      "Do u noe wat time e place dat sells 4d closes?\n",
      "1054\n",
      "Jay's getting really impatient and belligerent\n",
      "1056\n",
      "I'm at work. Please call\n",
      "1058\n",
      "Ard 515 like dat. Y?\n",
      "1059\n",
      "Tell me they're female :V how're you throwing in? We're deciding what all to get now\n",
      "1061\n",
      "I'm working technical support :)voice process.networking field.\n",
      "1062\n",
      "I might come to kerala for 2 days.so you can be prepared to take a leave once i finalise .dont plan any travel during my visit.need to finish urgent works.\n",
      "1063\n",
      "Ok. Not sure what time tho as not sure if can get to library before class. Will try. See you at some point! Have good eve.\n",
      "1066\n",
      "No my mum went 2 dentist.\n",
      "1067\n",
      "Once free call me sir. I am waiting for you.\n",
      "1070\n",
      "Jus finish bathing...\n",
      "1071\n",
      "alright, I'll make sure the car is back tonight\n",
      "1074\n",
      "Lul im gettin some juicy gossip at the hospital. Two nurses are talking about how fat they are gettin. And one thinks shes obese. Oyea.\n",
      "1075\n",
      "Aight ill get on fb in a couple minutes\n",
      "1076\n",
      "Oi. Ami parchi na re. Kicchu kaaj korte iccha korche na. Phone ta tul na. Plz. Plz.\n",
      "1077\n",
      "Where can download clear movies. Dvd copies.\n",
      "1081\n",
      "You tell what happen dont behave like this to me. Ok no need to say\n",
      "1082\n",
      "Can u get pic msgs to your phone?\n",
      "1085\n",
      "For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\n",
      "1086\n",
      "FR'NDSHIP is like a needle of a clock. Though V r in d same clock, V r nt able 2 met. Evn if V meet,itz only 4few seconds. Bt V alwys stay conected. Gud 9t;-)\n",
      "1087\n",
      "I don't think he has spatula hands!\n",
      "1090\n",
      "Goodmorning today i am late for  &lt;DECIMAL&gt; min.\n",
      "1094\n",
      "Well the weather in cali's great. But its complexities are great. You need a car to move freely, its taxes are outrageous. But all in all its a great place. The sad part is i missing home.\n",
      "1096\n",
      "Ryder unsold.now gibbs.\n",
      "1099\n",
      "NO GIFTS!! You trying to get me to throw myself off a cliff or something?\n",
      "1100\n",
      "Been up to ne thing interesting. Did you have a good birthday? When are u wrking nxt? I started uni today.\n",
      "1101\n",
      "You busy or can I come by at some point and figure out what we're doing tomorrow\n",
      "1102\n",
      "Yeah go on then, bored and depressed sittin waitin for phone to ring... Hope the wind drops though, scary\n",
      "1103\n",
      "Black shirt n blue jeans... I thk i c ü...\n",
      "1104\n",
      "Aiyah sorry lor... I watch tv watch until i forgot 2 check my phone.\n",
      "1106\n",
      "on hen night. Going with a swing\n",
      "1107\n",
      "Good afternoon, my love. How goes your day ? What are you up to ? I woke early and am online waiting for you ... Hmmm ... Italian boy is online I see . *grins*\n",
      "1109\n",
      "No you'll just get a headache trying to figure it out. U can trust me to do the math. I promise. O:-)\n",
      "1110\n",
      "S s..first time..dhoni rocks...\n",
      "1111\n",
      "Ok ill tell the company\n",
      "1112\n",
      "Awesome, think we can get an 8th at usf some time tonight?\n",
      "1115\n",
      "No no:)this is kallis home ground.amla home town is durban:)\n",
      "1116\n",
      "So lets make it saturday or monday as per convenience.\n",
      "1117\n",
      "Hey... What time is your driving on fri? We go for evaluation on fri?\n",
      "1123\n",
      "Ok.ok ok..then..whats ur todays plan\n",
      "1124\n",
      "Good morning princess! How are you?\n",
      "1125\n",
      "Aiyar sorry lor forgot 2 tell u...\n",
      "1128\n",
      "Height of \"Oh shit....!!\" situation: A guy throws a luv letter on a gal but falls on her brothers head whos a gay,.;-):-D\n",
      "1130\n",
      "So check your errors and if you had difficulties, do correction.\n",
      "1131\n",
      "Howz pain?hope u r fine..\n",
      "1133\n",
      "Good morning princess! How are you?\n",
      "1134\n",
      "As I entered my cabin my PA said, '' Happy B'day Boss !!''. I felt special. She askd me 4 lunch. After lunch she invited me to her apartment. We went there.\n",
      "1135\n",
      "U wake up already? Thanx 4 e tau sar piah it's quite nice.\n",
      "1136\n",
      "K do I need a login or anything\n",
      "1139\n",
      "What * u wearing?\n",
      "1143\n",
      "Have you had a good day? Mine was really busy are you up to much tomorrow night?\n",
      "1144\n",
      "And is there a way you can send shade's stuff to her. And she has been wonderful too.\n",
      "1145\n",
      "Really... I tot ur paper ended long ago... But wat u copied jus now got use? U happy lar... I still haf 2 study :-(\n",
      "1147\n",
      "Babe ? I lost you ... :-(\n",
      "1148\n",
      "Ok... Help me ask if she's working tmr a not?\n",
      "1149\n",
      "I'm not driving... Raining! Then i'll get caught at e mrt station lor.\n",
      "1150\n",
      "Not a drop in the tank\n",
      "1151\n",
      "(That said can you text him one more time?)\n",
      "1156\n",
      "Sorry man, accidentally left my phone on silent last night and didn't check it til I got up\n",
      "1157\n",
      "Hey.. Something came up last min.. Think i wun be signing up tmr.. Hee\n",
      "1158\n",
      "He's an adult and would learn from the experience. There's no real danger. I just dont like peeps using drugs they dont need. But no comment\n",
      "1159\n",
      "Hey! There's veggie pizza... :/\n",
      "1160\n",
      "Yun buying... But school got offer 2000 plus only...\n",
      "1161\n",
      "You sure your neighbors didnt pick it up\n",
      "1162\n",
      "K. I will sent it again\n",
      "1164\n",
      "New Theory: Argument wins d SITUATION, but loses the PERSON. So dont argue with ur friends just.. . . . kick them &amp; say, I'm always correct.!\n",
      "1165\n",
      "Well. Im computerless. Time to make some oreo truffles\n",
      "1167\n",
      "I am not having her number sir\n",
      "1168\n",
      "Lol now I'm after that hot air balloon!\n",
      "1169\n",
      "Ok . . now i am in bus. . If i come soon i will come otherwise tomorrow\n",
      "1170\n",
      "Msgs r not time pass.They silently say that I am thinking of U right now and also making U think of me at least 4 a moment. Gd nt.swt drms @Shesil\n",
      "1173\n",
      "Happy new years melody!\n",
      "1174\n",
      "Ü dun need to pick ur gf?\n",
      "1175\n",
      "Yay! You better not have told that to 5 other girls either.\n",
      "1176\n",
      "Horrible u eat macs eat until u forgot abt me already rite... U take so long 2 reply. I thk it's more toot than b4 so b prepared. Now wat shall i eat?\n",
      "1177\n",
      "Did he say how fantastic I am by any chance, or anything need a bigger life lift as losing the will 2 live, do you think I would be the first person 2 die from N V Q? \n",
      "1180\n",
      "To day class is there are no class.\n",
      "1181\n",
      "I'm in chennai velachery:)\n",
      "1183\n",
      "K give me a sec, breaking a  &lt;#&gt;  at cstore\n",
      "1184\n",
      "Am i that much bad to avoid like this?\n",
      "1185\n",
      "Yo, you around? Just got my car back\n",
      "1186\n",
      "Annoying isn't it.\n",
      "1187\n",
      "Goodmorning, Today i am late for  &lt;#&gt; min.\n",
      "1190\n",
      "In that case I guess I'll see you at campus lodge\n",
      "1191\n",
      "We're done...\n",
      "1193\n",
      "I was up all night too worrying about this appt. It's a shame we missed a girls night out with quizzes popcorn and you doing my hair.\n",
      "1195\n",
      "Ok... C ya...\n",
      "1198\n",
      "He also knows about lunch menu only da. . I know\n",
      "1199\n",
      "Al he does is moan at me if n e thin goes wrong its my fault&al de arguments r my fault&fed up of him of himso y bother? Hav 2go, thanx.xx\n",
      "1200\n",
      "NEFT Transaction with reference number  &lt;#&gt;  for Rs. &lt;DECIMAL&gt;  has been credited to the beneficiary account on  &lt;#&gt;  at  &lt;TIME&gt; : &lt;#&gt;\n",
      "1201\n",
      "Otherwise had part time job na-tuition..\n",
      "1202\n",
      "I know she called me\n",
      "1203\n",
      "Me also da, i feel yesterday night  wait til 2day night dear.\n",
      "1204\n",
      "Thanks for understanding. I've been trying to tell sura that.\n",
      "1206\n",
      "The whole car appreciated the last two! Dad and are having a map reading semi argument but apart from that things are going ok. P.\n",
      "1208\n",
      "I need you to be in my strong arms...\n",
      "1209\n",
      "Also maaaan are you missing out\n",
      "1210\n",
      "His bday real is in april .\n",
      "1212\n",
      "Ok then i will come to ur home after half an hour\n",
      "1213\n",
      "Yo, the game almost over? Want to go to walmart soon\n",
      "1215\n",
      "I'll text now! All creepy like so he won't think that we forgot\n",
      "1218\n",
      "Damn, can you make it tonight or do you want to just wait til tomorrow\n",
      "1219\n",
      "K..k..i'm also fine:)when will you complete the course?\n",
      "1220\n",
      "True. It is passable. And if you get a high score and apply for phd, you get 5years of salary. So it makes life easier.\n",
      "1222\n",
      "Prakesh is there know.\n",
      "1223\n",
      "Teach me apps da. When you come to college.\n",
      "1230\n",
      "Jus ans me lar. U'll noe later.\n",
      "1231\n",
      "I want to send something that can sell fast.  &lt;#&gt; k is not easy money.\n",
      "1233\n",
      "1's finish meeting call me.\n",
      "1235\n",
      "Hello-/@drivby-:0quit edrunk sorry iff pthis makes no senrd-dnot no how ^ dancce 2 drum n basq!ihave fun 2nhite x ros xxxxxxx\n",
      "1236\n",
      "Your opinion about me? 1. Over 2. Jada 3. Kusruthi 4. Lovable 5. Silent 6. Spl character 7. Not matured 8. Stylish 9. Simple Pls reply..\n",
      "1240\n",
      "Hope you are having a great new semester. Do wish you the very best. You are made for greatness.\n",
      "1241\n",
      "Oh yes I can speak txt 2 u no! Hmm. Did u get  email?\n",
      "1242\n",
      "I want to show you the world, princess :) how about europe?\n",
      "1243\n",
      "Nobody can decide where to eat and dad wants Chinese\n",
      "1244\n",
      "No shoot me. I'm in the docs waiting room. :/\n",
      "1246\n",
      "Hello which the site to download songs its urgent pls\n",
      "1247\n",
      "I do know what u mean,  is the king of not havin credit! I'm goin2bed now. Night night sweet! Only1more sleep! \n",
      "1250\n",
      "I call you later, don't have network. If urgnt, sms me.\n",
      "1253\n",
      "Yeah like if it goes like it did with my friends imma flip my shit in like half an hour\n",
      "1257\n",
      "Not yet chikku..going to room nw, i'm in bus..\n",
      "1258\n",
      "Am also doing in cbe only. But have to pay.\n",
      "1259\n",
      "Honey boo I'm missing u.\n",
      "1260\n",
      "We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
      "1261\n",
      "Yo, I'm at my parents' gettin cash. Good news: we picked up a downstem\n",
      "1262\n",
      "Thank you so much. When we skyped wit kz and sura, we didnt get the pleasure of your company. Hope you are good. We've given you ultimatum oh! We are countin down to aburo. Enjoy!\n",
      "1266\n",
      "Im in inperialmusic listening2the weirdest track ever byleafcutter john-sounds like insects being molested&someone plumbing,remixed by evil men on acid!\n",
      "1268\n",
      "SERIOUSLY. TELL HER THOSE EXACT WORDS RIGHT NOW.\n",
      "1272\n",
      "If you still havent collected the dough pls let me know so i can go to the place i sent it to get the control number\n",
      "1273\n",
      "Ok...\n",
      "1275\n",
      "Let me know how to contact you. I've you settled in a room. Lets know you are ok.\n",
      "1276\n",
      "Wot u up 2 u weirdo?\n",
      "1278\n",
      "Dont put your phone on silent mode ok\n",
      "1280\n",
      "Waiting 4 my tv show 2 start lor... U leh still busy doing ur report?\n",
      "1282\n",
      "Am I the only one who doesn't stalk profiles?\n",
      "1284\n",
      "Yes i thought so. Thanks.\n",
      "1286\n",
      "Just wondering, the others just took off\n",
      "1287\n",
      "Night has ended for another day, morning has come in a special way. May you smile like the sunny rays and leaves your worries at the blue blue bay. Gud mrng\n",
      "1288\n",
      "What do you do, my dog ? Must I always wait till the end of your day to have word from you ? Did you run out of time on your cell already?\n",
      "1289\n",
      "Happy new year to u too!\n",
      "1290\n",
      "Hey...Great deal...Farm tour 9am to 5pm $95/pax, $50 deposit by 16 May\n",
      "1291\n",
      "Eat jap done oso aft ur lect wat... Ü got lect at 12 rite... \n",
      "1292\n",
      "Hey babe! I saw you came online for a second and then you disappeared, what happened ?\n",
      "1294\n",
      "Happy birthday... May all ur dreams come true...\n",
      "1296\n",
      "TELL HER I SAID EAT SHIT.\n",
      "1299\n",
      "Your daily text from me – a favour this time\n",
      "1303\n",
      "FRAN I DECIDED 2 GO N E WAY IM COMPLETELY BROKE AN KNACKERED I GOT UP BOUT 3 C U 2MRW LOVE JANX P.S THIS IS MY DADS FONE, -NO CREDIT\n",
      "1304\n",
      "I cant pick the phone right now. Pls send a message\n",
      "1306\n",
      "Designation is software developer and may be she get chennai:)\n",
      "1309\n",
      "I jokin oni lar.. Ü busy then i wun disturb ü.\n",
      "1311\n",
      "I.ll always be there, even if its just in spirit. I.ll get a bb soon. Just trying to be sure i need it.\n",
      "1313\n",
      "I love u 2 babe! R u sure everything is alrite. Is he being an idiot? Txt bak girlie\n",
      "1314\n",
      "How abt making some of the pics bigger?\n",
      "1316\n",
      "Whenevr ur sad, Whenevr ur gray, Remembr im here 2 listn 2 watevr u wanna say, Jus walk wid me a little while,&amp; I promise I'll bring back ur smile.:-)\n",
      "1317\n",
      "Why nothing. Ok anyway give me treat\n",
      "1319\n",
      "Ok...\n",
      "1320\n",
      "Correct. So how was work today\n",
      "1321\n",
      "Just sent again. Do you scream and moan in bed, princess?\n",
      "1322\n",
      "I wake up long ago already... Dunno, what other thing?\n",
      "1324\n",
      "I thk 50 shd be ok he said plus minus 10.. Did ü leave a line in between paragraphs?\n",
      "1325\n",
      "Can you call me plz. Your number shows out of coveragd area. I have urgnt call in vasai &amp; have to reach before 4'o clock so call me plz\n",
      "1329\n",
      "My exam is for february 4. Wish you a great day.\n",
      "1330\n",
      "I dont know what to do to come out of this so only am ask questions like this dont mistake me.\n",
      "1331\n",
      "Aight no rush, I'll ask jay\n",
      "1332\n",
      "Good Morning plz call me sir\n",
      "1333\n",
      "It's ok lar. U sleep early too... Nite...\n",
      "1334\n",
      "Oh... Icic... K lor, den meet other day...\n",
      "1335\n",
      "Oh ! A half hour is much longer in Syria than Canada, eh ? Wow you must get SO much more work done in a day than us with all that extra time ! *grins*\n",
      "1336\n",
      "Sometimes we put walls around our hearts,not just to be safe from getting hurt.. But to find out who cares enough to break the walls &amp; get closer.. GOODNOON:)\n",
      "1337\n",
      "Sweet, we may or may not go to 4U to meet carlos so gauge patty's interest in that\n",
      "1338\n",
      "Then she buying today? Ü no need to c meh...\n",
      "1339\n",
      "Aight sorry I take ten years to shower. What's the plan?\n",
      "1340\n",
      "Every monday..nxt week vl be completing..\n",
      "1343\n",
      "Yeah I'll try to scrounge something up\n",
      "1344\n",
      "Crazy ar he's married. Ü like gd looking guys not me. My frens like say he's korean leona's fave but i dun thk he is. Aft some thinking mayb most prob i'll go.\n",
      "1345\n",
      "Were somewhere on Fredericksburg\n",
      "1347\n",
      "Is it ok if I stay the night here? Xavier has a sleeping bag and I'm getting tired\n",
      "1349\n",
      "Nothing much, chillin at home. Any super bowl plan?\n",
      "1351\n",
      "Bugis oso near wat... \n",
      "1352\n",
      "Yo theres no class tmrw right?\n",
      "1354\n",
      "Goodnight, sleep well da please take care pa. Please.\n",
      "1356\n",
      "Convey my regards to him\n",
      "1357\n",
      "U ned to convince him tht its not possible witot hurting his feeling its the main\n",
      "1359\n",
      "If i start sending blackberry torch to nigeria will you find buyer for me?like 4a month. And tell dad not to buy bb from anyone oh.\n",
      "1365\n",
      "Hey i will be really pretty late... You want to go for the lesson first? I will join you. I'm only reaching tp mrt\n",
      "1367\n",
      "Bbq this sat at mine from 6ish. Ur welcome 2 come\n",
      "1369\n",
      "Alright. I'm out--have a good night!\n",
      "1370\n",
      "Did you try making another butt.\n",
      "1371\n",
      "Hope you are feeling great. Pls fill me in. Abiola\n",
      "1372\n",
      "I though we shd go out n have some fun so bar in town or something – sound ok?\n",
      "1373\n",
      "1) Go to write msg 2) Put on Dictionary mode 3)Cover the screen with hand, 4)Press  &lt;#&gt; . 5)Gently remove Ur hand.. Its interesting..:)\n",
      "1377\n",
      "Auntie huai juan never pick up her phone\n",
      "1379\n",
      "Ya tel, wats ur problem..\n",
      "1382\n",
      "We spend our days waiting for the ideal path to appear in front of us.. But what we forget is.. \"paths are made by walking.. not by waiting..\" Goodnight!\n",
      "1384\n",
      "Please reserve ticket on saturday eve from chennai to thirunelvali and again from tirunelvali to chennai on sunday eve...i already see in net..no ticket available..i want to book ticket through tackle ..\n",
      "1385\n",
      "Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@\n",
      "1386\n",
      "That's ok. I popped in to ask bout something and she said you'd been in. Are you around tonght wen this girl comes?\n",
      "1388\n",
      "Hope ur head doesn't hurt 2 much ! Am ploughing my way through a pile of ironing ! Staying in with a chinky tonight come round if you like.\n",
      "1389\n",
      "Oh k.i think most of wi and nz players unsold.\n",
      "1390\n",
      "Haha... Where got so fast lose weight, thk muz go 4 a month den got effect... Gee,later we go aust put bk e weight.\n",
      "1392\n",
      "Haha just kidding, papa needs drugs\n",
      "1393\n",
      "Thk shld b can... Ya, i wana go 4 lessons... Haha, can go for one whole stretch...\n",
      "1394\n",
      "Oh ok..\n",
      "1395\n",
      "R we still meeting 4 dinner tonight?\n",
      "1396\n",
      "Thats cool! I am a gentleman and will treat you with dignity and respect.\n",
      "1397\n",
      "Shall i start from hear.\n",
      "1398\n",
      "Then we wait 4 u lor... No need 2 feel bad lar...\n",
      "1401\n",
      "No, I decided that only people who care about stuff vote and caring about stuff is for losers\n",
      "1402\n",
      "Kaiez... Enjoy ur tuition... Gee... Thk e second option sounds beta... I'll go yan jiu den msg u...\n",
      "1403\n",
      "You have registered Sinco as Payee. Log in at icicibank.com and enter URN  &lt;#&gt;  to confirm. Beware of frauds. Do NOT share or disclose URN to anyone.\n",
      "1404\n",
      "cool. We will have fun practicing making babies!\n",
      "1405\n",
      "Actually getting ready to leave the house.\n",
      "1408\n",
      "Then we gotta do it after that\n",
      "1409\n",
      "I've got ten bucks, jay is being noncomittal\n",
      "1410\n",
      "Where at were hungry too\n",
      "1411\n",
      "Pls speak to that customer machan.\n",
      "1412\n",
      "somewhere out there beneath the pale moon light someone think in of u some where out there where dreams come true... goodnite &amp; sweet dreams\n",
      "1413\n",
      "Wen ur lovable bcums angry wid u, dnt take it seriously.. Coz being angry is d most childish n true way of showing deep affection, care n luv!.. kettoda manda... Have nice day da.\n",
      "1415\n",
      "So wats ur opinion abt him and how abt is character?\n",
      "1416\n",
      "Jay is snickering and tells me that x is totally fucking up the chords as we speak\n",
      "1419\n",
      "Lmao. Take a pic and send it to me.\n",
      "1421\n",
      "No. She's currently in scotland for that.\n",
      "1422\n",
      "Do you work all this week ?\n",
      "1424\n",
      "Lol great now im getting hungry.\n",
      "1426\n",
      "I'll be at mu in like  &lt;#&gt;  seconds\n",
      "1428\n",
      "THING R GOOD THANX GOT EXAMS IN MARCH IVE DONE NO REVISION? IS FRAN STILL WITH BOYF? IVE GOTTA INTERVIW 4 EXETER BIT WORRIED!x\n",
      "1429\n",
      "Tell you what, if you make a little spreadsheet and track whose idea it was to smoke to determine who \"smokes too much\" for the entire month of february, I'll come up\n",
      "1431\n",
      "Don't look back at the building because you have no coat and i don't want you to get more sick. Just hurry home and wear a coat to the gym!!!\n",
      "1434\n",
      "You intrepid duo you! Have a great time and see you both soon. \n",
      "1435\n",
      "I asked sen to come chennai and search for job.\n",
      "1436\n",
      "Dad went out oredi... \n",
      "1437\n",
      "I jus hope its true that  missin me cos i'm really missin him! You haven't done anything to feel guilty about, yet.\n",
      "1439\n",
      "Arms fine, how's Cardiff and uni? \n",
      "1441\n",
      "Cool breeze... Bright sun... Fresh flower... Twittering birds... All these waiting to wish u: \"GOODMORNING &amp; HAVE A NICE DAY\" :)\n",
      "1442\n",
      "Ya:)going for restaurant..\n",
      "1443\n",
      "Its ok., i just askd did u knw tht no?\n",
      "1445\n",
      "Those ducking chinchillas\n",
      "1446\n",
      "I am in a marriage function\n",
      "1447\n",
      "Looks like u wil b getting a headstart im leaving here bout 2.30ish but if u r desperate for my company I could head in earlier-we were goin to meet in rummer.\n",
      "1448\n",
      "Don‘t give a flying monkeys wot they think and I certainly don‘t mind. Any friend of mine and all that!\n",
      "1450\n",
      "say thanks2. \n",
      "1451\n",
      "Msg me when rajini comes.\n",
      "1452\n",
      "Ya! when are ü taking ure practical lessons? I start in june..  \n",
      "1453\n",
      "That's good, because I need drugs\n",
      "1455\n",
      "Can ü all decide faster cos my sis going home liao..\n",
      "1459\n",
      "Fighting with the world is easy, u either win or lose bt fightng with some1 who is close to u is dificult if u lose - u lose if u win - u still lose.\n",
      "1462\n",
      "Kinda. First one gets in at twelve! Aah. Speak tomo\n",
      "1464\n",
      "Ok good then i later come find ü... C lucky i told ü to go earlier... Later pple take finish ü no more again...\n",
      "1465\n",
      "Wat makes u thk i'll fall down. But actually i thk i'm quite prone 2 falls. Lucky my dad at home i ask him come n fetch me already.\n",
      "1468\n",
      "I wont touch you with out your permission.\n",
      "1470\n",
      "7 wonders in My WORLD 7th You 6th Ur style 5th Ur smile 4th Ur Personality 3rd Ur Nature 2nd Ur SMS and 1st \"Ur Lovely Friendship\"... good morning dear\n",
      "1471\n",
      "Take some small dose tablet for fever\n",
      "1473\n",
      "Just sent you an email – to an address with incomm in it, is that right?\n",
      "1474\n",
      "Will do, you gonna be at blake's all night? I might be able to get out of here a little early\n",
      "1476\n",
      "Nice. Wait...should you be texting right now? I'm not gonna pay your ticket, ya know!\n",
      "1477\n",
      "I'm watching lotr w my sis dis aft. So u wan 2 meet me 4 dinner at nite a not?\n",
      "1482\n",
      "I'm a guy, browsin is compulsory\n",
      "1483\n",
      "Ok...\n",
      "1485\n",
      "Sorry, I'll call later\n",
      "1486\n",
      "(I should add that I don't really care and if you can't I can at least get this dude to fuck off but hey, your money if you want it)\n",
      "1487\n",
      "Hello lover! How goes that new job? Are you there now? Are you happy? Do you think of me? I wake, my slave and send you a teasing kiss from across the sea\n",
      "1489\n",
      "Tell them no need to investigate about me anywhere.\n",
      "1494\n",
      "How are you with moneY...as in to you...money aint a thing....how are you sha!\n",
      "1495\n",
      "It has everything to do with the weather. Keep extra warm. Its a cold but nothing serious. Pls lots of vitamin c\n",
      "1496\n",
      "Hey gals.. Anyone of u going down to e driving centre tmr?\n",
      "1497\n",
      "I'm always on yahoo messenger now. Just send the message to me and i.ll get it you may have to send it in the mobile mode sha but i.ll get it. And will reply.\n",
      "1498\n",
      "I'm putting it on now. It should be ready for  &lt;TIME&gt; \n",
      "1499\n",
      "Time n Smile r the two crucial things in our life. Sometimes time makes us to forget smile, and sometimes someone's smile makes us to forget time gud noon\n",
      "1503\n",
      "Don no da:)whats you plan?\n",
      "1504\n",
      "Ill be there on  &lt;#&gt;  ok.\n",
      "1505\n",
      "Oh my God. I'm almost home\n",
      "1506\n",
      "Total video converter free download type this in google search:)\n",
      "1508\n",
      "Wen ur lovable bcums angry wid u, dnt take it seriously.. Coz being angry is d most childish n true way of showing deep affection, care n luv!.. kettoda manda... Have nice day da.\n",
      "1509\n",
      "Sounds like something that someone testing me would sayy\n",
      "1513\n",
      "Hey sweet, I was wondering when you had a moment if you might come to me ? I want to send a file to someone but it won't go over yahoo for them because their connection sucks, remember when you set up that page for me to go to and download the format disc ? Could you tell me how to do that ? Or do you know some other way to download big files ? Because they can download stuff directly from the internet. Any help would be great, my prey ... *teasing kiss*\n",
      "1514\n",
      "Hows the champ just leaving glasgow!\n",
      "1515\n",
      "K:)all the best:)congrats...\n",
      "1516\n",
      "I wonder if you'll get this text?\n",
      "1519\n",
      "Shall i ask one thing if you dont mistake me.\n",
      "1520\n",
      "Check wid corect speling i.e. Sarcasm\n",
      "1522\n",
      "Are you angry with me. What happen dear\n",
      "1523\n",
      "I thk u dun haf 2 hint in e forum already lor... Cos i told ron n darren is going 2 tell shuhui.\n",
      "1524\n",
      "Yup ok thanx...\n",
      "1525\n",
      "Hi:)cts employee how are you?\n",
      "1526\n",
      "Pls pls find out from aunt nike.\n",
      "1527\n",
      "Wow ... I love you sooo much, you know ? I can barely stand it ! I wonder how your day goes and if you are well, my love ... I think of you and miss you\n",
      "1530\n",
      "Should I have picked up a receipt or something earlier\n",
      "1531\n",
      "I think chennai well settled?\n",
      "1532\n",
      "Oh dang! I didn't mean o send that to you! Lol!\n",
      "1533\n",
      "Unfortunately i've just found out that we have to pick my sister up from the airport that evening so don't think i'll be going out at all. We should try to go out one of th\n",
      "1534\n",
      "Horrible bf... I now v hungry...\n",
      "1537\n",
      "How's it feel? Mr. Your not my real Valentine just my yo Valentine even tho u hardly play!!\n",
      "1539\n",
      "Midnight at the earliest\n",
      "1540\n",
      "You're not sure that I'm not trying to make xavier smoke because I don't want to smoke after being told I smoke too much?\n",
      "1541\n",
      "K come to nordstrom when you're done\n",
      "1543\n",
      "Now press conference da:)\n",
      "1545\n",
      "After completed degree. There is no use in joining finance.\n",
      "1546\n",
      "Good afternoon, my love ! Any job prospects ? Are you missing me ? What do you do ? Are you being lazy and bleak, hmmm ? Or happy and filled with my love ?\n",
      "1547\n",
      "Shant disturb u anymore... Jia you...\n",
      "1548\n",
      "Bishan lar nearer... No need buy so early cos if buy now i gotta park my car...\n",
      "1549\n",
      "Me, i dont know again oh\n",
      "1550\n",
      "Dude sux for snake. He got old and raiden got buff\n",
      "1551\n",
      "He says hi and to get your ass back to south tampa (preferably at a kegger)\n",
      "1552\n",
      "In e msg jus now. U said thanks for gift.\n",
      "1553\n",
      "U too...\n",
      "1554\n",
      "Ok how you dear. Did you call chechi\n",
      "1555\n",
      "Yeah we do totes. When u wanna?\n",
      "1560\n",
      "Single line with a big meaning::::: \"Miss anything 4 ur \"Best Life\" but, don't miss ur best life for anything... Gud nyt...\n",
      "1562\n",
      "Dnt worry...use ice pieces in a cloth pack.also take 2 tablets.\n",
      "1563\n",
      "Dude just saw a parked car with its sunroof popped up. Sux\n",
      "1564\n",
      "Get ready to put on your excellent sub face :)\n",
      "1565\n",
      "Tmrw. Im finishing 9 doors\n",
      "1566\n",
      "The  &lt;#&gt; g that i saw a few days ago, the guy wants sell wifi only for  &lt;#&gt;  and with 3g for  &lt;#&gt; . That's why i blanked him.\n",
      "1567\n",
      "I am late. I will be there at\n",
      "1568\n",
      "whatever, im pretty pissed off.\n",
      "1572\n",
      "Near kalainar tv office.thenampet\n",
      "1575\n",
      "My sis is catching e show in e afternoon so i'm not watching w her. So c u wan 2 watch today or tmr lor.\n",
      "1577\n",
      "No. To be nosy I guess. Idk am I over reacting if I'm freaked?\n",
      "1578\n",
      "Remember all those whom i hurt during days of satanic imposter in me.need to pay a price,so be it.may destiny keep me going and as u said pray that i get the mind to get over the same.\n",
      "1580\n",
      "Why is that, princess? I bet the brothas are all chasing you!\n",
      "1582\n",
      "Hhahhaahahah rofl wtf nig was leonardo in your room or something\n",
      "1583\n",
      "Yep, at derek's house now, see you Sunday &lt;3\n",
      "1585\n",
      "Sorry, I'll call later\n",
      "1587\n",
      "There are no other charges after transfer charges and you can withdraw anyhow you like\n",
      "1589\n",
      "At 4. Let's go to bill millers\n",
      "1590\n",
      "I love you. You set my soul on fire. It is not just a spark. But it is a flame. A big rawring flame. XoXo\n",
      "1591\n",
      "Somewhr someone is surely made 4 u. And God has decided a perfect time to make u meet dat person. . . . till den, . . . . . Enjoy ur crushes..!!!;-)\n",
      "1596\n",
      "Pls confirm the time to collect the cheque.\n",
      "1599\n",
      "Daddy will take good care of you :)\n",
      "1600\n",
      "Yeah probably, I still gotta check out with leo\n",
      "1603\n",
      "Ok pa. Nothing problem:-)\n",
      "1605\n",
      "God picked up a flower and dippeditinaDEW, lovingly touched itwhichturnedinto u, and the he gifted tomeandsaid,THIS FRIEND IS 4U\n",
      "1607\n",
      "Ok no prob... I'll come after lunch then...\n",
      "1608\n",
      "Jus telling u dat i'll b leaving 4 shanghai on 21st instead so we'll haf more time 2 meet up cya...\n",
      "1609\n",
      "Are your freezing ? Are you home yet ? Will you remember to kiss your mom in the morning? Do you love me ? Do you think of me ? Are you missing me yet ?\n",
      "1611\n",
      "I'll probably be around mu a lot\n",
      "1614\n",
      "Thnx dude. u guys out 2nite?\n",
      "1615\n",
      "Me sef dey laugh you. Meanwhile how's my darling anjie!\n",
      "1616\n",
      "Mm i had my food da from out\n",
      "1618\n",
      "Did u download the fring app?\n",
      "1619\n",
      "The 2 oz guy is being kinda flaky but one friend is interested in picking up $ &lt;#&gt;  worth tonight if possible\n",
      "1620\n",
      "Friends that u can stay on fb chat with\n",
      "1621\n",
      "Fuck babe, I miss you sooooo much !! I wish you were here to sleep with me ... My bed is so lonely ... I go now, to sleep ... To dream of you, my love ...\n",
      "1622\n",
      "Living is very simple.. Loving is also simple.. Laughing is too simple.. Winning is tooo simple.. But, being 'SIMPLE' is very difficult.. Gud nte.:-\n",
      "1626\n",
      "Hi Dear Call me its urgnt. I don't know whats your problem. You don't want to work or if you have any other problem at least tell me. Wating for your reply.\n",
      "1627\n",
      "Dear how you. Are you ok?\n",
      "1629\n",
      "Yes princess! I want to make you happy...\n",
      "1630\n",
      "Sounds like you have many talents! would you like to go on a dinner date next week?\n",
      "1632\n",
      "We not watching movie already. Xy wants 2 shop so i'm shopping w her now.\n",
      "1633\n",
      "Hello my little party animal! I just thought I'd buzz you as you were with your friends ...*grins*... Reminding you were loved and send a naughty adoring kiss\n",
      "1637\n",
      "No shit, but I wasn't that surprised, so I went and spent the evening with that french guy I met in town here and we fooled around a bit but I didn't let him fuck me\n",
      "1639\n",
      "Great comedy..cant stop laughing da:)\n",
      "1641\n",
      "Alright, we're all set here, text the man\n",
      "1642\n",
      "Hi , where are you? We're at  and they're not keen to go out i kind of am but feel i shouldn't so can we go out tomo, don't mind do you?\n",
      "1644\n",
      "U WILL SWITCH YOUR FONE ON DAMMIT!!\n",
      "1645\n",
      "India have to take lead:)\n",
      "1646\n",
      "I.ll post her out l8r. In class\n",
      "1648\n",
      "Evening * v good if somewhat event laden. Will fill you in, don't you worry … Head * ok but throat * wrecked. See you at six then!\n",
      "1651\n",
      "I dont have any of your file in my bag..i was in work when you called me.i 'll tell you if i find anything in my room.\n",
      "1652\n",
      "I wan but too early lei... Me outside now wun b home so early... Neva mind then...\n",
      "1654\n",
      "I was at bugis juz now wat... But now i'm walking home oredi... Ü so late then reply... I oso saw a top dat i like but din buy... Where r ü now? \n",
      "1655\n",
      "Wishing you and your family Merry \"X\" mas and HAPPY NEW Year in advance..\n",
      "1658\n",
      "S:-)if we have one good partnership going we will take lead:)\n",
      "1660\n",
      "Yeah, where's your class at?\n",
      "1661\n",
      "No just send to you. Bec you in temple na.\n",
      "1662\n",
      "You aren't coming home between class, right? I need to work out and shower!\n",
      "1664\n",
      "S but mostly not like that.\n",
      "1665\n",
      "Ü v ma fan...\n",
      "1669\n",
      "Yes..but they said its IT.,\n",
      "1670\n",
      "Very hurting n meaningful lines ever: \"I compromised everything for my love, But at d end my love compromised me for everything:-(\".. Gud mornin:-)\n",
      "1671\n",
      "Lmao!nice 1\n",
      "1679\n",
      "So many people seems to be special at first sight, But only very few will remain special to you till your last sight.. Maintain them till life ends.. Sh!jas\n",
      "1680\n",
      "Today is \"song dedicated day..\" Which song will u dedicate for me? Send this to all ur valuable frnds but first rply me...\n",
      "1681\n",
      "Okay... We wait ah\n",
      "1682\n",
      "Y lei?\n",
      "1683\n",
      "HI BABE U R MOST LIKELY TO BE IN BED BUT IM SO SORRY ABOUT TONIGHT! I REALLY WANNA SEE U TOMORROW SO CALL ME AT 9. LOVE ME XXX\n",
      "1684\n",
      "Already am squatting is the new way of walking\n",
      "1686\n",
      "Cramps stopped. Going back to sleep\n",
      "1689\n",
      "Nan sonathaya soladha. Why boss?\n",
      "1690\n",
      "Bring tat cd don forget\n",
      "1692\n",
      "I don't know but I'm raping dudes at poker\n",
      "1693\n",
      "Weightloss! No more girl friends. Make loads of money on ebay or something. And give thanks to God.\n",
      "1694\n",
      "Was gr8 to see that message. So when r u leaving? Congrats dear. What school and wat r ur plans.\n",
      "1696\n",
      "Finish already... Yar they keep saying i mushy... I so embarrassed ok...\n",
      "1697\n",
      "Sorry man, my stash ran dry last night and I can't pick up more until sunday\n",
      "1698\n",
      "Hai priya are you right. What doctor said pa. Where are you.\n",
      "1701\n",
      "Please ask mummy to call father\n",
      "1702\n",
      "Can come my room but cannot come my house cos my house still messy... Haha...\n",
      "1703\n",
      "I have lost 10 kilos as of today!\n",
      "1704\n",
      "Just taste fish curry :-P\n",
      "1705\n",
      "What can i do? Might accidant tookplace between somewhere ghodbandar rd. Traffic moves slovely. So plz slip &amp; don't worry.\n",
      "1706\n",
      "Yun ah.now ü wkg where?btw if ü go nus sc. Ü wana specialise in wad?\n",
      "1707\n",
      "Yes! I am a one woman man! Please tell me your likes and dislikes in bed...\n",
      "1708\n",
      "Was doing my test earlier. I appreciate you. Will call you tomorrow.\n",
      "1709\n",
      "How's my loverboy doing ? What does he do that keeps him from coming to his Queen, hmmm ? Doesn't he ache to speak to me ? Miss me desparately ?\n",
      "1711\n",
      "(No promises on when though, haven't even gotten dinner yet)\n",
      "1712\n",
      "I got your back! Do you have any dislikes in bed?\n",
      "1713\n",
      "o turns out i had stereo love on mi phone under the unknown album.\n",
      "1715\n",
      "Yeah I don't see why not\n",
      "1717\n",
      "Sorry about earlier. Putting out fires.Are you around to talk after 9? Or do you actually have a life, lol!\n",
      "1719\n",
      "As in missionary hook up, doggy hook up, standing...|\n",
      "1720\n",
      "Then u better go sleep.. Dun disturb u liao.. U wake up then msg me lor..\n",
      "1721\n",
      "Fighting with the world is easy, u either win or lose bt fightng with some1 who is close to u is dificult if u lose - u lose if u win - u still lose.\n",
      "1723\n",
      "Thought praps you meant another one. Goodo! I'll look tomorrow \n",
      "1724\n",
      "Hi Jon, Pete here, Ive bin 2 Spain recently & hav sum dinero left, Bill said u or ur rents mayb interested in it, I hav 12,000pes, so around £48, tb, James.\n",
      "1725\n",
      "There bold 2  &lt;#&gt; . Is that yours\n",
      "1727\n",
      "ALRITE HUNNY!WOT U UP 2 2NITE? DIDNT END UP GOIN DOWN TOWN JUS DA PUB INSTEAD! JUS CHILLIN AT DA MO IN ME BEDROOM!LOVE JEN XXX.\n",
      "1728\n",
      "I went to project centre\n",
      "1729\n",
      "As per your request 'Maangalyam (Alaipayuthe)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "1731\n",
      "Doing project w frens lor. \n",
      "1733\n",
      "K, can that happen tonight?\n",
      "1735\n",
      "I think we're going to finn's now, come\n",
      "1737\n",
      "I will come tomorrow di\n",
      "1739\n",
      "K go and sleep well. Take rest:-).\n",
      "1742\n",
      "I can do that! I want to please you both inside and outside the bedroom...\n",
      "1743\n",
      "EY! CALM DOWNON THEACUSATIONS.. ITXT U COS IWANA KNOW WOTU R DOIN AT THEW/END... HAVENTCN U IN AGES..RING ME IF UR UP4 NETHING SAT.LOVE J XXX.\n",
      "1744\n",
      "I love to wine and dine my lady!\n",
      "1747\n",
      "I don know account details..i will ask my mom and send you.my mom is out of reach now.\n",
      "1749\n",
      "Feel Yourself That You Are Always Happy.. Slowly It Becomes Your Habit &amp; Finally It Becomes Part Of Your Life.. Follow It.. Happy Morning &amp; Have A Happy Day:)\n",
      "1751\n",
      "Got it..mail panren paru..\n",
      "1755\n",
      "How is your schedule next week? I am out of town this weekend.\n",
      "1756\n",
      "Really good:)dhanush rocks once again:)\n",
      "1757\n",
      "Lmao ok I wont be needing u to do my hair anymore.\n",
      "1758\n",
      "Miss ya, need ya, want ya, love ya.\n",
      "1759\n",
      "Sorry i'm not free...\n",
      "1760\n",
      "Do u ever get a song stuck in your head for no reason and it won't go away til u listen to it like 5 times?\n",
      "1761\n",
      "Nt yet chikku..simple habba..hw abt u?\n",
      "1762\n",
      "Got ur mail Dileep.thank you so muchand look forward to lots of support...very less contacts here,remember one venugopal you mentioned.tomorrow if not late,i shall try to come up till there.goodnight dear.\n",
      "1763\n",
      "Sometimes Heart Remembrs someone Very much... Forgets someone soon... Bcoz Heart will not like everyone. But liked ones will be Remembered Everytime... BSLVYL\n",
      "1768\n",
      "K, want us to come by now?\n",
      "1769\n",
      "How. Its a little difficult but its a simple way to enter this place\n",
      "1770\n",
      "Ha... Both of us doing e same thing. But i got tv 2 watch. U can thk of where 2 go tonight or u already haf smth in mind...\n",
      "1771\n",
      "Dont show yourself. How far. Put new pictures up on facebook.\n",
      "1772\n",
      "Watching tv now. I got new job :)\n",
      "1773\n",
      "Good afternoon sexy buns! How goes the job search ? I wake and you are my first thought as always, my love. I wish your fine and happy and know I adore you!\n",
      "1774\n",
      "I'm not coming over, do whatever you want\n",
      "1775\n",
      "Its ok chikku, and its my 1 of favourite song..:-)\n",
      "1776\n",
      "Did u see what I posted on your Facebook?\n",
      "1779\n",
      "7 wonders in My WORLD 7th You 6th Ur style 5th Ur smile 4th Ur Personality 3rd Ur Nature 2nd Ur SMS and 1st \"Ur Lovely Friendship\"... good morning dear\n",
      "1782\n",
      ";-( oh well, c u later\n",
      "1786\n",
      "I dun believe u. I thk u told him.\n",
      "1787\n",
      "Do you know why god created gap between your fingers..? So that, One who is made for you comes &amp; fills those gaps by holding your hand with LOVE..!\n",
      "1790\n",
      "Takin a shower now but yeah I'll leave when I'm done\n",
      "1791\n",
      "Am not working but am up to eyes in philosophy so will text u later when a bit more free for chat...\n",
      "1792\n",
      "U havent lost me ill always b here 4u.i didnt intend 2 hurt u but I never knew how u felt about me when Iwas+marine&thats what itried2tell urmom.i careabout u\n",
      "1794\n",
      "You bad girl. I can still remember them\n",
      "1797\n",
      "Hey, can you tell me blake's address? Carlos wanted me to meet him there but I got lost and he's not answering his phone\n",
      "1798\n",
      "Can i get your opinion on something first?\n",
      "1799\n",
      "That one week leave i put know that time. Why.\n",
      "1801\n",
      "excellent. I spent  &lt;#&gt;  years in the Air Force. Iraq and afghanistan. I am stable and honest. do you like traveling?\n",
      "1803\n",
      "Ok lor thanx... Ü in school?\n",
      "1804\n",
      "I'm in class. Did you get my text.\n",
      "1806\n",
      "God bless.get good sleep my dear...i will pray!\n",
      "1809\n",
      "Aiyo a bit pai seh ü noe... Scared he dun rem who i am then die... Hee... But he become better lookin oredi leh...\n",
      "1810\n",
      "Aight, I'll ask a few of my roommates\n",
      "1811\n",
      "Now, whats your house # again ? And do you have any beer there ?\n",
      "1812\n",
      "Do ü all wan 2 meet up n combine all the parts? How's da rest of da project going?\n",
      "1813\n",
      "Getting tickets 4 walsall tue 6 th march. My mate is getting me them on sat. ill pay my treat. Want 2 go. Txt bak .Terry\n",
      "1814\n",
      "Yes we are chatting too.\n",
      "1815\n",
      "HI ITS JESS I DONT KNOW IF YOU ARE AT WORK BUT CALL ME WHEN U CAN IM AT HOME ALL EVE. XXX\n",
      "1821\n",
      "I'll probably be by tomorrow (or even later tonight if something's going on)\n",
      "1827\n",
      "Dude. What's up. How Teresa. Hope you have been okay. When i didnt hear from these people, i called them and they had received the package since dec  &lt;#&gt; . Just thot you'ld like to know. Do have a fantastic year and all the best with your reading. Plus if you can really really Bam first aid for Usmle, then your work is done.\n",
      "1831\n",
      "That's the way you should stay oh.\n",
      "1832\n",
      "Hello- thanx for taking that call. I got a job! Starts on monday!\n",
      "1833\n",
      "What time is ur flight tmr?\n",
      "1834\n",
      "When should I come over?\n",
      "1835\n",
      "I have a rather prominent bite mark on my right cheek\n",
      "1836\n",
      "* Will be september by then!\n",
      "1837\n",
      "Are you wet right now?\n",
      "1840\n",
      "Are we doing the norm tomorrow? I finish just a 4.15 cos of st tests. Need to sort library stuff out at some point tomo - got letter from today - access til end march so i better get move on!\n",
      "1841\n",
      "Yeah. I got a list with only u and Joanna if I'm feeling really anti social\n",
      "1846\n",
      "Hi. || Do u want | to join me with sts later? || Meeting them at five. || Call u after class.\n",
      "1847\n",
      "Its on in engalnd! But telly has decided it won't let me watch it and mia and elliot were kissing! Damn it!\n",
      "1849\n",
      "I dont want to hear philosophy. Just say what happen\n",
      "1850\n",
      "You got job in wipro:)you will get every thing in life in 2 or 3 years.\n",
      "1852\n",
      "Dunno da next show aft 6 is 850. Toa payoh got 650.\n",
      "1854\n",
      "I just made some payments so dont have that much. Sorry. Would you want it fedex or the other way.\n",
      "1855\n",
      "They did't play one day last year know even though they have very good team.. Like india.\n",
      "1856\n",
      "K.:)you are the only girl waiting in reception ah?\n",
      "1858\n",
      "I hate when she does this. She turns what should be a fun shopping trip into an annoying day of how everything would look in her house.\n",
      "1859\n",
      "Sir, i am waiting for your call.\n",
      "1860\n",
      "What's up. Do you want me to come online?\n",
      "1861\n",
      "It could work, we'll reach a consensus at the next meeting\n",
      "1862\n",
      "Aiyah then i wait lor. Then u entertain me. Hee...\n",
      "1864\n",
      "I'll let you know when it kicks in\n",
      "1865\n",
      "You call him now ok i said call him\n",
      "1870\n",
      "Mom wants to know where you at\n",
      "1871\n",
      "Aight, I'll text you when I'm back\n",
      "1872\n",
      "Dont know supports ass and srt i thnk. I think ps3 can play through usb too\n",
      "1873\n",
      "Oh ok i didnt know what you meant. Yep i am baby jontin\n",
      "1877\n",
      "Watching tv now. I got new job :)\n",
      "1878\n",
      "This pen thing is beyond a joke. Wont a Biro do? Don't do a masters as can't do this ever again! \n",
      "1881\n",
      "Just seeing your missed call my dear brother. Do have a gr8 day.\n",
      "1883\n",
      "Sorry, I can't help you on this.\n",
      "1884\n",
      "Come to me, slave. Your doing it again ... Going into your shell and unconsciously avoiding me ... You are making me unhappy :-(\n",
      "1885\n",
      "I love your ass! Do you enjoy doggy style? :)\n",
      "1886\n",
      "I think asking for a gym is the excuse for lazy people. I jog.\n",
      "1889\n",
      "No. On the way home. So if not for the long dry spell the season would have been over\n",
      "1891\n",
      "Ok but knackered. Just came home and went to sleep! Not good at this full time work lark.\n",
      "1894\n",
      "Good Morning plz call me sir\n",
      "1897\n",
      "I tot u outside cos darren say u come shopping. Of course we nice wat. We jus went sim lim look at mp3 player.\n",
      "1898\n",
      "Aight, sounds good. When do you want me to come down?\n",
      "1899\n",
      "Wat would u like 4 ur birthday?\n",
      "1900\n",
      "I love working from home :)\n",
      "1901\n",
      "And miss vday the parachute and double coins??? U must not know me very well...\n",
      "1905\n",
      "Wah... Okie okie... Muz make use of e unlimited... Haha... \n",
      "1906\n",
      "There're some people by mu, I'm at the table by lambda\n",
      "1908\n",
      "ELLO BABE U OK?\n",
      "1909\n",
      "Hello beautiful r u ok? I've kinda ad a row wiv and he walked out the pub?? I wanted a night wiv u Miss u \n",
      "1913\n",
      "For real tho this sucks. I can't even cook my whole electricity is out. And I'm hungry.\n",
      "1914\n",
      "You want to go? \n",
      "1916\n",
      "Its not that time of the month nor mid of the time?\n",
      "1917\n",
      "Fffff. Can you text kadeem or are you too far gone\n",
      "1919\n",
      "Is fujitsu s series lifebook good?\n",
      "1920\n",
      "Yar i wanted 2 scold u yest but late already... I where got zhong se qing you? If u ask me b4 he ask me then i'll go out w u all lor. N u still can act so real.\n",
      "1921\n",
      "Dont know you bring some food\n",
      "1923\n",
      "I'll be in sch fr 4-6... I dun haf da book in sch... It's at home...\n",
      "1927\n",
      "Dont give a monkeys wot they think and i certainly don't mind. Any friend of mine&all that! Just don't sleep wiv , that wud be annoyin!\n",
      "1928\n",
      "Omg it could snow here tonite!\n",
      "1932\n",
      "What pa tell me.. I went to bath:-)\n",
      "1933\n",
      "Jus finished avatar nigro\n",
      "1934\n",
      "R u over scratching it?\n",
      "1935\n",
      "Hope you are having a great day.\n",
      "1936\n",
      "Did either of you have any idea's? Do you know of anyplaces doing something?\n",
      "1938\n",
      "The fact that you're cleaning shows you know why i'm upset. Your priority is constantly \"what i want to do,\" not \"what i need to do.\"\n",
      "1939\n",
      "Excellent! Are you ready to moan and scream in ecstasy?\n",
      "1941\n",
      "Dude avatar 3d was imp. At one point i thought there were actually flies in the room and almost tried hittng one as a reflex\n",
      "1945\n",
      "Yeah, I'll leave in a couple minutes &amp; let you know when I get to mu\n",
      "1946\n",
      "Can ü call me at 10:10 to make sure dat i've woken up...\n",
      "1950\n",
      "Wait 2 min..stand at bus stop\n",
      "1957\n",
      "K...k:)why cant you come here and search job:)\n",
      "1959\n",
      "Lol ... Oh no babe, I wont be sliding into your place after midnight, but thanks for the invite\n",
      "1960\n",
      "Howz that persons story\n",
      "1964\n",
      "Yes :)it completely in out of form:)clark also utter waste.\n",
      "1965\n",
      "Honeybee Said: *I'm d Sweetest in d World* God Laughed &amp; Said: *Wait,U Havnt Met d Person Reading This Msg* MORAL: Even GOD Can Crack Jokes! GM+GN+GE+GN:)\n",
      "1966\n",
      "Thanks. It was only from tescos but quite nice. All gone now. Speak soon \n",
      "1967\n",
      "What's a feathery bowa? Is that something guys have that I don't know about?\n",
      "1968\n",
      "Even i cant close my eyes you are in me our vava playing umma :-D\n",
      "1969\n",
      "2 laptop... I noe infra but too slow lar... I wan fast one\n",
      "1971\n",
      "Nvm it's ok...\n",
      "1974\n",
      "I had askd u a question some hours before. Its answer\n",
      "1975\n",
      "Thats cool. Where should i cum? On you or in you? :)\n",
      "1981\n",
      "Sorry, I'll call later\n",
      "1982\n",
      "Sorry, I'll call later in meeting any thing related to trade please call Arul. &lt;#&gt; \n",
      "1986\n",
      "The length is e same but e top shorter n i got a fringe now. I thk i'm not going liao. Too lazy. Dun wan 2 distract u also.\n",
      "1987\n",
      "S..antha num corrct dane\n",
      "1990\n",
      "The basket's gettin full so I might be by tonight\n",
      "1991\n",
      "HI DARLIN IVE JUST GOT BACK AND I HAD A REALLY NICE NIGHT AND THANKS SO MUCH FOR THE LIFT SEE U TOMORROW XXX\n",
      "1994\n",
      "Eh den sat u book e kb liao huh...\n",
      "1995\n",
      "Have you been practising your curtsey?\n",
      "2004\n",
      "S....s...india going to draw the series after many years in south african soil..\n",
      "2007\n",
      "Shopping lor. Them raining mah hard 2 leave orchard.\n",
      "2008\n",
      "Hi here. have birth at on the  to  at 8lb 7oz. Mother and baby doing brilliantly.\n",
      "2009\n",
      "See the forwarding message for proof\n",
      "2011\n",
      "Dunno lei... I thk mum lazy to go out... I neva ask her yet...\n",
      "2013\n",
      "Beautiful Truth against Gravity.. Read carefully: \"Our heart feels light when someone is in it.. But it feels very heavy when someone leaves it..\" GOODMORNING\n",
      "2015\n",
      "Ambrith..madurai..met u in arun dha marrge..remembr?\n",
      "2017\n",
      "Princess, is your kitty shaved or natural?\n",
      "2020\n",
      "From tomorrow onwards eve 6 to 3 work.\n",
      "2025\n",
      "U having lunch alone? I now so bored...\n",
      "2027\n",
      "Nah man, my car is meant to be crammed full of people\n",
      "2028\n",
      "No got new job at bar in airport on satsgettin 4.47per hour but means no lie in! keep in touch\n",
      "2029\n",
      "Kallis is ready for bat in 2nd innings\n",
      "2031\n",
      "Ugh y can't u just apologize, admit u were wrong and ask me to take u back?\n",
      "2032\n",
      "I noe la... U wana pei bf oso rite... K lor, other days den...\n",
      "2034\n",
      "IM GONNA MISS U SO MUCH\n",
      "2035\n",
      "Is avatar supposed to have subtoitles\n",
      "2036\n",
      "Simply sitting and watching match in office..\n",
      "2037\n",
      "You can jot down things you want to remember later.\n",
      "2038\n",
      "Oh sorry please its over\n",
      "2039\n",
      "Hey are we going for the lo lesson or gym? \n",
      "2040\n",
      "Dont pack what you can buy at any store.like cereals. If you must pack food, pack gari or something 9ja that you will miss.\n",
      "2041\n",
      "You always make things bigger than they are\n",
      "2042\n",
      "Ü dun wan to watch infernal affair?\n",
      "2046\n",
      "Okay... I booked all already... Including the one at bugis.\n",
      "2048\n",
      "No de. But call me after some time. Ill tell you k\n",
      "2049\n",
      "So dont use hook up any how\n",
      "2050\n",
      "How much is blackberry bold2 in nigeria.\n",
      "2051\n",
      "Hi where you. You in home or calicut?\n",
      "2052\n",
      "Hey darlin.. i can pick u up at college if u tell me wen & where 2 mt.. love Pete xx\n",
      "2054\n",
      "Oh... I was thkin of goin yogasana at 10 den no nd to go at 3 den can rush to parco 4 nb... Okie lor, u call me when ready...\n",
      "2055\n",
      "Y so late but i need to go n get da laptop...\n",
      "2056\n",
      "Sir, I am waiting for your mail.\n",
      "2059\n",
      "Ugh fuck it I'm resubbing to eve\n",
      "2060\n",
      "He didn't see his shadow. We get an early spring yay\n",
      "2061\n",
      "I did. One slice and one breadstick. Lol\n",
      "2063\n",
      "Is there any training tomorrow?\n",
      "2065\n",
      "Pass dis to all ur contacts n see wat u get! Red;i'm in luv wid u. Blue;u put a smile on my face. Purple;u r realy hot. Pink;u r so swt. Orange;i thnk i lyk u. Green;i realy wana go out wid u. Yelow;i wnt u bck. Black;i'm jealous of u. Brown;i miss you Nw plz giv me one color\n",
      "2066\n",
      "Cos daddy arranging time c wat time fetch ü mah...\n",
      "2067\n",
      "Then. You are eldest know.\n",
      "2069\n",
      "Its hard to believe things like this. All can say lie but think twice before saying anything to me.\n",
      "2072\n",
      "Good night my dear.. Sleepwell&amp;Take care\n",
      "2073\n",
      "That is wondarfull song\n",
      "2075\n",
      "Yar lor actually we quite fast... Cos da ge slow wat... Haha...\n",
      "2076\n",
      "Must come later.. I normally bathe him in da afternoon mah..\n",
      "2082\n",
      "I'm aight. Wat's happening on your side.\n",
      "2083\n",
      "I'm done oredi...\n",
      "2085\n",
      "How are you. Wish you a great semester\n",
      "2087\n",
      "Dude how do you like the buff wind.\n",
      "2091\n",
      "S:-)kallis wont play in first two odi:-)\n",
      "2092\n",
      "Then get some cash together and I'll text jason\n",
      "2096\n",
      "Probably, want to pick up more?\n",
      "2098\n",
      "Are you the cutest girl in the world or what\n",
      "2099\n",
      "No dice, art class 6 thru 9 :( thanks though. Any idea what time I should come tomorrow?\n",
      "2101\n",
      "Oh Howda gud gud.. Mathe en samachara chikku:-)\n",
      "2102\n",
      "I thk 530 lor. But dunno can get tickets a not. Wat u doing now?\n",
      "2103\n",
      "Audrie lousy autocorrect\n",
      "2108\n",
      "Hmmm ... And imagine after you've come home from that having to rub my feet, make me dinner and help me get ready for my date ! Are you sure your ready for that kind of life ?\n",
      "2110\n",
      "Lara said she can loan me  &lt;#&gt; .\n",
      "2111\n",
      "Do we have any spare power supplies\n",
      "2114\n",
      "Yeah, don't go to bed, I'll be back before midnight\n",
      "2118\n",
      "Wish u many many returns of the day.. Happy birthday vikky..\n",
      "2120\n",
      "I hope you know I'm still mad at you.\n",
      "2121\n",
      "Argh my 3g is spotty, anyway the only thing I remember from the research we did was that province and sterling were the only problem-free places we looked at\n",
      "2122\n",
      "In xam hall boy asked girl Tell me the starting term for dis answer I can den manage on my own After lot of hesitation n lookin around silently she said THE! intha ponnungale ipaditan;)\n",
      "2123\n",
      "Do you know when the result.\n",
      "2127\n",
      "You do got a shitload of diamonds though\n",
      "2128\n",
      "Tessy..pls do me a favor. Pls convey my birthday wishes to Nimya..pls dnt forget it. Today is her birthday Shijas\n",
      "2129\n",
      "Well I'm going to be an aunty!\n",
      "2130\n",
      "Mine here like all fr china then so noisy.\n",
      "2131\n",
      "Later i guess. I needa do mcat study too.\n",
      "2132\n",
      "S...from the training manual it show there is no tech process:)its all about password reset and troubleshooting:)\n",
      "2134\n",
      "Spoke with uncle john today. He strongly feels that you need to sacrifice to keep me here. He's going to call you. When he does, i beg you to just listen. Dont make any promises or make it clear things are not easy. And i need you to please let us work things out. As long as i keep expecting help, my creativity will be stifled so pls just keep him happy, no promises on your part.\n",
      "2136\n",
      "Carlos took a while (again), we leave in a minute\n",
      "2137\n",
      "Well done and ! luv ya all \n",
      "2138\n",
      "Then why you came to hostel.\n",
      "2139\n",
      "K still are you loving me.\n",
      "2140\n",
      "But i juz remembered i gotta bathe my dog today..\n",
      "2142\n",
      "Alright took the morphine. Back in yo.\n",
      "2147\n",
      "So can collect ur laptop?\n",
      "2149\n",
      "I will once i get home\n",
      "2151\n",
      "The table's occupied, I'm waiting by the tree\n",
      "2155\n",
      "Oh god i am happy to see your message after 3 days\n",
      "2158\n",
      "Sad story of a Man - Last week was my b'day. My Wife did'nt wish me. My Parents forgot n so did my Kids . I went to work. Even my Colleagues did not wish. As I entered my cabin my PA said, '' Happy B'day Boss !!''. I felt special. She askd me 4 lunch. After lunch she invited me to her apartment. We went there. She said,'' do u mind if I go into the bedroom for a minute ? '' ''OK'', I sed in a sexy mood. She came out 5 minuts latr wid a cake...n My Wife, My Parents, My Kidz, My Friends n My Colleagues. All screaming.. SURPRISE !! and I was waiting on the sofa.. ... ..... ' NAKED...!\n",
      "2162\n",
      "Is she replying. Has boye changed his phone number\n",
      "2167\n",
      "Thank you. And by the way, I just lost.\n",
      "2171\n",
      "CAN I PLEASE COME UP NOW IMIN TOWN.DONTMATTER IF URGOIN OUTL8R,JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIGNORE MYCALLS,U NO THECD ISV.IMPORTANT TOME 4 2MORO\n",
      "2172\n",
      "I wont. So wat's wit the guys\n",
      "2173\n",
      "Yavnt tried yet and never played original either\n",
      "2176\n",
      "I'm at work. Please call\n",
      "2177\n",
      "get ready to moan and scream :)\n",
      "2178\n",
      "Oh k :)why you got job then whats up?\n",
      "2179\n",
      "I don,t think so. You don't need to be going out that late on a school night. ESPECIALLY when the one class you have is the one you missed last wednesday and probably failed a test in on friday\n",
      "2180\n",
      "And popping &lt;#&gt; ibuprofens was no help.\n",
      "2181\n",
      "Babe ! How goes that day ? What are you doing ? Where are you ? I sip my cappuccino and think of you, my love ... I send a kiss to you from across the sea\n",
      "2182\n",
      "Ok.\n",
      "2183\n",
      "PS U no ur a grown up now right?\n",
      "2185\n",
      "I know a few people I can hit up and fuck to the yes\n",
      "2187\n",
      "So is there anything specific I should be doing with regards to jaklin or what because idk what the fuck\n",
      "2188\n",
      "Oh god. I'm gonna Google nearby cliffs now.\n",
      "2192\n",
      "Thankyou so much for the call. I appreciate your care.\n",
      "2193\n",
      "Congrats ! Treat pending.i am not on mail for 2 days.will mail once thru.Respect mother at home.check mails.\n",
      "2195\n",
      "Hi my email address has changed now it is \n",
      "2196\n",
      "V-aluable. A-ffectionate. L-oveable. E-ternal. N-oble. T-ruthful. I-ntimate. N-atural. E-namous. Happy \"VALENTINES DAY\" in advance\n",
      "2197\n",
      "Not much, just some textin'. How bout you?\n",
      "2201\n",
      "Haha... can... But i'm having dinner with my cousin...\n",
      "2202\n",
      "A boy was late 2 home. His father: \"POWER OF FRNDSHIP\"\n",
      "2203\n",
      "(And my man carlos is definitely coming by mu tonight, no excuses)\n",
      "2205\n",
      "Raji..pls do me a favour. Pls convey my Birthday wishes to Nimya. Pls. Today is her birthday.\n",
      "2208\n",
      "Usually the body takes care of it buy making sure it doesnt progress. Can we pls continue this talk on saturday.\n",
      "2210\n",
      "Hmm well, night night \n",
      "2211\n",
      "Just wanted to say holy shit you guys weren't kidding about this bud\n",
      "2212\n",
      "Just gettin a bit arty with my collages at the mo, well tryin 2 ne way! Got a roast in a min lovely i shall enjoy that!\n",
      "2214\n",
      "Goodmorning, today i am late for 2hrs. Because of back pain.\n",
      "2215\n",
      "Ok then i'll let him noe later n ask him call u tmr...\n",
      "2216\n",
      "Prabha..i'm soryda..realy..frm heart i'm sory\n",
      "2217\n",
      "OK i'm waliking ard now... Do u wan me 2 buy anything go ur house?\n",
      "2218\n",
      "* Will have two more cartons off u and is very pleased with shelves\n",
      "2219\n",
      "Nice talking to you! please dont forget my pix :) i want to see all of you...\n",
      "2222\n",
      "I notice you like looking in the shit mirror youre turning into a right freak\n",
      "2226\n",
      "Alrite jod hows the revision goin? Keris bin doin a smidgin. N e way u wanna cum over after college?xx\n",
      "2228\n",
      "Oh k.k..where did you take test?\n",
      "2229\n",
      "Those were my exact intentions\n",
      "2230\n",
      "haha but no money leh... Later got to go for tuition... Haha and looking for empty slots for driving lessons\n",
      "2231\n",
      "Hey... Thk we juz go accordin to wat we discussed yest lor, except no kb on sun... Cos there's nt much lesson to go if we attend kb on sat...\n",
      "2232\n",
      "K, wen ur free come to my home and also tel vikky i hav sent mail to him also.. Better come evening il be free today aftr 6pm..:-)\n",
      "2234\n",
      "Good Morning plz call me sir\n",
      "2235\n",
      "What's your room number again? Wanna make sure I'm knocking on the right door\n",
      "2237\n",
      "Pls tell nelson that the bb's are no longer comin. The money i was expecting aint coming\n",
      "2238\n",
      "Give her something to drink, if she takes it and doesn't vomit then you her temp might drop. If she unmits however let me know.\n",
      "2239\n",
      "Think you sent the text to the home phone. That cant display texts. If you still want to send it his number is\n",
      "2240\n",
      "Every day i use to sleep after  &lt;#&gt;  so only.\n",
      "2241\n",
      "K I'll call you when I'm close\n",
      "2242\n",
      "U buy newspapers already?\n",
      "2245\n",
      "No management puzzeles.\n",
      "2246\n",
      "How did you find out in a way that didn't include all of these details\n",
      "2249\n",
      "will you like to be spoiled? :)\n",
      "2251\n",
      "I am getting threats from your sales executive Shifad as i raised complaint against him. Its an official message.\n",
      "2252\n",
      "hope things went well at 'doctors' ;) reminds me i still need 2go.did u c d little thing i left in the lounge?\n",
      "2254\n",
      "Lol enjoy role playing much?\n",
      "2255\n",
      "Ok. Me watching tv too.\n",
      "2263\n",
      "It should take about  &lt;#&gt;  min\n",
      "2265\n",
      "Ok . . now i am in bus. . If i come soon i will come otherwise tomorrow\n",
      "2266\n",
      "I cant pick the phone right now. Pls send a message\n",
      "2268\n",
      "Finish liao... U?\n",
      "2270\n",
      "Haha i think i did too\n",
      "2275\n",
      "Think I could stop by in like an hour or so? My roommate's looking to stock up for a trip\n",
      "2276\n",
      "Is that on the telly? No its Brdget Jones!\n",
      "2277\n",
      "Love you aathi..love u lot..\n",
      "2279\n",
      "Hmm...Bad news...Hype park plaza $700 studio taken...Only left 2 bedrm-$900...\n",
      "2282\n",
      "I hav almost reached. Call, i m unable to connect u.\n",
      "2284\n",
      "I reach home safe n sound liao...\n",
      "2285\n",
      "Velly good, yes please!\n",
      "2287\n",
      "I have had two more letters from . I will copy them for you cos one has a message for you. Speak soon\n",
      "2288\n",
      "Alex knows a guy who sells mids but he's down in south tampa and I don't think I could set it up before like 8\n",
      "2289\n",
      "Dont you have message offer\n",
      "2292\n",
      "Remind me how to get there and I shall do so\n",
      "2293\n",
      ":-( that's not v romantic!\n",
      "2298\n",
      "Draw va?i dont think so:)\n",
      "2299\n",
      "Dont pick up d call when something important is There to tell. Hrishi\n",
      "2301\n",
      "Nothin comes to my mind. Ü help me buy hanger lor. Ur laptop not heavy?\n",
      "2302\n",
      "&lt;#&gt; , that's all? Guess that's easy enough\n",
      "2303\n",
      "We can make a baby in yo tho\n",
      "2304\n",
      "Should I tell my friend not to come round til like  &lt;#&gt; ish?\n",
      "2314\n",
      "So what do you guys do.\n",
      "2315\n",
      "Also that chat was awesome but don't make it regular unless you can see her in person\n",
      "2319\n",
      "On the way to office da..\n",
      "2320\n",
      "In which place do you want da.\n",
      "2323\n",
      "Should I be stalking u?\n",
      "2326\n",
      "Apps class varaya elaya.\n",
      "2330\n",
      "Am surfing online store. For offers do you want to buy any thing.\n",
      "2331\n",
      "Long beach lor. Expected... U having dinner now?\n",
      "2332\n",
      "At home by the way\n",
      "2334\n",
      "What happen to her tell the truth\n",
      "2335\n",
      "Do you like Italian food?\n",
      "2336\n",
      "Which is weird because I know I had it at one point\n",
      "2337\n",
      "Aww you must be nearly dead!Well Jez isComing over toDo some workAnd that whillTake forever!\n",
      "2339\n",
      "Alright, see you in a bit\n",
      "2341\n",
      "I will take care of financial problem.i will help:)\n",
      "2342\n",
      "Tell dear what happen to you. Why you talking to me like an alian\n",
      "2344\n",
      "1) Go to write msg 2) Put on Dictionary mode 3)Cover the screen with hand, 4)Press  &lt;#&gt; . 5)Gently remove Ur hand.. Its interesting..:)\n",
      "2345\n",
      "Okie...\n",
      "2346\n",
      "Hi this is yijue, can i meet u at 11 tmr?\n",
      "2347\n",
      "Its posible dnt live in  &lt;#&gt; century cm frwd n thnk different\n",
      "2348\n",
      "But i dint slept in afternoon.\n",
      "2349\n",
      "That seems unnecessarily affectionate\n",
      "2351\n",
      "You will be in the place of that man\n",
      "2353\n",
      "Thats cool. How was your day?\n",
      "2355\n",
      "R we going with the  &lt;#&gt;  bus?\n",
      "2356\n",
      "Hello, my love ! How went your day ? Are you alright ? I think of you, my sweet and send a jolt to your heart to remind you ... I LOVE YOU! Can you hear it ? I screamed it across the sea for all the world to hear. Ahmad al Hallaq is loved ! and owned ! *possessive passionate kiss*\n",
      "2358\n",
      "Okay same with me. Well thanks for the clarification\n",
      "2359\n",
      "I'll talk to the others and probably just come early tomorrow then\n",
      "2361\n",
      "Had the money issue weigh me down but thanks to you, I can breathe easier now. I.ll make sure you dont regret it. Thanks.\n",
      "2371\n",
      "That day ü say ü cut ur hair at paragon, is it called hair sense? Do ü noe how much is a hair cut? \n",
      "2372\n",
      "Hmm, too many of them unfortunately... Pics obviously arent hot cakes. Its kinda fun tho\n",
      "2373\n",
      "Watching tv lor... Y she so funny we bluff her 4 wat. Izzit because she thk it's impossible between us?\n",
      "2375\n",
      "Dunno lei he neva say...\n",
      "2376\n",
      "Thanx 4 2day! U r a goodmate I THINK UR RITE SARY! ASUSUAL!1 U CHEERED ME UP! LOVE U FRANYxxxxx\n",
      "2377\n",
      "I'm on my way home. Went to change batt 4 my watch then go shop a bit lor.\n",
      "2379\n",
      "Hi, Mobile no.  &lt;#&gt;  has added you in their contact list on www.fullonsms.com It s a great place to send free sms to people For more visit fullonsms.com\n",
      "2384\n",
      "Your pussy is perfect!\n",
      "2387\n",
      "No message..no responce..what happend?\n",
      "2388\n",
      "Also where's the piece\n",
      "2389\n",
      "wiskey Brandy Rum Gin Beer Vodka Scotch Shampain Wine \"KUDI\"yarasu dhina vaazhthukkal. ..\n",
      "2391\n",
      "First has she gained more than  &lt;#&gt; kg since she took in. Second has she done the blood sugar tests. If she has and its ok and her blood pressure is within normal limits then no worries\n",
      "2395\n",
      "I don't run away frm u... I walk slowly &amp; it kills me that u don't care enough to stop me...\n",
      "2396\n",
      "Babe, I'm back ... Come back to me ...\n",
      "2397\n",
      "Well you told others you'd marry them...\n",
      "2398\n",
      "Neshanth..tel me who r u?\n",
      "2399\n",
      "YO YO YO BYATCH WHASSUP?\n",
      "2400\n",
      "Oh... Kay... On sat right?\n",
      "2401\n",
      "Hi! This is Roger from CL. How are you?\n",
      "2403\n",
      "Oh oh... Wasted... Den muz chiong on sat n sun liao...\n",
      "2404\n",
      "Jesus christ bitch I'm trying to give you drugs answer your fucking phone\n",
      "2407\n",
      "One of best dialogue in cute reltnship..!! \"Wen i Die, Dont Come Near My Body..!! Bcoz My Hands May Not Come 2 Wipe Ur Tears Off That Time..!Gud ni8\n",
      "2408\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "2409\n",
      "Dear where you will be when i reach there\n",
      "2410\n",
      "Aww that's the first time u said u missed me without asking if I missed u first. You DO love me! :)\n",
      "2414\n",
      "Lol please do. Actually send a pic of yourself right now. I wanna see. Pose with a comb and hair dryer or something.\n",
      "2415\n",
      "O was not into fps then.\n",
      "2416\n",
      "Huh means computational science... Y they like dat one push here n there...\n",
      "2417\n",
      "Could you not read me, my Love ? I answered you\n",
      "2418\n",
      "Oh... Lk tt den we take e one tt ends at cine lor... Dun wan yogasana oso can... \n",
      "2419\n",
      "Madam,regret disturbance.might receive a reference check from DLF Premarica.kindly be informed.Rgds,Rakhesh,Kerala.\n",
      "2422\n",
      "Err... Cud do. I'm going to  at 8pm. I haven't got a way to contact him until then.\n",
      "2424\n",
      "Lmao but its so fun...\n",
      "2426\n",
      "Hey!!! I almost forgot ... Happy B-day babe ! I love ya!!\n",
      "2428\n",
      "Do you think i can move  &lt;#&gt;  in a week\n",
      "2431\n",
      "How was txting and driving\n",
      "2432\n",
      "That's good. Lets thank God. Please complete the drug. Have lots of water. And have a beautiful day.\n",
      "2433\n",
      "Really dun bluff me leh... U sleep early too. Nite...\n",
      "2435\n",
      "Uncle boye. I need movies oh. Guide me. Plus you know torrents are not particularly legal here. And the system is slowing down. What should i do. Have a gr8 day. Plus have you started cos i dont meet you online. How was the honey moon.\n",
      "2436\n",
      "Oh ya ya. I remember da. .\n",
      "2437\n",
      "Btw regarding that we should really try to see if anyone else can be our 4th guy before we commit to a random dude\n",
      "2440\n",
      "Rightio. 11.48 it is then. Well arent we all up bright and early this morning.\n",
      "2441\n",
      "Great. I'm in church now, will holla when i get out\n",
      "2443\n",
      "I donno if they are scorable\n",
      "2444\n",
      "&lt;#&gt;  great loxahatchee xmas tree burning update: you can totally see stars here\n",
      "2445\n",
      "Yes but i dont care! I need you bad, princess!\n",
      "2446\n",
      "The guy (kadeem) hasn't been selling since the break, I know one other guy but he's paranoid as fuck and doesn't like selling without me there and I can't be up there til late tonight\n",
      "2448\n",
      "Tmr then ü brin lar... Aiya later i come n c lar... Mayb ü neva set properly ü got da help sheet wif ü...\n",
      "2449\n",
      "Do u knw dis no. &lt;#&gt; ?\n",
      "2450\n",
      "Then she dun believe wat?\n",
      "2451\n",
      "K..give back my thanks.\n",
      "2452\n",
      "I know complain num only..bettr directly go to bsnl offc nd apply for it..\n",
      "2453\n",
      "Okay. I've seen it. So i should pick it on friday?\n",
      "2454\n",
      "How much she payed. Suganya.\n",
      "2455\n",
      "Left dessert. U wan me 2 go suntec look 4 u?\n",
      "2456\n",
      "Abeg, make profit. But its a start. Are you using it to get sponsors for the next event?\n",
      "2458\n",
      "K.k..how is your sister kids?\n",
      "2461\n",
      "i cant talk to you now.i will call when i can.dont keep calling.\n",
      "2462\n",
      "Anything lar...\n",
      "2464\n",
      "Good afternoon, babe. How goes that day ? Any job prospects yet ? I miss you, my love ... *sighs* ... :-(\n",
      "2465\n",
      "They will pick up and drop in car.so no problem..\n",
      "2466\n",
      "S.i think he is waste for rr..\n",
      "2467\n",
      "He is world famamus....\n",
      "2468\n",
      "Is there coming friday is leave for pongal?do you get any news from your work place.\n",
      "2469\n",
      "Lol well don't do it without me. We could have a big sale together.\n",
      "2470\n",
      "* Am on my way\n",
      "2471\n",
      "Eat at old airport road... But now 630 oredi... Got a lot of pple...\n",
      "2472\n",
      "sry can't talk on phone, with parents\n",
      "2474\n",
      "Ok lor wat time ü finish?\n",
      "2475\n",
      "Princess, i like to make love  &lt;#&gt;  times per night. Hope thats not a problem!\n",
      "2477\n",
      "i dnt wnt to tlk wid u\n",
      "2478\n",
      "I'm done. I'm sorry. I hope your next space gives you everything you want. Remember all the furniture is yours. If i'm not around when you move it, just lock all the locks and leave the key with jenne.\n",
      "2479\n",
      "Not yet. Just i'd like to keep in touch and it will be the easiest way to do that from barcelona. By the way how ru and how is the house?\n",
      "2482\n",
      "K.:)do it at evening da:)urgent:)\n",
      "2483\n",
      "Pansy! You've been living in a jungle for two years! Its my driving you should be more worried about!\n",
      "2484\n",
      "Mm have some kanji dont eat anything heavy ok\n",
      "2485\n",
      "Only if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\n",
      "2488\n",
      "K ill drink.pa then what doing. I need srs model pls send it to my mail id pa.\n",
      "2489\n",
      "Aiyah e rain like quite big leh. If drizzling i can at least run home.\n",
      "2491\n",
      "Dun b sad.. It's over.. Dun thk abt it already. Concentrate on ur other papers k.\n",
      "2492\n",
      "Greetings me, ! Consider yourself excused.\n",
      "2493\n",
      "No drama Pls.i have had enough from you and family while i am struggling in the hot sun in a strange place.No reason why there should be an ego of not going 'IF NOT INVITED' when actually its necessity to go.wait for very serious reppurcussions.\n",
      "2494\n",
      "they released another Italian one today and it has a cosign option\n",
      "2497\n",
      "HCL chennai requires FRESHERS for voice process.Excellent english needed.Salary upto  &lt;#&gt; .Call Ms.Suman  &lt;#&gt;  for Telephonic interview -via Indyarocks.com\n",
      "2500\n",
      "Yup i've finished c ü there...\n",
      "2501\n",
      "Remember to ask alex about his pizza\n",
      "2502\n",
      "No da..today also i forgot..\n",
      "2503\n",
      "Ola would get back to you maybe not today but I ve told him you can be his direct link in the US in getting cars he bids for online, you arrange shipping and you get a cut. Or U????? For a partnership where U????? Invest money for shipping and he takes care of the rest!U??Wud b self reliant soon dnt worry\n",
      "2505\n",
      "Hello, my boytoy! I made it home and my constant thought is of you, my love. I hope your having a nice visit but I can't wait till you come home to me ...*kiss*\n",
      "2507\n",
      "Who u talking about?\n",
      "2508\n",
      "Yup...\n",
      "2511\n",
      "Yunny i'm walking in citylink now ü faster come down... Me very hungry...\n",
      "2512\n",
      "Er yep sure. Props?\n",
      "2516\n",
      "Bognor it is! Should be splendid at this time of year.\n",
      "2518\n",
      "Sorry, I'll call later\n",
      "2519\n",
      "Joy's father is John. Then John is the NAME of Joy's father. Mandan\n",
      "2521\n",
      "Misplaced your number and was sending texts to your old number. Wondering why i've not heard from you this year. All the best in your mcat. Got this number from my atlanta friends\n",
      "2523\n",
      "Dunno lei... I might b eatin wif my frens... If ü wan to eat then i wait 4 ü lar\n",
      "2527\n",
      "Do u noe how 2 send files between 2 computers?\n",
      "2529\n",
      "jay says he'll put in  &lt;#&gt;\n",
      "2530\n",
      "Can you just come in for a sec? There's somebody here I want you to see\n",
      "2531\n",
      "So the sun is anti sleep medicine.\n",
      "2532\n",
      "What's happening with you. Have you gotten a job and have you begun registration for permanent residency\n",
      "2534\n",
      "Glad it went well :) come over at 11 then we'll have plenty of time before claire goes to work.\n",
      "2535\n",
      "Ok enjoy . R u there in home.\n",
      "2536\n",
      "Can you pls pls send me a mail on all you know about relatives coming to deliver here? All you know about costs, risks, benefits and anything else. Thanks.\n",
      "2537\n",
      "You do what all you like\n",
      "2539\n",
      "The monthly amount is not that terrible and you will not pay anything till 6months after finishing school.\n",
      "2541\n",
      "They said if its gonna snow, it will start around 8 or 9 pm tonite! They are predicting an inch of accumulation.\n",
      "2542\n",
      "I dont. Can you send it to me. Plus how's mode.\n",
      "2544\n",
      "Package all your programs well\n",
      "2545\n",
      "She is our sister.. She belongs 2 our family.. She is d hope of tomorrow.. Pray 4 her,who was fated 4 d Shoranur train incident. Lets hold our hands together &amp; fuelled by love &amp; concern prior 2 her grief &amp; pain. Pls join in dis chain &amp; pass it. STOP VIOLENCE AGAINST WOMEN.\n",
      "2546\n",
      "So are you guys asking that i get that slippers again or its gone with last year\n",
      "2547\n",
      "Company is very good.environment is terrific and food is really nice:)\n",
      "2549\n",
      "Honestly i've just made a lovely cup of tea and promptly dropped my keys in it and then burnt my fingers getting them out!\n",
      "2550\n",
      "Yup but not studying surfing lor. I'm in e lazy mode today.\n",
      "2551\n",
      "Please sen :)my kind advice :-)please come here and try:-)\n",
      "2553\n",
      "Oh fine, I'll be by tonight\n",
      "2554\n",
      "Ü give me some time to walk there.\n",
      "2555\n",
      "I'll reach in ard 20 mins ok...\n",
      "2557\n",
      "Fuck babe ... What happened to you ? How come you never came back?\n",
      "2559\n",
      "Some friends want me to drive em someplace, probably take a while\n",
      "2562\n",
      "And maybe some pressies\n",
      "2563\n",
      "Yeah I am, so I'll leave maybe 7ish?\n",
      "2565\n",
      "Under the sea, there lays a rock. In the rock, there is an envelope. In the envelope, there is a paper. On the paper, there are 3 words... '\n",
      "2566\n",
      "I told her I had a Dr appt next week. She thinks I'm gonna die. I told her its just a check. Nothing to be worried about. But she didn't listen.\n",
      "2567\n",
      "You in your room? I need a few\n",
      "2570\n",
      "Ultimately tor motive tui achieve korli.\n",
      "2571\n",
      "From 5 to 2 only my work timing.\n",
      "2572\n",
      "… and don‘t worry we‘ll have finished by march … ish!\n",
      "2573\n",
      "The house is on the water with a dock, a boat rolled up with a newscaster who dabbles in jazz flute behind the wheel\n",
      "2578\n",
      "Hey whats up? U sleeping all morning?\n",
      "2580\n",
      "I dunno until when... Lets go learn pilates...\n",
      "2582\n",
      "Yup i'm elaborating on the safety aspects and some other issues..\n",
      "2585\n",
      "Hi happy birthday. Hi hi hi hi hi hi hi\n",
      "2588\n",
      "Aight, see you in a bit\n",
      "2589\n",
      "My superior telling that friday is leave for all other department except ours:)so it will be leave for you:)any way call waheed fathima hr and conform it:)\n",
      "2592\n",
      "Still work going on:)it is very small house.\n",
      "2593\n",
      "My friend just got here and says he's upping his order by a few grams (he's got $ &lt;#&gt; ), when can you get here?\n",
      "2594\n",
      "Tmr timin still da same wat cos i got lesson until 6...\n",
      "2595\n",
      "That‘s the thing with apes, u can fight to the death to keep something, but the minute they have it when u let go, thats it!\n",
      "2597\n",
      "No i'm not gonna be able to. || too late notice. || i'll be home in a few weeks anyway. || what are the plans\n",
      "2598\n",
      "Got fujitsu, ibm, hp, toshiba... Got a lot of model how to say...\n",
      "2600\n",
      "Gosh that , what a pain. Spose I better come then.\n",
      "2601\n",
      "As usual..iam fine, happy &amp; doing well..:)\n",
      "2603\n",
      "So when you gonna get rimac access \n",
      "2604\n",
      "Im at arestaurant eating squid! i will be out about 10:30 wanna dosomething or is that to late?\n",
      "2605\n",
      "You call times job today ok umma and ask them to speed up\n",
      "2609\n",
      "Hello madam how are you ?\n",
      "2610\n",
      "Awesome, text me when you're restocked\n",
      "2611\n",
      "As usual..iam fine, happy &amp; doing well..:)\n",
      "2614\n",
      "Thanks for sending this mental ability question..\n",
      "2615\n",
      "Sir, hope your day is going smoothly. i really hoped i wont have to bother you about this. I have some bills that i can't settle this month. I am out of all extra cash. I know this is a challenging time for you also but i have to let you know.\n",
      "2616\n",
      "2marrow only. Wed at  &lt;#&gt;  to 2 aha.\n",
      "2617\n",
      "I went to ur hon lab but no one is there.\n",
      "2619\n",
      "Hey pple...$700 or $900 for 5 nights...Excellent location wif breakfast hamper!!!\n",
      "2622\n",
      "Lol! Nah wasn't too bad thanks. Its good to b home but its been quite a reality check. Hows ur day been? Did u do anything with website?\n",
      "2624\n",
      "I'm coming home 4 dinner.\n",
      "2625\n",
      "S da..al r above  &lt;#&gt;\n",
      "2627\n",
      "Unni thank you dear for the recharge..Rakhesh\n",
      "2629\n",
      "Haha... They cant what... At the most tmr forfeit... haha so how?\n",
      "2633\n",
      "I WILL CAL YOU SIR. In meeting\n",
      "2634\n",
      "That's what I love to hear :V see you sundayish, then\n",
      "2635\n",
      "Sorry da thangam, very very sorry i am held up with prasad.\n",
      "2637\n",
      "Thank god they are in bed!\n",
      "2638\n",
      "No I don't have cancer. Moms making a big deal out of a regular checkup aka pap smear\n",
      "2639\n",
      "Am in gobi arts college\n",
      "2643\n",
      "They can try! They can get lost, in fact. Tee hee\n",
      "2644\n",
      "Hi! You just spoke to MANEESHA V. We'd like to know if you were satisfied with the experience. Reply Toll Free with Yes or No.\n",
      "2645\n",
      "My friends use to call the same.\n",
      "2646\n",
      "Sorry, I'll call later\n",
      "2647\n",
      "Em, its olowoyey@ usc.edu have a great time in argentina. Not sad about secretary, everything is a blessing\n",
      "2648\n",
      "It,,s a taxt massage....tie-pos argh ok! Lool!\n",
      "2649\n",
      "Hi, can i please get a  &lt;#&gt;  dollar loan from you. I.ll pay you back by mid february. Pls.\n",
      "2650\n",
      "You might want to pull out more just in case and just plan on not spending it if you can, I don't have much confidence in derek and taylor's money management\n",
      "2651\n",
      "Do you like shaking your booty on the dance floor?\n",
      "2653\n",
      "No need for the drug anymore.\n",
      "2654\n",
      "Sorry da:)i was thought of calling you lot of times:)lil busy.i will call you at noon..\n",
      "2655\n",
      "Its sarcasm.. .nt scarcasim\n",
      "2656\n",
      "Great! I have to run now so ttyl!\n",
      "2657\n",
      "Feel like trying kadeem again? :V\n",
      "2659\n",
      "Not yet chikku..wat abt u?\n",
      "2660\n",
      "Ok...\n",
      "2665\n",
      "He remains a bro amongst bros\n",
      "2667\n",
      "* Was a nice day and, impressively, i was sensible, went home early and now feel fine. Or am i just boring?! When's yours, i can't remember.\n",
      "2671\n",
      "Yes. They replied my mail. I'm going to the management office later. Plus will in to bank later also.or on wednesday.\n",
      "2672\n",
      "That's cool, I'll come by like  &lt;#&gt; ish\n",
      "2674\n",
      "Good afternoon, my boytoy ... How are you feeling today ? Better I hope? Are you being my good boy? Are you my obedient, slave? Do you please your Queen?\n",
      "2677\n",
      "* Am on a train back from northampton so i'm afraid not!\n",
      "2681\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "2682\n",
      "I'm on da bus going home...\n",
      "2684\n",
      "I'm okay. Chasing the dream. What's good. What are you doing next.\n",
      "2685\n",
      "Yupz... I've oredi booked slots 4 my weekends liao... \n",
      "2687\n",
      "There r many model..sony ericson also der.. &lt;#&gt; ..it luks good bt i forgot modl no\n",
      "2688\n",
      "Okie\n",
      "2689\n",
      "Yes I know the cheesy songs from frosty the snowman :)\n",
      "2690\n",
      "Ya ok, vikky vl c witin  &lt;#&gt; mins and il reply u..\n",
      "2694\n",
      "All these nice new shirts and the only thing I can wear them to is nudist themed ;_; you in mu?\n",
      "2696\n",
      "And whenever you and i see we can still hook up too.\n",
      "2698\n",
      "Can you use foreign stamps for whatever you send them off for? \n",
      "2700\n",
      "Oh baby of the house. How come you dont have any new pictures on facebook\n",
      "2701\n",
      "Feb  &lt;#&gt;  is \"I LOVE U\" day. Send dis to all ur \"VALUED FRNDS\" evn me. If 3 comes back u'll gt married d person u luv! If u ignore dis u will lose ur luv 4 Evr\n",
      "2702\n",
      "Hiya, sorry didn't hav signal. I haven't seen or heard from and neither has, which is unusual in itself! I'll put on the case and get him to sort it out! Hugs and snogs.\n",
      "2703\n",
      "Omw back to tampa from west palm, you hear what happened?\n",
      "2704\n",
      "Yup no more already... Thanx 4 printing n handing it up.\n",
      "2707\n",
      "S now only i took tablets . Reaction morning only.\n",
      "2709\n",
      "Nah, I'm a perpetual DD\n",
      "2715\n",
      "I am thinking of going down to reg for pract lessons.. Flung my advance.. Haha wat time u going?\n",
      "2716\n",
      "Cool. I am  &lt;#&gt;  inches long. hope you like them big!\n",
      "2717\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "2718\n",
      "Okie.. Thanx..\n",
      "2722\n",
      "I'm working technical support :)voice process.\n",
      "2723\n",
      "It's justbeen overa week since we broke up and already our brains are going to mush!\n",
      "2725\n",
      "Nope... C ü then...\n",
      "2726\n",
      "No. But we'll do medical missions to nigeria\n",
      "2728\n",
      "Whatsup there. Dont u want to sleep\n",
      "2731\n",
      "I havent lei.. Next mon can?\n",
      "2732\n",
      "Mm feeling sleepy. today itself i shall get that dear\n",
      "2734\n",
      "Do ü noe if ben is going?\n",
      "2735\n",
      "Can you do a mag meeting this avo at some point?\n",
      "2736\n",
      "I meant middle left or right?\n",
      "2737\n",
      "Really? I crashed out cuddled on my sofa.\n",
      "2738\n",
      "Hi Chachi tried calling u now unable to reach u .. Pl give me a missed cal once u c tiz msg  Kanagu\n",
      "2739\n",
      "I sent you the prices and do you mean the  &lt;#&gt; g,\n",
      "2740\n",
      "Are you this much buzy\n",
      "2743\n",
      "No * am working on the ringing u thing but have whole houseful of screaming brats so * am pulling my hair out! Loving u\n",
      "2744\n",
      "But my family not responding for anything. Now am in room not went to home for diwali but no one called me and why not coming. It makes me feel like died.\n",
      "2745\n",
      "Tick, tick, tick ... Babe\n",
      "2746\n",
      "R ü going 4 today's meeting?\n",
      "2748\n",
      "Ya had just now.onion roast.\n",
      "2750\n",
      "You said not now. No problem. When you can. Let me know.\n",
      "2751\n",
      "Ok but tell me half an hr b4 u come i need 2 prepare.\n",
      "2754\n",
      "Derp. Which is worse, a dude who always wants to party or a dude who files a complaint about the three drug abusers he lives with\n",
      "2755\n",
      "Ok Chinese food on its way. When I get fat you're paying for my lipo.\n",
      "2757\n",
      "Have a good trip. Watch out for . Remember when you get back we must decide about easter.\n",
      "2758\n",
      "Yo we are watching a movie on netflix\n",
      "2759\n",
      "What time. I‘m out until prob 3 or so\n",
      "2760\n",
      "Can meh? Thgt some will clash... Really ah, i dun mind... I dun seen to have lost any weight... Gee...\n",
      "2761\n",
      "I dont thnk its a wrong calling between us\n",
      "2762\n",
      "I am not sure about night menu. . . I know only about noon menu\n",
      "2763\n",
      "ARR birthday today:) i wish him to get more oscar.\n",
      "2764\n",
      "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it...\n",
      "2765\n",
      "Open rebtel with firefox. When it loads just put plus sign in the user name place, and it will show you two numbers. The lower number is my number. Once you pick that number the pin will display okay!\n",
      "2768\n",
      "Wow v v impressed. Have funs shopping!\n",
      "2769\n",
      "I am on the way to ur home\n",
      "2771\n",
      "No problem. Talk to you later\n",
      "2772\n",
      "Then ur sis how?\n",
      "2773\n",
      "Still in customer place\n",
      "2775\n",
      "Dude u knw also telugu..thts gud..k, gud nyt..\n",
      "2776\n",
      "We confirm eating at esplanade?\n",
      "2777\n",
      "Send me your id and password\n",
      "2778\n",
      "Kind of. Took it to garage. Centre part of exhaust needs replacing. Part ordered n taking it to be fixed tomo morning.\n",
      "2782\n",
      "Then its most likely called Mittelschmertz. Google it. If you dont have paracetamol dont worry it will go.\n",
      "2784\n",
      "Just arrived, see you in a couple days &lt;3\n",
      "2786\n",
      "Yeah get the unlimited\n",
      "2788\n",
      "Forgot it takes me 3 years to shower, sorry. Where you at/your phone dead yet?\n",
      "2790\n",
      "When you are big..| God will bring success.\n",
      "2792\n",
      "… we r stayin here an extra week, back next wed. How did we do in the rugby this weekend? Hi to and and , c u soon \"\n",
      "2794\n",
      "Not from this campus. Are you in the library?\n",
      "2795\n",
      "The affidavit says  &lt;#&gt;  E Twiggs St, division g, courtroom  &lt;#&gt; , &lt;TIME&gt;  AM. I'll double check and text you again tomorrow\n",
      "2796\n",
      "How will I creep on you now? ;_;\n",
      "2797\n",
      "Tell your friends what you plan to do on Valentines day @ &lt;URL&gt;\n",
      "2798\n",
      "If I get there before you after your ten billion calls and texts so help me god\n",
      "2799\n",
      "Purity of friendship between two is not about smiling after reading the forwarded message..Its about smiling just by seeing the name. Gud evng musthu\n",
      "2803\n",
      "And smile for me right now as you go and the world will wonder what you are smiling about and think your crazy and keep away from you ... *grins*\n",
      "2806\n",
      "I think it's all still in my car\n",
      "2807\n",
      "Can a not?\n",
      "2810\n",
      "Oh yeah I forgot. U can only take 2 out shopping at once.\n",
      "2811\n",
      "Mm so you asked me not to call radio\n",
      "2812\n",
      "Thinkin about someone is all good. No drugs for that\n",
      "2813\n",
      "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it...\n",
      "2814\n",
      "Enjoy the showers of possessiveness poured on u by ur loved ones, bcoz in this world of lies, it is a golden gift to be loved truly..\n",
      "2816\n",
      "Some are lasting as much as 2 hours. You might get lucky.\n",
      "2817\n",
      "Genius what's up. How your brother. Pls send his number to my skype.\n",
      "2819\n",
      "Thk some of em find wtc too far... Weiyi not goin... E rest i dunno yet... R ur goin 4 dinner den i might b able to join...\n",
      "2820\n",
      "Don't forget who owns you and who's private property you are ... And be my good boy always .. *passionate kiss*\n",
      "2824\n",
      "Then u ask darren go n pick u lor... But i oso sian tmr haf 2 meet lect...\n",
      "2825\n",
      "No need to buy lunch for me.. I eat maggi mee..\n",
      "2831\n",
      "Howz that persons story\n",
      "2832\n",
      "Thanx 4 sending me home...\n",
      "2836\n",
      "Ya they are well and fine., BBD(pooja) full pimples..even she become quite black..and ur rite here its too cold, wearing sweatter..\n",
      "2839\n",
      "Were trying to find a Chinese food place around here\n",
      "2840\n",
      "Easy mate, * guess the quick drink was bit ambitious.\n",
      "2841\n",
      "BABE !!! I miiiiiiissssssssss you ! I need you !!! I crave you !!! :-( ... Geeee ... I'm so sad without you babe ... I love you ...\n",
      "2842\n",
      "Ok thanx...\n",
      "2843\n",
      "aathi..where are you dear..\n",
      "2844\n",
      "Tunji, how's the queen? how are you doing. This is just wishing you a great day. Abiola.\n",
      "2846\n",
      "Will be out of class in a few hours. Sorry\n",
      "2847\n",
      "Wat time u finish ur lect today?\n",
      "2851\n",
      "She's fine. Good to hear from you. How are you my dear? Happy new year oh.\n",
      "2853\n",
      "how tall are you princess?\n",
      "2854\n",
      "I doubt you could handle 5 times per night in any case...\n",
      "2855\n",
      "Haha... Hope ü can hear the receipt sound... Gd luck!\n",
      "2856\n",
      "Your gonna be the death if me. I'm gonna leave a note that says its all robs fault. Avenge me.\n",
      "2862\n",
      "I am not at all happy with what you saying or doing\n",
      "2864\n",
      "Ok that would b lovely, if u r sure. Think about wot u want to do, drinkin, dancin, eatin, cinema, in, out, about... Up to u! Wot about ? \n",
      "2865\n",
      "What I'm saying is if you haven't explicitly told nora I know someone I'm probably just not gonna bother\n",
      "2866\n",
      "He says hi and to get your ass back to south tampa (preferably at a kegger)\n",
      "2867\n",
      "Smith waste da.i wanna gayle.\n",
      "2869\n",
      "Aight, tomorrow around  &lt;#&gt;  it is\n",
      "2870\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "2874\n",
      "I dont understand your message.\n",
      "2875\n",
      "Crucify is c not s. You should have told me earlier.\n",
      "2877\n",
      "Fuck cedar key and fuck her (come over anyway tho)\n",
      "2878\n",
      "twenty past five he said will this train have been to durham already or not coz i am in a reserved seat\n",
      "2880\n",
      "U still painting ur wall?\n",
      "2883\n",
      "Hi Harish's rent has been transfred to ur Acnt.\n",
      "2884\n",
      "Anything lor is she coming?\n",
      "2885\n",
      "Cbe is really good nowadays:)lot of shop and showrooms:)city is shaping good.\n",
      "2887\n",
      "No probs hon! How u doinat the mo?\n",
      "2889\n",
      "I take it we didn't have the phone callon Friday. Can we assume we won't have it this year now?\n",
      "2890\n",
      "My battery is low babe\n",
      "2891\n",
      "Shuhui has bought ron's present it's a swatch watch...\n",
      "2893\n",
      "Babe? You said 2 hours and it's been almost 4 ... Is your internet down ?\n",
      "2894\n",
      "K I'll be sure to get up before noon and see what's what\n",
      "2898\n",
      "Ü collecting ur laptop then going to configure da settings izzit?\n",
      "2899\n",
      "If you r @ home then come down within 5 min\n",
      "2900\n",
      "Aight, I should be there by 8 at the latest, probably closer to 7. Are jay and tyler down or should we just do two trips?\n",
      "2901\n",
      "Come aftr  &lt;DECIMAL&gt; ..now i m cleaning the house\n",
      "2904\n",
      "Tell me pa. How is pain de.\n",
      "2905\n",
      "HI DARLIN I HOPE YOU HAD A NICE NIGHT I WISH I HAD COME CANT WAIT TO SEE YOU LOVE FRAN PS I WANT DIRTY ANAL SEX AND I WANT A 10 MAN GANG BANG\n",
      "2906\n",
      "Ha. You don‘t know either. I did a a clever but simple thing with pears the other day, perfect for christmas.\n",
      "2907\n",
      "Helloooo... Wake up..! \"Sweet\" \"morning\" \"welcomes\" \"You\" \"Enjoy\" \"This Day\" \"with full of joy\".. \"GUD MRNG\".\n",
      "2908\n",
      "ALRITE\n",
      "2909\n",
      "Why must we sit around and wait for summer days to celebrate. Such a magical sight when the worlds dressed in white. Oooooh let there be snow.\n",
      "2911\n",
      "How do you guys go to see movies on your side.\n",
      "2912\n",
      "Sorry,in meeting I'll call later\n",
      "2913\n",
      "You didn't have to tell me that...now i'm thinking. Plus he's going to stop all your runs\n",
      "2918\n",
      "Yes. that will be fine. Love you. Be safe.\n",
      "2919\n",
      "Thanks chikku..:-) gud nyt:-*\n",
      "2921\n",
      "Thanx 4 the time weve spent 2geva, its bin mint! Ur my Baby and all I want is u!xxxx\n",
      "2922\n",
      "Yo, any way we could pick something up tonight?\n",
      "2924\n",
      "Fine am simply sitting.\n",
      "2926\n",
      "Are you coming to day for class.\n",
      "2927\n",
      "Im done. Just studyn in library\n",
      "2929\n",
      "Anything...\n",
      "2930\n",
      "Where wuld I be without my baby? The thought alone mite break me and I dont wanna go crazy but everyboy needs his lady xxxxxxxx\n",
      "2931\n",
      "Wat's my dear doing? Sleeping ah?\n",
      "2932\n",
      "Hi' Test on  &lt;#&gt; rd ....\n",
      "2934\n",
      "Yo do you know anyone  &lt;#&gt;  or otherwise able to buy liquor? Our guy flaked and right now if we don't get a hold of somebody its just 4 loko all night\n",
      "2935\n",
      "Yup n her fren lor. I'm meeting my fren at 730.\n",
      "2936\n",
      "Yeah, we got one lined up for us\n",
      "2937\n",
      "And stop wondering \"wow is she ever going to stop tm'ing me ?!\" because I will tm you whenever I want because you are MINE ... *laughs*\n",
      "2938\n",
      "Lol yep did that yesterday. Already got my fireplace. Now its just another icon sitting there for me.\n",
      "2942\n",
      "My supervisor find 4 me one lor i thk his students. I havent ask her yet. Tell u aft i ask her.\n",
      "2946\n",
      "Hey babe, sorry i didn't get sooner. Gary can come and fix it cause he thinks he knows what it is but he doesn't go as far a Ptbo and he says it will cost  &lt;#&gt;  bucks. I don't know if it might be cheaper to find someone there ? We don't have any second hand machines at all right now, let me know what you want to do babe\n",
      "2950\n",
      "at bruce b downs &amp; fletcher now\n",
      "2953\n",
      "Tell me whos this pls:-)\n",
      "2956\n",
      "Id have to check but there's only like 1 bowls worth left\n",
      "2957\n",
      "Yes there were many sweets\n",
      "2960\n",
      "Buzzzz! *grins* Did I buzz your ass? Buzz your chest ? Buzz your cock ? Where do you keep your phone ? Is the vibrator on ? Did you feel it shake ?\n",
      "2961\n",
      "Sir send to group mail check it.\n",
      "2962\n",
      "I'm doing da intro covers energy trends n pros n cons... Brief description of nuclear fusion n oso brief history of iter n jet got abt 7 n half pages..\n",
      "2963\n",
      "NONE!NOWHERE IKNO DOESDISCOUNT!SHITINNIT\n",
      "2967\n",
      "Are you being good, baby? :)\n",
      "2968\n",
      "NEFT Transaction with reference number  &lt;#&gt;  for Rs. &lt;DECIMAL&gt;  has been credited to the beneficiary account on  &lt;#&gt;  at  &lt;TIME&gt; : &lt;#&gt;\n",
      "2970\n",
      "Ma head dey swell oh. Thanks for making my day\n",
      "2971\n",
      "U should make a fb list\n",
      "2974\n",
      "Happy New Year Princess!\n",
      "2975\n",
      "I'll text carlos and let you know, hang on\n",
      "2976\n",
      "Don't worry, * is easy once have ingredients!\n",
      "2977\n",
      "I love u 2 my little pocy bell I am sorry but I love u\n",
      "2979\n",
      "Yar lor... Keep raining non stop... Or u wan 2 go elsewhere?\n",
      "2981\n",
      "What u mean u almost done? Done wif sleeping? But i tot u going to take a nap.. Yup i send her liao so i'm picking her up at ard 4 smth lor..\n",
      "2982\n",
      "7 wonders in My WORLD 7th You 6th Ur style 5th Ur smile 4th Ur Personality 3rd Ur Nature 2nd Ur SMS and 1st \"Ur Lovely Friendship\"... good morning dear\n",
      "2983\n",
      "Tonight? Yeah, I'd be down for that\n",
      "2985\n",
      "He said that he had a right giggle when he saw u again! You would possibly be the first person2die from NVQ, but think how much you could for! \n",
      "2986\n",
      "No break time one... How... I come out n get my stuff fr ü?\n",
      "2989\n",
      "Do you still have the grinder?\n",
      "2991\n",
      "Love isn't a decision, it's a feeling. If we could decide who to love, then, life would be much simpler, but then less magical\n",
      "2994\n",
      "So i'm doing a list of buyers.\n",
      "2997\n",
      "They released vday shirts and when u put it on it makes your bottom half naked instead of those white underwear.\n",
      "2999\n",
      "No b4 Thursday\n",
      "3001\n",
      "Id onluy matters when getting on from offcampus\n",
      "3003\n",
      "Excellent, I'll see what riley's plans are\n",
      "3004\n",
      "I will see in half an hour\n",
      "3006\n",
      "Ew are you one of them?\n",
      "3007\n",
      "Also hi wesley how've you been\n",
      "3008\n",
      "Ah you see. You have to be in the lingo. I will let you know wot on earth it is when has finished making it!\n",
      "3011\n",
      "Imagine Life WITHOUT ME... see.. How fast u are searching me?Don't worry.. l'm always there To disturb U.. Goodnoon..:)\n",
      "3015\n",
      "I might go 2 sch. Yar at e salon now v boring.\n",
      "3019\n",
      "I didn't get the second half of that message\n",
      "3021\n",
      "I thank you so much for all you do with selflessness. I love you plenty.\n",
      "3026\n",
      "What part of \"don't initiate\" don't you understand\n",
      "3027\n",
      "I finished my lunch already. U wake up already?\n",
      "3028\n",
      "You still at the game?\n",
      "3030\n",
      "What is your record for one night? :)\n",
      "3031\n",
      "Also sir, i sent you an email about how to log into the usc payment portal. I.ll send you another message that should explain how things are back home. Have a great weekend.\n",
      "3034\n",
      "Aight, lemme know what's up\n",
      "3036\n",
      "Raji..pls do me a favour. Pls convey my Birthday wishes to Nimya. Pls. Today is her birthday.\n",
      "3037\n",
      ";-) ok. I feel like john lennon.\n",
      "3038\n",
      "Cos darren say ü considering mah so i ask ü...\n",
      "3040\n",
      "Wishing you and your family Merry \"X\" mas and HAPPY NEW Year in advance..\n",
      "3041\n",
      "One day a crab was running on the sea shore..The waves came n cleared the footprints of the crab.. Crab asked: being my frnd y r u clearing my beautiful footprints? Waves replied: A fox was following ur footprints to catch you! thats y i cleared it off:) frndsship never lets u dwn :-) GUD nyt..\n",
      "3043\n",
      "Slaaaaave ! Where are you ? Must I summon you to me all the time now ? Don't you wish to come to me on your own anymore?\n",
      "3044\n",
      "Your bill at 3 is £33.65 so thats not bad!\n",
      "3045\n",
      "Let me know how it changes in the next 6hrs. It can even be appendix but you are out of that age range. However its not impossible. So just chill and let me know in 6hrs\n",
      "3046\n",
      "Hello, yeah i've just got out of the bath and need to do my hair so i'll come up when i'm done, yeah?\n",
      "3047\n",
      "So how's the weather over there?\n",
      "3048\n",
      "Ok. Not much to do here though. H&M Friday, cant wait. Dunno wot the hell im gonna do for another 3 weeks! Become a slob- oh wait, already done that! \n",
      "3050\n",
      "Lol they don't know about my awesome phone. I could click delete right now if I want.\n",
      "3051\n",
      "Ok\n",
      "3052\n",
      "Awesome question with a cute answer: Someone asked a boy \"how is ur life?\" . . He smiled &amp; answered: . . \"She is fine!\" Gudnite\n",
      "3053\n",
      "Please leave this topic..sorry for telling that..\n",
      "3054\n",
      "Pls send me the correct name da.\n",
      "3055\n",
      "What happened to our yo date?\n",
      "3058\n",
      "Just woke up. Yeesh its late. But I didn't fall asleep til &lt;#&gt; am :/\n",
      "3060\n",
      "Dear all, as we know  &lt;#&gt; th is the  &lt;#&gt; th birthday of our loving Gopalettan. We are planning to give a small gift on that day. Those who like to participate in that you are welcome. Please contact our admin team for more details\n",
      "3061\n",
      "K..k...from tomorrow onwards started ah?\n",
      "3062\n",
      "What u talking bout early morning? It's almost noon where your at!\n",
      "3063\n",
      "Fine. Do you remember me.\n",
      "3065\n",
      "Ok. How many should i buy.\n",
      "3066\n",
      "Sounds good, keep me posted\n",
      "3068\n",
      "Ok. So april. Cant wait\n",
      "3070\n",
      "Ay wana meet on sat?ü wkg on sat?\n",
      "3072\n",
      "Apart from the one i told you about yesterday?\n",
      "3073\n",
      "Ok lor... But buy wat?\n",
      "3074\n",
      "Somebody should go to andros and steal ice\n",
      "3075\n",
      "Don know. I did't msg him recently.\n",
      "3076\n",
      "Take us out shopping and Mark will distract Isaiah.=D\n",
      "3077\n",
      "Mum, hope you are having a great day. Hoping this text meets you well and full of life. Have a great day. Abiola\n",
      "3078\n",
      "There is no sense in my foot and penis.\n",
      "3080\n",
      "*deep sigh* ... I miss you :-( ... I am really surprised you haven't gone to the net cafe yet to get to me ... Don't you miss me?\n",
      "3081\n",
      "S.s:)i thinl role is like sachin.just standing. Others have to hit.\n",
      "3084\n",
      "K..k:)how about your training process?\n",
      "3085\n",
      "Ok lor. I ned 2 go toa payoh 4 a while 2 return smth u wan 2 send me there or wat?\n",
      "3086\n",
      "In da car park \n",
      "3087\n",
      "I wish that I was with you. Holding you tightly. Making you see how important you are. How much you mean to me ... How much I need you ... In my life ...\n",
      "3088\n",
      "So i asked how's anthony. Dad. And your bf\n",
      "3090\n",
      "What Today-sunday..sunday is holiday..so no work..\n",
      "3093\n",
      "Dear, take care. I am just reaching home.love u a lot.\n",
      "3094\n",
      "staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323\n",
      "3095\n",
      "Have you emigrated or something? Ok maybe 5.30 was a bit hopeful...\n",
      "3096\n",
      "Olol i printed out a forum post by a guy with the exact same  prob which was fixed with a gpu replacement. Hopefully they dont ignore that.\n",
      "3097\n",
      "We walked from my moms. Right on stagwood pass right on winterstone left on victors hill. Address is &lt;#&gt;\n",
      "3098\n",
      "Yo, you at jp and hungry like a mofo?\n",
      "3099\n",
      "This is all just creepy and crazy to me.\n",
      "3100\n",
      "Ok... I din get ur msg...\n",
      "3101\n",
      "Tessy..pls do me a favor. Pls convey my birthday wishes to Nimya..pls dnt forget it. Today is her birthday Shijas\n",
      "3103\n",
      "Even if he my friend he is a priest call him now\n",
      "3104\n",
      "U so lousy, run already come back then half dead... Hee...\n",
      "3105\n",
      "That's y i said it's bad dat all e gals know u... Wat u doing now?\n",
      "3106\n",
      "Or remind me in a few hrs.\n",
      "3109\n",
      "Hello hun how ru? Its here by the way. Im good. Been on 2 dates with that guy i met in walkabout so far. We have to meet up soon. Hows everyone else?\n",
      "3110\n",
      "Lol I was gonna last month. I cashed some in but I left &lt;#&gt; just in case. I was collecting more during the week cause they announced it on the blog.\n",
      "3114\n",
      "Wat time liao, where still got.\n",
      "3116\n",
      "I wait 4 ü inside da car park...\n",
      "3117\n",
      "Uncle Abbey! Happy New Year. Abiola\n",
      "3119\n",
      "R u saying i should re order the slippers cos i had to pay for returning it.\n",
      "3122\n",
      "Small problem in auction:)punj now asking tiwary\n",
      "3125\n",
      "My uncles in Atlanta. Wish you guys a great semester.\n",
      "3128\n",
      "Thats cool. i liked your photos. You are very sexy!\n",
      "3129\n",
      "would u fuckin believe it they didnt know i had thurs pre booked off so they re cancelled me AGAIN! that needs to b sacked\n",
      "3130\n",
      "Haha better late than ever, any way I could swing by?\n",
      "3131\n",
      "Ok. But i finish at 6.\n",
      "3134\n",
      "So no messages. Had food?\n",
      "3135\n",
      "Ok going to sleep. Hope i can meet her.\n",
      "3136\n",
      "Wat makes some people dearer is not just de happiness dat u feel when u meet them but de pain u feel when u miss dem!!!\n",
      "3137\n",
      "Can you let me know details of fri when u find out cos I'm not in tom or fri. mentionned chinese. Thanks\n",
      "3138\n",
      "You're right I have now that I think about it\n",
      "3139\n",
      "Wat r u doing now?\n",
      "3142\n",
      "Customer place i will call you\n",
      "3145\n",
      "Haha I heard that, text me when you're around\n",
      "3146\n",
      "I.ll get there tomorrow and send it to you\n",
      "3148\n",
      "Oh thats late! Well have a good night and i will give u a call tomorrow. Iam now going to go to sleep night night\n",
      "3150\n",
      "Sorry,  in meeting I'll call you later\n",
      "3152\n",
      "Yeah but which is worse for i\n",
      "3154\n",
      "I tagged MY friends that you seemed to count as YOUR friends.\n",
      "3156\n",
      "Ok...\n",
      "3158\n",
      "Havent shopping now lor i juz arrive only\n",
      "3159\n",
      "Thank u. IT BETTER WORK OUT CAUSE I WILL FEEL USED OTHERWISE\n",
      "3160\n",
      "Are you up for the challenge? I know i am :)\n",
      "3161\n",
      "How much did ur hdd casing cost.\n",
      "3163\n",
      "I can't describe how lucky you are that I'm actually awake by noon\n",
      "3165\n",
      "TODAY is Sorry day.! If ever i was angry with you, if ever i misbehaved or hurt you? plz plz JUST SLAP URSELF Bcoz, Its ur fault, I'm basically GOOD\n",
      "3169\n",
      "Ugh hopefully the asus ppl dont randomly do a reformat.\n",
      "3170\n",
      "Haven't seen my facebook, huh? Lol!\n",
      "3171\n",
      "Mah b, I'll pick it up tomorrow\n",
      "3172\n",
      "Still otside le..u come 2morrow maga..\n",
      "3175\n",
      "It vl bcum more difficult..\n",
      "3178\n",
      "In meeting da. I will call you\n",
      "3181\n",
      "There the size of elephant tablets & u shove um up ur ass!!\n",
      "3183\n",
      "My Parents, My Kidz, My Friends n My Colleagues. All screaming.. SURPRISE !! and I was waiting on the sofa.. ... ..... ' NAKED...!\n",
      "3185\n",
      "Good morning pookie pie! Lol hope I didn't wake u up\n",
      "3187\n",
      "Happy birthday to you....dear.with lots of love.rakhesh NRI\n",
      "3188\n",
      "Howz that persons story\n",
      "3191\n",
      "Hi neva worry bout da truth coz the truth will lead me 2 ur heart. Its the least a unique person like u deserve. Sleep tight or morning\n",
      "3193\n",
      "Is ur paper today in e morn or aft?\n",
      "3195\n",
      "And you! Will expect you whenever you text! Hope all goes well tomo \n",
      "3197\n",
      "I av a new number,  . Wil u only use this one,ta.\n",
      "3198\n",
      "So its to be poking man everyday that they teach you in canada abi! How are you. Just saying hi.\n",
      "3200\n",
      "No de.am seeing in online shop so that i asked.\n",
      "3203\n",
      "Okay lor... Wah... like that def they wont let us go... Haha... What did they say in the terms and conditions?\n",
      "3205\n",
      "She's good. How are you. Where r u working now\n",
      "3208\n",
      "This phone has the weirdest auto correct.\n",
      "3209\n",
      "Oops my phone died and I didn't even know. Yeah I like it better.\n",
      "3210\n",
      "Havent mus ask if u can 1st wat. Of meet 4 lunch den u n him meet can already lor. Or u wan 2 go ask da ge 1st then confirm w me asap?\n",
      "3211\n",
      "She said,'' do u mind if I go into the bedroom for a minute ? '' ''OK'', I sed in a sexy mood. She came out 5 minuts latr wid a cake...n My Wife,\n",
      "3212\n",
      "OH YEAH,AND HAV A GREAT TIME IN NEWQUAY-SEND ME A POSTCARD !1 LOOK AFTER ALL THE GIRLS WHILE IM GONE(U KNOW THE 1IM TALKIN BOUT!)xx\n",
      "3213\n",
      "We got a divorce. Lol. She.s here\n",
      "3216\n",
      "I want snow. It's just freezing and windy.\n",
      "3218\n",
      "Come to mahal bus stop.. &lt;DECIMAL&gt;\n",
      "3219\n",
      "Don know:)this week i'm going to tirunelvai da.\n",
      "3220\n",
      "Me too baby! I promise to treat you well! I bet you will take good care of me...\n",
      "3223\n",
      "Hi, my love! How goes that day? Fuck, this morning I woke and dropped my cell on the way down the stairs but it seems alright ... *phews* I miss you !\n",
      "3227\n",
      "Rose for red,red for blood,blood for heart,heart for u. But u for me.... Send tis to all ur friends.. Including me.. If u like me.. If u get back, 1-u r poor in relation! 2-u need some 1 to support 3-u r frnd 2 many 4-some1 luvs u 5+- some1 is praying god to marry u.:-) try it....\n",
      "3231\n",
      "I feel like a dick because I keep sleeping through your texts and facebook messages. Sup, you in town?\n",
      "3232\n",
      "No plm i will come da. On the way.\n",
      "3233\n",
      "Guess he wants alone time. We could just show up and watch when they do..\n",
      "3234\n",
      "Height of recycling: Read twice- People spend time for earning money and the same money is spent for spending time!;-) Good morning.. keep smiling:-)\n",
      "3236\n",
      "Yes, princess. Toledo.\n",
      "3237\n",
      "Aight text me when you're back at mu and I'll swing by, need somebody to get the door for me\n",
      "3239\n",
      "Good. No swimsuit allowed :)\n",
      "3241\n",
      "A cute thought for friendship: \"Its not necessary to share every secret with ur close Frnd, but watever u shared should be true\"....\n",
      "3244\n",
      "Pls accept me for one day. Or am begging you change the number.\n",
      "3249\n",
      "Also track down any lighters you can find\n",
      "3250\n",
      "Sorry, I can't help you on this.\n",
      "3252\n",
      "I‘ll leave around four, ok?\n",
      "3254\n",
      "K:)k..its good:)when are you going?\n",
      "3255\n",
      "I can make lasagna for you... vodka...\n",
      "3256\n",
      "HI ITS KATE CAN U GIVE ME A RING ASAP XXX\n",
      "3259\n",
      "He fucking chickened out. He messaged me he would be late and woould buzz me and then I didn't hear a word from him\n",
      "3261\n",
      "I'm always looking for an excuse to be in the city.\n",
      "3262\n",
      "Yup i'm still having coffee wif my frens... My fren drove she'll give me a lift...\n",
      "3263\n",
      "O shore are you takin the bus\n",
      "3264\n",
      "So u gonna get deus ex?\n",
      "3265\n",
      "I will send them to your email. Do you mind  &lt;#&gt;  times per night?\n",
      "3268\n",
      "Ok then i come n pick u at engin?\n",
      "3269\n",
      "Which is why i never wanted to tell you any of this. Which is why i'm so short with you and on-edge as of late.\n",
      "3270\n",
      "Raviyog Peripherals bhayandar east\n",
      "3273\n",
      "MOON has come to color your dreams, STARS to make them musical and my SMS to give you warm and Peaceful Sleep. Good Night\n",
      "3274\n",
      "Just finished eating. Got u a plate. NOT leftovers this time.\n",
      "3275\n",
      "Thanx a lot...\n",
      "3276\n",
      "Hurry home u big butt. Hang up on your last caller if u have to. Food is done and I'm starving. Don't ask what I cooked.\n",
      "3277\n",
      "Lol your right. What diet? Everyday I cheat anyway. I'm meant to be a fatty :(\n",
      "3278\n",
      "Its a great day. Do have yourself a beautiful one.\n",
      "3280\n",
      "Solve d Case : A Man Was Found Murdered On  &lt;DECIMAL&gt; . &lt;#&gt;  AfterNoon. 1,His wife called Police. 2,Police questioned everyone. 3,Wife: Sir,I was sleeping, when the murder took place. 4.Cook: I was cooking. 5.Gardener: I was picking vegetables. 6.House-Maid: I went 2 d post office. 7.Children: We went 2 play. 8.Neighbour: We went 2 a marriage. Police arrested d murderer Immediately. Who's It? Reply With Reason, If U r Brilliant.\n",
      "3281\n",
      "Badrith is only for chennai:)i will surely pick for us:)no competition for him.\n",
      "3282\n",
      "I tot it's my group mate... Lucky i havent reply... Wat time do ü need to leave... \n",
      "3283\n",
      "Hey you around? I've got enough for a half + the ten I owe you\n",
      "3287\n",
      "Not to worry. I'm sure you'll get it.\n",
      "3288\n",
      "The gas station is like a block away from my house, you'll drive right by it since armenia ends at swann and you have to take howard\n",
      "3291\n",
      "My tuition is at 330. Hm we go for the 1120 to 1205 one? Do you mind?\n",
      "3293\n",
      "Dear good morning how you feeling dear\n",
      "3294\n",
      "A little. Meds say take once every 8 hours. It's only been 5 but pain is back. So I took another. Hope I don't die\n",
      "3297\n",
      "Hi there. We have now moved in2 our pub . Would be great 2 c u if u cud come up.\n",
      "3300\n",
      "Honeybee Said: *I'm d Sweetest in d World* God Laughed &amp; Said: *Wait,U Havnt Met d Person Reading This Msg* MORAL: Even GOD Can Crack Jokes! GM+GN+GE+GN:)\n",
      "3301\n",
      "Just do what ever is easier for you\n",
      "3303\n",
      "Stop calling everyone saying I might have cancer. My throat hurts to talk. I can't be answering everyones calls. If I get one more call I'm not babysitting on Monday\n",
      "3304\n",
      "It'll be tough, but I'll do what I have to\n",
      "3305\n",
      "IM GONNAMISSU SO MUCH!!I WOULD SAY IL SEND U A POSTCARD BUTTHERES ABOUTAS MUCH CHANCE OF MEREMEMBERIN ASTHERE IS OFSI NOT BREAKIN HIS CONTRACT!! LUV Yaxx\n",
      "3307\n",
      "HI DARLIN I FINISH AT 3 DO U 1 2 PICK ME UP OR MEET ME? TEXT BACK ON THIS NUMBER LUV KATE XXX\n",
      "3308\n",
      "Set a place for me in your heart and not in your mind, as the mind easily forgets but the heart will always remember. Wish you Happy Valentines Day!\n",
      "3309\n",
      "But i'm surprised she still can guess right lor...\n",
      "3310\n",
      "Okie ü wan meet at bishan? Cos me at bishan now. I'm not driving today.\n",
      "3311\n",
      "Oh ho. Is this the first time u use these type of words\n",
      "3312\n",
      "HI DARLIN HOW WAS WORK DID U GET INTO TROUBLE? IJUST TALKED TO YOUR MUM ALL MORNING! I HAD A REALLY GOOD TIME LAST NIGHT IM GOIN OUT SOON BUT CALL ME IF U CAN\n",
      "3314\n",
      "Huh... Hyde park not in mel ah, opps, got confused... Anyway, if tt's e best choice den we juz have to take it...\n",
      "3315\n",
      "Oh gei. That happend to me in tron. Maybe ill dl it in 3d when its out\n",
      "3317\n",
      "I know girls always safe and selfish know i got it pa. Thank you. good night.\n",
      "3319\n",
      "I'm freezing and craving ice. Fml\n",
      "3320\n",
      "Kay... Since we are out already \n",
      "3323\n",
      "Ok darlin i supose it was ok i just worry too much.i have to do some film stuff my mate and then have to babysit again! But you can call me there.xx\n",
      "3327\n",
      "Huh so fast... Dat means u havent finished painting?\n",
      "3328\n",
      " what number do u live at? Is it 11?\n",
      "3330\n",
      "Sac will score big hundred.he is set batsman:-)\n",
      "3332\n",
      "How much it will cost approx . Per month.\n",
      "3333\n",
      "Ok... The theory test? when are ü going to book? I think it's on 21 may. Coz thought wanna go out with jiayin. But she isnt free\n",
      "3335\n",
      "That's fine, have him give me a call if he knows what he wants or has any questions\n",
      "3336\n",
      "Sorry, got a late start, we're on the way\n",
      "3337\n",
      "Then u go back urself lor...\n",
      "3338\n",
      "I AM AT THE GAS STATION. GO THERE.\n",
      "3339\n",
      "K, if u bored up just come to my home..\n",
      "3340\n",
      "Babe !!!! I LOVE YOU !!!! *covers your face in kisses*\n",
      "3341\n",
      "Like I made him throw up when we were smoking in our friend's car one time, it was awesome\n",
      "3342\n",
      "Still i have not checked it da. . .\n",
      "3344\n",
      "I haven't forgotten you, i might have a couple bucks to send you tomorrow, k? I love ya too\n",
      "3345\n",
      "Oh great. I.ll disturb him more so that we can talk.\n",
      "3350\n",
      "Oh is it! Which brand?\n",
      "3354\n",
      "I emailed yifeng my part oredi.. Can ü get it fr him..\n",
      "3355\n",
      "R u sure they'll understand that! Wine * good idea just had a slurp!\n",
      "3356\n",
      "Minimum walk is 3miles a day.\n",
      "3357\n",
      "Ok not a problem will get them a taxi. C ing  tomorrow and tuesday. On tuesday think we r all going to the cinema. \n",
      "3361\n",
      "Please attend the phone:)\n",
      "3362\n",
      "You only hate me. You can call any but you didnt accept even a single call of mine. Or even you messaged\n",
      "3364\n",
      "Can... I'm free...\n",
      "3365\n",
      "Yo my trip got postponed, you still stocked up?\n",
      "3366\n",
      "Sorry, I'll call later\n",
      "3367\n",
      "I am waiting for your call sir.\n",
      "3369\n",
      "Hey elaine, is today's meeting still on?\n",
      "3370\n",
      "Sorry i've not gone to that place. I.ll do so tomorrow. Really sorry.\n",
      "3371\n",
      "Most of the tiime when i don't let you hug me it's so i don't break into tears.\n",
      "3373\n",
      "And now electricity just went out fml.\n",
      "3375\n",
      "Also andros ice etc etc\n",
      "3376\n",
      ":) \n",
      "3377\n",
      "Good afternon, my love. How are today? I hope your good and maybe have some interviews. I wake and miss you babe. A passionate kiss from across the sea\n",
      "3378\n",
      "Yup. Wun believe wat? U really neva c e msg i sent shuhui?\n",
      "3379\n",
      "Hows that watch resizing\n",
      "3380\n",
      "Dear umma she called me now :-)\n",
      "3383\n",
      "Well, I meant as opposed to my drunken night of before\n",
      "3384\n",
      "K... Must book a not huh? so going for yoga basic on sunday?\n",
      "3386\n",
      "Ok can...\n",
      "3390\n",
      "O. Guess they both got screwd\n",
      "3392\n",
      "I'm in a meeting, call me later at\n",
      "3394\n",
      "Ok thanx...\n",
      "3395\n",
      "Bull. Your plan was to go floating off to IKEA with me without a care in the world. So i have to live with your mess another day.\n",
      "3399\n",
      "It only does simple arithmetic not percentages.\n",
      "3400\n",
      "Yeah we wouldn't leave for an hour at least, how's 4 sound?\n",
      "3402\n",
      "Thanks honey. Have a great day.\n",
      "3407\n",
      "HEY DAS COOL... IKNOW ALL 2 WELLDA PERIL OF STUDENTFINANCIAL CRISIS!SPK 2 U L8R.\n",
      "3408\n",
      "Beautiful Truth against Gravity.. Read carefully: \"Our heart feels light when someone is in it.. But it feels very heavy when someone leaves it..\" GOODMORNING\n",
      "3410\n",
      "Whats that coming over the hill..... Is it a monster! Hope you have a great day. Things r going fine here, busy though! \n",
      "3411\n",
      "Joy's father is John. Then John is the ____ of Joy's father. If u ans ths you hav  &lt;#&gt;  IQ. Tis s IAS question try to answer.\n",
      "3412\n",
      "Only once then after ill obey all yours.\n",
      "3413\n",
      "No she didnt. I will search online and let you know.\n",
      "3415\n",
      "No pic. Please re-send.\n",
      "3416\n",
      "He remains a bro amongst bros\n",
      "3417\n",
      "Uhhhhrmm isnt having tb test bad when youre sick\n",
      "3418\n",
      "But i haf enuff space got like 4 mb...\n",
      "3427\n",
      "Sure but since my parents will be working on Tuesday I don't really need a cover story\n",
      "3429\n",
      "Hi darlin did youPhone me? Im atHome if youwanna chat.\n",
      "3430\n",
      "I don't know jack shit about anything or i'd say/ask something helpful but if you want you can pretend that I did and just text me whatever in response to the hypotheticalhuagauahahuagahyuhagga\n",
      "3432\n",
      "Yeah if we do have to get a random dude we need to change our info sheets to PARTY  &lt;#&gt; /7 NEVER STUDY just to be safe\n",
      "3436\n",
      "Hi darlin i cantdo anythingtomorrow as myparents aretaking me outfor a meal. when are u free? Katexxx\n",
      "3437\n",
      "If india win or level series means this is record:)\n",
      "3444\n",
      "Your board is working fine. The issue of overheating is also reslove. But still software inst is pending. I will come around 8'o clock.\n",
      "3445\n",
      "Yes but I don't care cause I know its there!\n",
      "3446\n",
      "wiskey Brandy Rum Gin Beer Vodka Scotch Shampain Wine \"KUDI\"yarasu dhina vaazhthukkal. ..\n",
      "3447\n",
      "Mon okie lor... Haha, best is cheap n gd food la, ex oso okie... Depends on whether wana eat western or chinese food... Den which u prefer... \n",
      "3449\n",
      "Its  &lt;#&gt; k here oh. Should i send home for sale.\n",
      "3450\n",
      "Sorry. || mail? || \n",
      "3451\n",
      "Ya just telling abt tht incident..\n",
      "3452\n",
      "Yes we were outside for like 2 hours. And I called my whole family to wake them up cause it started at 1 am\n",
      "3454\n",
      "Nowadays people are notixiquating the laxinorficated opportunity for bambling of entropication.... Have you ever oblisingately opted ur books for the masteriastering amplikater of fidalfication? It is very champlaxigating, i think it is atrocious.. Wotz Ur Opinion???? Junna\n",
      "3455\n",
      "I dont have any of your file in my bag..i was in work when you called me.i 'll tell you if i find anything in my room.\n",
      "3456\n",
      "No need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room.\n",
      "3457\n",
      "Ok. I.ll do you right later.\n",
      "3459\n",
      "Have your lunch and come quickly and open the door:)\n",
      "3461\n",
      "I am back. Bit long cos of accident on a30. Had to divert via wadebridge.I had a brilliant weekend thanks. Speak soon. Lots of love\n",
      "3462\n",
      "K.. I yan jiu liao... Sat we can go 4 bugis vill one frm 10 to 3 den hop to parco 4 nb. Sun can go cine frm 1030 to 2, den hop to orc mrt 4 hip hop at 4...\n",
      "3465\n",
      "My phone\n",
      "3466\n",
      "Haha figures, well I found the piece and priscilla's bowl\n",
      "3469\n",
      "yay! finally lol. i missed our cinema trip last week :-(\n",
      "3470\n",
      "All day working day:)except saturday and sunday..\n",
      "3474\n",
      "You getting back any time soon?\n",
      "3475\n",
      ", how's things? Just a quick question.\n",
      "3476\n",
      "Night has ended for another day, morning has come in a special way. May you smile like the sunny rays and leaves your worries at the blue blue bay. Gud mrng\n",
      "3479\n",
      "I can ask around but there's not a lot in terms of mids up here\n",
      "3480\n",
      "Be sure to check your yahoo email. We sent photos yesterday\n",
      "3481\n",
      "What was she looking for?\n",
      "3482\n",
      "Wherre's my boytoy ? :-(\n",
      "3484\n",
      "Hello, my love! How goes that day ? I wish your well and fine babe and hope that you find some job prospects. I miss you, boytoy ... *a teasing kiss*\n",
      "3485\n",
      "Tell my  bad character which u Dnt lik in me. I'll try to change in  &lt;#&gt; . I ll add tat 2 my new year resolution. Waiting for ur reply.Be frank...good morning.\n",
      "3486\n",
      "No:-)i got rumour that you going to buy apartment in chennai:-)\n",
      "3487\n",
      "Yeah, probably earlier than that\n",
      "3488\n",
      "Change windows logoff sound..\n",
      "3490\n",
      "I'm also came to room.\n",
      "3491\n",
      "Huh but i got lesson at 4 lei n i was thinkin of going to sch earlier n i tot of parkin at kent vale...  \n",
      "3492\n",
      "Ok.\n",
      "3494\n",
      "Cool, text me when you head out\n",
      "3497\n",
      "Happy birthday... May u find ur prince charming soon n dun work too hard...\n",
      "3499\n",
      "You said to me before i went back to bed that you can't sleep for anything.\n",
      "3500\n",
      "I hope you arnt pissed off but id would really like to see you tomorrow. Love me xxxxxxxxxxxxxX\n",
      "3502\n",
      "says the  &lt;#&gt;  year old with a man and money. I'm down to my last  &lt;#&gt; . Still waiting for that check.\n",
      "3503\n",
      "I will come to ur home now\n",
      "3504\n",
      "Free any day but i finish at 6 on mon n thurs...\n",
      "3506\n",
      "life alle mone,eppolum oru pole allalo\n",
      "3511\n",
      "I just saw ron burgundy captaining a party boat so yeah\n",
      "3512\n",
      "I'm serious. You are in the money base\n",
      "3514\n",
      "Staff of placement training in Amrita college.\n",
      "3515\n",
      "I always chat with you. In fact i need money can you raise me?\n",
      "3517\n",
      "Well, I was about to give up cos they all said no they didn‘t do one nighters. I persevered and found one but it is very cheap so i apologise in advance. It is just somewhere to sleep isnt it?\n",
      "3518\n",
      "So you think i should actually talk to him? Not call his boss in the morning? I went to this place last year and he told me where i could go and get my car fixed cheaper. He kept telling me today how much he hoped i would come back in, how he always regretted not getting my number, etc.\n",
      "3519\n",
      "Are you willing to go for apps class.\n",
      "3520\n",
      "Hanging out with my brother and his family\n",
      "3521\n",
      "No it will reach by 9 only. She telling she will be there. I dont know\n",
      "3522\n",
      "Hey... are you going to quit soon? Xuhui and i working till end of the month \n",
      "3524\n",
      "Try neva mate!!\n",
      "3525\n",
      "Yeah that'd pretty much be the best case scenario\n",
      "3526\n",
      "I not free today i haf 2 pick my parents up tonite...\n",
      "3527\n",
      "HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DEAD 2 DA WRLD. BEEN SLEEPING ON DA SOFA ALL DAY, HAD A COOL NYTHO, TX 4 FONIN HON, CALL 2MWEN IM BK FRMCLOUD 9! J X\n",
      "3528\n",
      "Should i send you naughty pix? :)\n",
      "3537\n",
      "Oic cos me n my sis got no lunch today my dad went out... So dunno whether 2 eat in sch or wat...\n",
      "3538\n",
      "Mmmmm ... It was sooooo good to wake to your words this morning, my Love!! Mmmm fuck ... I love you too, my Lion ... *devouring kiss from across the sea*\n",
      "3540\n",
      "What happen dear. Why you silent. I am tensed\n",
      "3541\n",
      "I'll get there at 3, unless you guys want me to come some time sooner\n",
      "3543\n",
      "Ü come lt 25 n pass to me lar\n",
      "3544\n",
      "I'm e person who's doing e sms survey...\n",
      "3545\n",
      "Lol ok ill try to send. Be warned Sprint is dead slow. You'll prolly get it tomorrow\n",
      "3549\n",
      "Single line with a big meaning::::: \"Miss anything 4 ur \"Best Life\" but, don't miss ur best life for anything... Gud nyt...\n",
      "3550\n",
      "I got like $ &lt;#&gt; , I can get some more later though. Get whatever you feel like\n",
      "3553\n",
      "Lol u still feeling sick?\n",
      "3554\n",
      "Din i tell u jus now 420\n",
      "3555\n",
      "am up to my eyes in philosophy\n",
      "3557\n",
      "Ok lor. I'm in town now lei.\n",
      "3566\n",
      "We know TAJ MAHAL as symbol of love. But the other lesser known facts 1. Mumtaz was Shahjahan's 4th wife, out of his 7 wifes. 2. Shahjahan killed Mumtaz's husband to marry her. 3. Mumtaz died in her  &lt;#&gt; th delivery. 4. He then married Mumtaz's sister. Question arises where the Hell is the LOVE?:-| -The Great Hari-\n",
      "3567\n",
      "Its ok..come to my home it vl nice to meet and v can chat..\n",
      "3570\n",
      "She's fine. Sends her greetings\n",
      "3572\n",
      "But you dint in touch with me.\n",
      "3573\n",
      "Yup, leaving right now, be back soon\n",
      "3575\n",
      "Yeah sure I'll leave in a min\n",
      "3577\n",
      "The sign of maturity is not when we start saying big things.. But actually it is, when we start understanding small things... *HAVE A NICE EVENING* BSLVYL\n",
      "3579\n",
      "They said ü dun haf passport or smth like dat.. Or ü juz send to my email account..  \n",
      "3580\n",
      "Multiply the numbers independently and count decimal points then, for the division, push the decimal places like i showed you.\n",
      "3581\n",
      "Have a lovely night and when you wake up to see this message, i hope you smile knowing all is as should be. Have a great morning\n",
      "3583\n",
      "You are right. Meanwhile how's project twins comin up\n",
      "3584\n",
      "I sent your maga that money yesterday oh.\n",
      "3586\n",
      "Heart is empty without love.. Mind is empty without wisdom.. Eyes r empty without dreams &amp; Life is empty without frnds.. So Alwys Be In Touch. Good night &amp; sweet dreams\n",
      "3588\n",
      "Our ride equally uneventful - not too many of those pesky cyclists around at that time of night ;).\n",
      "3589\n",
      "If you were/are free i can give. Otherwise nalla adi entey nattil kittum\n",
      "3590\n",
      "I've sent my wife your text. After we buy them she'll tell you what to do. So just relax. We should go get them this wkend.\n",
      "3592\n",
      "How much would it cost to hire a hitman\n",
      "3593\n",
      "I anything lor...\n",
      "3596\n",
      "Huh but i cant go 2 ur house empty handed right?\n",
      "3597\n",
      "Good morning princess! Happy New Year!\n",
      "3599\n",
      "Aight, we'll head out in a few\n",
      "3603\n",
      "Cps is causing the outages to conserve energy.\n",
      "3604\n",
      "I'm not sure, I was just checking out what was happening around the area\n",
      "3607\n",
      "That means you got an A in epi, she.s fine. She.s here now.\n",
      "3608\n",
      "I have no idea where you are\n",
      "3611\n",
      "Call me. I m unable to cal. Lets meet bhaskar, and deep\n",
      "3612\n",
      "No. I.ll meet you in the library\n",
      "3613\n",
      "K, my roommate also wants a dubsack and another friend may also want some so plan on bringing extra, I'll tell you when they know for sure\n",
      "3615\n",
      "Ok c ü then.\n",
      "3616\n",
      "I enjoy watching and playing football and basketball. Anything outdoors. And you?\n",
      "3617\n",
      "Can you please ask macho what his price range is, does he want something new or used plus it he only interfued in the blackberry bold  &lt;#&gt;  or any bb\n",
      "3618\n",
      "Sorry sent blank msg again. Yup but trying 2 do some serious studying now.\n",
      "3621\n",
      "I meant as an apology from me for texting you to get me drugs at  &lt;#&gt; at night\n",
      "3622\n",
      "That means from february to april i'll be getting a place to stay down there so i don't have to hustle back and forth during audition season as i have since my sister moved away from harlem.\n",
      "3623\n",
      "Goin to workout lor... Muz lose e fats... \n",
      "3624\n",
      "Damn, poor zac doesn't stand a chance\n",
      "3625\n",
      "No message..no responce..what happend?\n",
      "3626\n",
      "I want to tel u one thing u should not mistake me k THIS IS THE MESSAGE THAT YOU SENT:)\n",
      "3628\n",
      "Still chance there. If you search hard you will get it..let have a try :)\n",
      "3629\n",
      "Meeting u is my work. . . Tel me when shall i do my work tomorrow\n",
      "3630\n",
      "Should I head straight there or what\n",
      "3632\n",
      "Thank you princess! You are so sexy...\n",
      "3634\n",
      "Hui xin is in da lib.\n",
      "3636\n",
      "It's not that you make me cry. It's just that when all our stuff happens on top of everything else, it pushes me over the edge. You don't underdtand how often i cry over my sorry, sorry life.\n",
      "3637\n",
      "ME 2 BABE I FEEL THE SAME LETS JUST 4GET ABOUT IT+BOTH TRY +CHEER UP+NOT FIT SOO MUCHXXLOVE U LOCAXX\n",
      "3638\n",
      "You know what hook up means right?\n",
      "3640\n",
      "Wat's da model num of ur phone?\n",
      "3643\n",
      "My house here e sky quite dark liao... If raining then got excuse not 2 run already rite... Hee...\n",
      "3644\n",
      "Sorry, left phone upstairs. OK, might be hectic but would be all my birds with one fell swoop. It's a date.\n",
      "3645\n",
      "* Thought I didn't see you.\n",
      "3647\n",
      "Carlos says we can pick up from him later so yeah we're set\n",
      "3649\n",
      "As per your request 'Maangalyam (Alaipayuthe)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "3651\n",
      "We are hoping to get away by 7, from Langport. You still up for town tonight?\n",
      "3653\n",
      "Probably not, still going over some stuff here\n",
      "3654\n",
      "It has issues right now. Ill fix for her by tomorrow.\n",
      "3655\n",
      "Why i come in between you people\n",
      "3657\n",
      "Oh really?? Did you make it on air? What's your talent?\n",
      "3660\n",
      "Wait.i will come out.. &lt;#&gt;  min:)\n",
      "3661\n",
      "I will reach ur home in  &lt;#&gt;  minutes\n",
      "3663\n",
      "What are you doing in langport? Sorry, but I'll probably be in bed by 9pm. It sucks being ill at xmas! When do you and go2sri lanka? \n",
      "3665\n",
      "Huh? 6 also cannot? Then only how many mistakes?\n",
      "3666\n",
      "Ha... U jus ate honey ar? So sweet...\n",
      "3667\n",
      "I'm turning off my phone. My moms telling everyone I have cancer. And my sister won't stop calling. It hurts to talk. Can't put up with it. See u when u get home. Love u\n",
      "3668\n",
      "Honey ? Sweetheart ? Darling ? Sexy buns ? Sugar plum ? Loverboy ? I miss you, boytoy ... *smacks your ass* Did you go to the gym too ?\n",
      "3669\n",
      "Thanks for loving me so. You rock\n",
      "3670\n",
      "Yeah imma come over cause jay wants to do some drugs\n",
      "3671\n",
      "Ok thanx... Take care then...\n",
      "3672\n",
      "Yup. Thk of u oso boring wat.\n",
      "3673\n",
      " came to look at the flat, seems ok, in his 50s? * Is away alot wiv work. Got woman coming at 6.30 too.\n",
      "3677\n",
      "Hey r ü still online? I've finished the formatting...\n",
      "3678\n",
      "Great! So what attracts you to the brothas?\n",
      "3682\n",
      "LOL what happens in Vegas stays in vegas\n",
      "3683\n",
      "Hello, hello, hi lou sorry it took so long 2 reply- I left mobile at friends in Lancaster, just got it bak Neway im sorry I couldnt make ur bday 2 hun!\n",
      "3685\n",
      "Dad says hurry the hell up\n",
      "3687\n",
      "I get out of class in bsn in like  &lt;#&gt;  minutes, you know where advising is?\n",
      "3689\n",
      "I'll meet you in the lobby\n",
      "3690\n",
      "You still coming tonight?\n",
      "3691\n",
      "What happen dear tell me\n",
      "3693\n",
      "No i am not having not any movies in my laptop\n",
      "3694\n",
      "I was about to do it when i texted. I finished a long time ago and showered and er'ything!\n",
      "3695\n",
      "Ok im not sure what time i finish tomorrow but i wanna spend the evening with you cos that would be vewy vewy lubly! Love me xxx\n",
      "3696\n",
      "Hello, As per request from  &lt;#&gt;  Rs.5 has been transfered to you\n",
      "3697\n",
      "I am in tirupur.  call you da.\n",
      "3699\n",
      "S:)but he had some luck.2 catches put down:)\n",
      "3700\n",
      "How i noe... Did ü specify da domain as nusstu... Ü still in sch...\n",
      "3701\n",
      "Oh...i asked for fun. Haha...take care. ü\n",
      "3702\n",
      "Shall i get my pouch?\n",
      "3703\n",
      "Hey loverboy! I love you !! I had to tell ... I look at your picture and ache to feel you between my legs ... Fuck I want you ... I need you ... I crave you .\n",
      "3704\n",
      "How is my boy? No sweet words left for me this morning ... *sighs* ... How goes you day, my love ? Did you start your studying?\n",
      "3705\n",
      "Kent vale lor... Ü wait 4 me there ar?\n",
      "3706\n",
      "Ok. Very good. Its all about making that money.\n",
      "3707\n",
      "Reading gud habit.. Nan bari hudgi yorge pataistha ertini kano:-)\n",
      "3710\n",
      "Ok.ok ok..then..whats ur todays plan\n",
      "3711\n",
      "ARE YOU IN TOWN? THIS IS V. IMPORTANT\n",
      "3713\n",
      "Wat u doing there?\n",
      "3714\n",
      "If i not meeting ü all rite then i'll go home lor. If ü dun feel like comin it's ok.\n",
      "3715\n",
      "Oh, i will get paid. The most outstanding one is for a commercial i did for Hasbro...in AUGUST! They made us jump through so many hoops to get paid. Still not.\n",
      "3721\n",
      "Yeah why not, is the gang all ready\n",
      "3723\n",
      "I'm in a movie... Collect car oredi...\n",
      "3726\n",
      "Nothing spl..wat abt u and whr ru?\n",
      "3727\n",
      "No chikku nt yet.. Ya i'm free\n",
      "3728\n",
      "Aldrine, rakhesh ex RTM here.pls call.urgent.\n",
      "3729\n",
      "The search 4 happiness is 1 of d main sources of unhappiness! Accept life the way it comes! U will find happiness in every moment u live.\n",
      "3730\n",
      "I'm at home. Please call\n",
      "3732\n",
      "Isn't frnd a necesity in life? imagine urself witout a frnd.. hw'd u feel at ur colleg? wat'll u do wth ur cell? wat abt functions? thnk abt events espe'll cared, missed &amp; irritated u? 4wrd it to all those dear-loving frnds wthout whom u cant live.. I jst did it.. Takecare..:) GOODMORNING\n",
      "3733\n",
      "Gud mrng dear hav a nice day\n",
      "3734\n",
      "Old Orchard near univ. How about you?\n",
      "3737\n",
      "Hows the street where the end of library walk is?\n",
      "3739\n",
      "We stopped to get ice cream and will go back after\n",
      "3743\n",
      "Hey i'm bored... So i'm thinking of u... So wat r u doing?\n",
      "3744\n",
      "Nah, Wednesday. When should I bring the mini cheetos bag over?\n",
      "3745\n",
      "Nobody names their penis a girls name this story doesn't add up at all\n",
      "3746\n",
      "Aight, let me know when you're gonna be around usf\n",
      "3748\n",
      "Ü neva tell me how i noe... I'm not at home in da aft wat... \n",
      "3751\n",
      "Buzz! Hey, my Love ! I think of you and hope your day goes well. Did you sleep in ? I miss you babe. I long for the moment we are together again*loving smile*\n",
      "3752\n",
      "Haha... Sounds crazy, dunno can tahan anot...\n",
      "3753\n",
      "Why are u up so early?\n",
      "3756\n",
      "Im on gloucesterroad what are uup to later?\n",
      "3757\n",
      "Yes:)here tv is always available in work place..\n",
      "3760\n",
      "GOD ASKED, \"What is forgiveness?\" A little child gave lovely reply, \"It is d wonderful fruit that a tree gives when it is being hurt by a stone.. Good night......\n",
      "3761\n",
      "We'll join the  &lt;#&gt;  bus\n",
      "3762\n",
      "Was just about to ask. Will keep this one. Maybe that's why you didn't get all the messages we sent you on glo\n",
      "3764\n",
      "K.i will send in  &lt;#&gt;  min:)\n",
      "3767\n",
      "Yes.mum lookin strong:)\n",
      "3768\n",
      "Sir Goodmorning, Once free call me.\n",
      "3769\n",
      "Where are you call me.\n",
      "3771\n",
      "Love it! The girls at the office may wonder why you are smiling but sore...\n",
      "3772\n",
      "Hi, wlcome back, did wonder if you got eaten by a lion or something, nothing much\n",
      "3773\n",
      "Does uncle timi help in clearing cars\n",
      "3775\n",
      "Ok... But bag again..\n",
      "3777\n",
      "Ok lor. Msg me b4 u call.\n",
      "3781\n",
      "Then ur physics get a-?\n",
      "3783\n",
      "How r ü going to send it to me?\n",
      "3784\n",
      "Can you do online transaction?\n",
      "3787\n",
      "Wat r u doing?\n",
      "3790\n",
      "Are you sure you don't mean \"get here, we made you hold all the weed\"\n",
      "3793\n",
      "Love it! I want to flood that pretty pussy with cum...\n",
      "3794\n",
      "Hey are you angry with me. Reply me dr.\n",
      "3795\n",
      "Short But Cute: \"Be a good person, but dont try to prove it..\" .Gud noon....\n",
      "3796\n",
      "Also remember the beads don't come off. Ever.\n",
      "3797\n",
      "They have a thread on the wishlist section of the forums where ppl post nitro requests. Start from the last page and collect from the bottom up.\n",
      "3798\n",
      "For The First Time In The History 'Need' 'Comfort' And 'Luxury' Are Sold At Same Price In India..!! Onion-Rs. &lt;#&gt;  Petrol-Rs. &lt;#&gt;  Beer-Rs. &lt;#&gt;  SHESIL  &lt;#&gt;\n",
      "3799\n",
      "Feb  &lt;#&gt;  is \"I LOVE U\" day. Send dis to all ur \"VALUED FRNDS\" evn me. If 3 comes back u'll gt married d person u luv! If u ignore dis u will lose ur luv 4 Evr\n",
      "3802\n",
      "It's ok, at least armand's still around\n",
      "3803\n",
      "No da. I am happy that we sit together na\n",
      "3806\n",
      "Dude while were makin those weirdy brownies my sister made awesome cookies. I took pics.\n",
      "3808\n",
      "Pls dont restrict her from eating anythin she likes for the next two days.\n",
      "3810\n",
      "At the funeral home with Audrey and dad\n",
      "3811\n",
      "Aight, can you text me the address?\n",
      "3812\n",
      "Excellent! Wish we were together right now!\n",
      "3813\n",
      "Yep then is fine 7.30 or 8.30 for ice age.\n",
      "3814\n",
      "Pls i wont belive god.not only jesus.\n",
      "3816\n",
      "Not yet chikku..k, then wat abt tht guy did he stopped irritating or msging to u..\n",
      "3818\n",
      "This is my number by vivek..\n",
      "3820\n",
      "sorry brah, just finished the last of my exams, what up\n",
      "3821\n",
      "I got arrested for possession at, I shit you not,  &lt;TIME&gt;  pm\n",
      "3824\n",
      "Please protect yourself from e-threats. SIB never asks for sensitive information like Passwords,ATM/SMS PIN thru email. Never share your password with anybody.\n",
      "3825\n",
      "I miss you so much I'm so desparate I have recorded the message you left for me the other day and listen to it just to hear the sound of your voice. I love you\n",
      "3826\n",
      "Hi. I'm always online on yahoo and would like to chat with you someday\n",
      "3827\n",
      "Goodmorning,my grandfather expired..so am on leave today.\n",
      "3829\n",
      "Where are you ? What are you doing ? Are yuou working on getting the pc to your mom's ? Did you find a spot that it would work ? I need you\n",
      "3831\n",
      "I agree. So i can stop thinkin about ipad. Can you please ask macho the same question.\n",
      "3832\n",
      "Let's pool our money together and buy a bunch of lotto tickets. If we win I get &lt;#&gt; % u get &lt;#&gt; %. Deal?\n",
      "3834\n",
      "I had askd u a question some hours before. Its answer\n",
      "3835\n",
      "Watching tv lor. Nice one then i like lor.\n",
      "3836\n",
      "I'm thinking that chennai forgot to come for auction..\n",
      "3837\n",
      "Then ü come n pick me at 530 ar?\n",
      "3838\n",
      "Early bird! Any purchases yet?\n",
      "3842\n",
      "Howz pain.it will come down today.do as i said ystrday.ice and medicine.\n",
      "3843\n",
      "chile, please! It's only a  &lt;DECIMAL&gt;  hour drive for me. I come down all the time and will be subletting feb-april for audition season.\n",
      "3844\n",
      "Yes ammae....life takes lot of turns you can only sit and try to hold the steering...\n",
      "3845\n",
      "Yeah that's what I thought, lemme know if anything's goin on later\n",
      "3849\n",
      "Can you plz tell me the ans. BSLVYL sent via fullonsms.com\n",
      "3850\n",
      "U in town alone?\n",
      "3851\n",
      "I to am looking forward to all the sex cuddling.. Only two more sleeps \n",
      "3852\n",
      "We have all rounder:)so not required:)\n",
      "3853\n",
      "No, its true..k,Do u knw dis no. &lt;#&gt; ?\n",
      "3855\n",
      "oh ya... Got hip hop open. Haha i was thinking can go for jazz then zoom to cine... Actually tonight i'm free leh... And there's a kb lesson tonight\n",
      "3857\n",
      "I'm ok. Will do my part tomorrow\n",
      "3861\n",
      "Yep. I do like the pink furniture tho.\n",
      "3863\n",
      "Customer place, i wil cal u sir.\n",
      "3865\n",
      "A pure hearted person can have a wonderful smile that makes even his/her enemies to feel guilty for being an enemy.. So catch the world with your smile..:) GOODMORNING &amp; HAVE A SMILEY SUNDAY..:)\n",
      "3866\n",
      "THATS ALRITE GIRL, U KNOW GAIL IS NEVA WRONG!!TAKE CARE SWEET AND DONT WORRY.C U L8TR HUN!LOVE Yaxxx\n",
      "3867\n",
      "Theoretically yeah, he could be able to come\n",
      "3870\n",
      "No let me do the math. Your not good at it.\n",
      "3871\n",
      "Oh ok wait 4 me there... My lect havent finish\n",
      "3872\n",
      "Yeah my usual guy's out of town but there're definitely people around I know\n",
      "3873\n",
      "I am joining today formally.Pls keep praying.will talk later.\n",
      "3875\n",
      "No. Did you multimedia message them or e-mail?\n",
      "3876\n",
      "Okie but i scared u say i fat... Then u dun wan me already...\n",
      "3877\n",
      "did u get that message\n",
      "3878\n",
      "Sorry sir, i will call you tomorrow.  senthil.hsbc\n",
      "3880\n",
      "She left it very vague. She just said she would inform the person in accounting about the delayed rent and that i should discuss with the housing agency about my renting another place. But checking online now and all places around usc are  &lt;#&gt;  and up\n",
      "3882\n",
      "Can you plz tell me the ans. BSLVYL sent via fullonsms.com\n",
      "3883\n",
      "Short But Cute: \"Be a good person, but dont try to prove it..\" .Gud noon....\n",
      "3884\n",
      "Gumby's has a special where a  &lt;#&gt; \" cheese pizza is $2 so I know what we're doin tonight\n",
      "3886\n",
      "Like a personal sized or what\n",
      "3889\n",
      "ok....take care.umma to you too...\n",
      "3890\n",
      "Unlimited texts. Limited minutes.\n",
      "3892\n",
      "No problem. We will be spending a lot of quality time together...\n",
      "3898\n",
      "No. Thank you. You've been wonderful\n",
      "3899\n",
      "Otherwise had part time job na-tuition..\n",
      "3902\n",
      "That depends. How would you like to be treated? :)\n",
      "3904\n",
      "Waiting in e car 4 my mum lor. U leh? Reach home already?\n",
      "3908\n",
      "No that just means you have a fat head\n",
      "3909\n",
      "Sounds like a plan! Cardiff is still here and still cold! I'm sitting on the radiator!\n",
      "3910\n",
      "Serious? What like proper tongued her\n",
      "3912\n",
      "How i noe... She's in da car now... Later then c lar... I'm wearing shorts...\n",
      "3915\n",
      "Today is ACCEPT DAY..U Accept me as? Brother Sister Lover Dear1 Best1 Clos1 Lvblefrnd Jstfrnd Cutefrnd Lifpartnr Belovd Swtheart Bstfrnd No rply means enemy\n",
      "3916\n",
      "Ard 530 lor. I ok then message ü lor.\n",
      "3917\n",
      "Ok. C u then.\n",
      "3920\n",
      "I wish! I don't think its gonna snow that much. But it will be more than those flurries we usually get that melt before they hit the ground. Eek! We haven't had snow since &lt;#&gt; before I was even born!\n",
      "3923\n",
      "Oh really? perform, write a paper, go to a movie AND be home by midnight, huh?\n",
      "3925\n",
      "How? Izzit still raining?\n",
      "3926\n",
      "As if i wasn't having enough trouble sleeping.\n",
      "3928\n",
      "Lol ... I really need to remember to eat when I'm drinking but I do appreciate you keeping me company that night babe *smiles*\n",
      "3929\n",
      "Babe ? I lost you ... Will you try rebooting ?\n",
      "3930\n",
      "Yes. Nigh you cant aha.\n",
      "3931\n",
      "I thk ü gotta go home by urself. Cos i'll b going out shopping 4 my frens present. \n",
      "3933\n",
      "Sos! Any amount i can get pls.\n",
      "3936\n",
      "You need to get up. Now.\n",
      "3938\n",
      "Yeah, in fact he just asked if we needed anything like an hour ago. When and how much?\n",
      "3939\n",
      "WHEN THE FIRST STRIKE IS A RED ONE. THE BIRD + ANTELOPE BEGIN TOPLAY IN THE FIELDOF SELFINDEPENDENCE BELIEVE THIS + THE FLOWER OF CONTENTION WILL GROW.RANDOM!\n",
      "3940\n",
      "Y ü wan to go there? C doctor?\n",
      "3941\n",
      "Does daddy have a bb now.\n",
      "3944\n",
      "I got a call from a landline number. . . I am asked to come to anna nagar . . . I will go in the afternoon\n",
      "3947\n",
      "How u doin baby girl ?? hope u are okay every time I call ure phone is off! I miss u get in touch\n",
      "3951\n",
      "I got to video tape pple type in message lor. U so free wan 2 help me? Hee... Cos i noe u wan 2 watch infernal affairs so ask u along. Asking shuhui oso.\n",
      "3952\n",
      "Hi dude hw r u da realy mising u today\n",
      "3953\n",
      "Me hungry buy some food good lei... But mum n yun dun wan juz buy a little bit... \n",
      "3955\n",
      "I probably won't eat at all today. I think I'm gonna pop. How was your weekend? Did u miss me?\n",
      "3956\n",
      "I knew it... U slept v late yest? Wake up so late...\n",
      "3957\n",
      "Haha... dont be angry with yourself... Take it as a practice for the real thing. =) \n",
      "3959\n",
      "So i could kiss and feel you next to me...\n",
      "3960\n",
      "Have a nice day my dear.\n",
      "3961\n",
      "I sent lanre fakeye's Eckankar details to the mail box\n",
      "3962\n",
      "Your dad is back in ph?\n",
      "3964\n",
      "If you ask her or she say any please message.\n",
      "3965\n",
      "If e timing can, then i go w u lor...\n",
      "3966\n",
      "Love you aathi..love u lot..\n",
      "3967\n",
      "I was just callin to say hi. Take care bruv!\n",
      "3973\n",
      "WOT U UP 2 J?\n",
      "3974\n",
      "Night night, see you tomorrow\n",
      "3976\n",
      "do u think that any girl will propose u today by seing ur bloody funky shit fucking face...............asssssholeeee................\n",
      "3977\n",
      "I wish u were here. I feel so alone\n",
      "3982\n",
      "Huh i cant thk of more oredi how many pages do we have?\n",
      "3983\n",
      "His frens go then he in lor. Not alone wif my mum n sis lor.\n",
      "3985\n",
      "Hey, I missed you tm of last night as my phone was on the charge ... *smiles* ... I am meeting a friend shortly\n",
      "3987\n",
      "Friendship is not a game to play, It is not a word to say, It doesn\\'t start on March and ends on May, It is tomorrow, yesterday, today and e\n",
      "3990\n",
      "Ok lor. Anyway i thk we cant get tickets now cos like quite late already. U wan 2 go look 4 ur frens a not? Darren is wif them now...\n",
      "3994\n",
      "Nimbomsons. Yep phone knows that one. Obviously, cos thats a real word\n",
      "3996\n",
      "R u in this continent?\n",
      "3997\n",
      "We'll you pay over like  &lt;#&gt; yrs so its not too difficult\n",
      "4000\n",
      "K...k...when will you give treat?\n",
      "4003\n",
      "Did you get any gift? This year i didnt get anything. So bad\n",
      "4004\n",
      "somewhere out there beneath the pale moon light someone think in of u some where out there where dreams come true... goodnite &amp; sweet dreams\n",
      "4005\n",
      "Well there's a pattern emerging of my friends telling me to drive up and come smoke with them and then telling me that I'm a weed fiend/make them smoke too much/impede their doing other things so you see how I'm hesitant\n",
      "4006\n",
      ", ow u dey.i paid 60,400thousad.i told  u would call . \n",
      "4007\n",
      "IM FINE BABES AINT BEEN UP 2 MUCH THO! SAW SCARY MOVIE YEST ITS QUITE FUNNY! WANT 2MRW AFTERNOON? AT TOWN OR MALL OR SUMTHIN?xx\n",
      "4009\n",
      "Forgot you were working today! Wanna chat, but things are ok so drop me a text when you're free / bored etc and i'll ring. Hope all is well, nose essay and all xx\n",
      "4010\n",
      "Ha... Then we must walk to everywhere... Cannot take tram. My cousin said can walk to vic market from our hotel \n",
      "4013\n",
      "Discussed with your mother ah?\n",
      "4014\n",
      "Ok.\n",
      "4017\n",
      "In which place i can get rooms cheap:-)\n",
      "4021\n",
      "University of southern california.\n",
      "4022\n",
      "We have to pick rayan macleran there.\n",
      "4023\n",
      "U gd lor go shopping i got stuff to do. U wan 2 watch infernal affairs a not? Come lar...\n",
      "4024\n",
      "Well. Balls. Time to make calls\n",
      "4025\n",
      "Wat time ü wan today?\n",
      "4028\n",
      "Yes, princess. Are you going to make me moan?\n",
      "4030\n",
      "[…] anyway, many good evenings to u! s\n",
      "4031\n",
      "Cool, I'll text you in a few\n",
      "4032\n",
      "Sorry vikky, i'm Watching olave mandara movie kano in trishul theatre wit my frnds..\n",
      "4034\n",
      "I am taking you for italian food. How about a pretty dress with no panties? :)\n",
      "4035\n",
      "Wot u up 2? Thout u were gonna call me!! Txt bak luv K\n",
      "4038\n",
      "Dont flatter yourself... Tell that man of mine two pints of carlin in ten minutes please.... \n",
      "4039\n",
      "Hope you are not scared!\n",
      "4040\n",
      "I cant pick the phone right now. Pls send a message\n",
      "4041\n",
      "I'm at home n ready...\n",
      "4043\n",
      "What time do u get out?\n",
      "4044\n",
      "I am literally in bed and have been up for like  &lt;#&gt;  hours\n",
      "4045\n",
      "Yes, my reg is  Ciao!\n",
      "4046\n",
      "If You mean the website. Yes.\n",
      "4049\n",
      "Lol or I could just starve and lose a pound by the end of the day.\n",
      "4051\n",
      "Ok ok take care. I can understand.\n",
      "4052\n",
      "Motivate Behind every darkness, there is a shining light waiting for you to find it... Behind every best friend, there is always trust and love... BSLVYL\n",
      "4053\n",
      "Ya ok, then had dinner?\n",
      "4054\n",
      "I was slept that time.you there?\n",
      "4055\n",
      "dont make ne plans for nxt wknd coz she wants us to come down then ok\n",
      "4057\n",
      "Ha ha nan yalrigu heltini..Iyo kothi chikku, u shared many things wit me..so far i didn't told any body and even uttered a word abt u.. If ur trusting me so much how can i tell these to others.. Plz nxt time dont use those words to me..ok, chikku:-);-)B-)\n",
      "4059\n",
      "Hi di is yijue we're meeting at 7 pm at esaplanade tonight.\n",
      "4062\n",
      "Aight I've been set free, think you could text me blake's address? It occurs to me I'm not quite as sure what I'm doing as I thought I was\n",
      "4063\n",
      "Hi dear we saw dear. We both are happy. Where you my battery is low\n",
      "4065\n",
      "Prof: you have passed in all the papers in this sem congrats . . . . Student: Enna kalaachutaarama..!! Prof:???? Gud mrng!\n",
      "4066\n",
      "Dont kick coco when he's down\n",
      "4074\n",
      "Was the actual exam harder than NBME\n",
      "4075\n",
      "A lot of this sickness thing going round. Take it easy. Hope u feel better soon. Lol\n",
      "4078\n",
      "Hey sathya till now we dint meet not even a single time then how can i saw the situation sathya.\n",
      "4080\n",
      "O i played smash bros  &lt;#&gt;  religiously.\n",
      "4081\n",
      "Sir, good morning. Hope you had a good weekend. I called to let you know that i was able to raise  &lt;#&gt;  from my dad. He however said he would make the rest available by mid feb. This amount is still quite short and i was hoping you would help. Do have a good day. Abiola\n",
      "4082\n",
      "Hurry home. Soup is DONE!\n",
      "4083\n",
      "No no. I will check all rooms befor activities\n",
      "4084\n",
      "Good afternoon, my love. It was good to see your words on YM and get your tm. Very smart move, my slave ... *smiles* ... I drink my coffee and await you.\n",
      "4085\n",
      "Quite ok but a bit ex... U better go eat smth now else i'll feel guilty...\n",
      "4090\n",
      "How are you, my Love ? Are you with your brother ? Time to talk english with him ? *grins* Say : Hey Muhommad, Penny says hello from across the sea\n",
      "4098\n",
      "i am going to bed now prin\n",
      "4099\n",
      "I think just yourself …Thanks and see you tomo\n",
      "4100\n",
      "If u dun drive then how i go 2 sch.\n",
      "4101\n",
      "I not at home now lei...\n",
      "4105\n",
      "Do u hav any frnd by name ashwini in ur college?\n",
      "4106\n",
      "Jus finish my lunch on my way home lor... I tot u dun wan 2 stay in sch today...\n",
      "4109\n",
      "Pls send me your address sir.\n",
      "4110\n",
      "I want to lick your pussy now...\n",
      "4111\n",
      "Yo, you gonna still be in stock tomorrow/today? I'm trying to get a dubsack\n",
      "4113\n",
      "I'll see, but prolly yeah\n",
      "4114\n",
      "Thought we could go out for dinner. I'll treat you! Seem ok?\n",
      "4116\n",
      "Sorry. You never hear unless you book it. One was kinda a joke--thet were really looking for skinny white girls. The other was one line--you can only do so much on camera with that. Something like that they're casting on the look.\n",
      "4119\n",
      "Watch lor. I saw a few swatch one i thk quite ok. Ard 116 but i need 2nd opinion leh...\n",
      "4120\n",
      "Hiya do u like the hlday pics looked horrible in them so took mo out! Hows the camp Amrca thing? Speak soon Serena:)\n",
      "4121\n",
      "Babe! How goes that day ? What are you up to ? I miss you already, my Love ... * loving kiss* ... I hope everything goes well.\n",
      "4125\n",
      "Hey sexy buns ! Have I told you ? I adore you, loverboy. I hope you remember to thank your sister in law for those meatballs *grins* ... i love you, babe\n",
      "4126\n",
      "May b approve panalam...but it should have more posts..\n",
      "4128\n",
      "Sorry, I'll call later\n",
      "4129\n",
      "I dont thnk its a wrong calling between us\n",
      "4130\n",
      "Me i'm not workin. Once i get job...\n",
      "4131\n",
      "And by when you're done I mean now\n",
      "4133\n",
      "Hi baby ive just got back from work and i was wanting to see u allday! I hope i didnt piss u off on the phone today. If u are up give me a call xxx\n",
      "4135\n",
      "Is it your yahoo boys that bring in the perf? Or legal.\n",
      "4136\n",
      "No need to say anything to me. I know i am an outsider\n",
      "4137\n",
      "have you ever had one foot before?\n",
      "4138\n",
      "Just got to  &lt;#&gt;\n",
      "4139\n",
      "Good! No, don‘t need any receipts—well done! (…) Yes, please tell . What‘s her number, i could ring her\n",
      "4140\n",
      "Ever green quote ever told by Jerry in cartoon \"A Person Who Irritates u Always Is the one Who Loves u Vry Much But Fails to Express It...!..!! :-) :-) gud nyt\n",
      "4141\n",
      "Leave it wif me lar... Ü wan to carry meh so heavy... Is da num 98321561 familiar to ü?\n",
      "4142\n",
      "Beautiful truth : Expression of the face could Be seen by everyone... But the depression of heart Could be understood only By the Loved ones.. Gud Ni8;-)\n",
      "4143\n",
      "Infact happy new year. How are you where are you when are we seeing\n",
      "4145\n",
      "That's a shame! Maybe cld meet for few hrs tomo?\n",
      "4146\n",
      "Lol I would but despite these cramps I like being a girl.\n",
      "4147\n",
      "I cant wait for cornwall. Hope tonight isnt too bad as well but its rock night shite. Anyway im going for a kip now have a good night. Speak to you soon.\n",
      "4148\n",
      "Pls help me tell sura that i'm expecting a battery from hont. And that if should pls send me a message about how to download movies. Thanks\n",
      "4150\n",
      "Haven't found a way to get another app for your phone, eh ? Will you go to the net cafe ? Did you take that job? Geeee I need you babe. I crave to see you ...\n",
      "4152\n",
      "Ü comin to fetch us oredi...\n",
      "4153\n",
      "What's nannys address?\n",
      "4158\n",
      "This single single answers are we fighting? Plus i said am broke and you didnt reply\n",
      "4159\n",
      "It certainly puts things into perspective when something like this happens\n",
      "4161\n",
      "i felt so...not any conveying reason.. Ese he... What about me?\n",
      "4163\n",
      "How's it going? Got any exciting karaoke type activities planned? I'm debating whether to play football this eve. Feeling lazy though.\n",
      "4164\n",
      "I told that am coming on wednesday.\n",
      "4165\n",
      "Its ok, called mom instead have fun\n",
      "4167\n",
      "Well if I'm that desperate I'll just call armand again\n",
      "4168\n",
      "Are you at work right now ?\n",
      "4171\n",
      "Mmmmmm ... I love you,so much, Ahmad ... I can't wait for this year to begin as every second takes me closer to being at your side. Happy New Year, my love!!\n",
      "4172\n",
      "Pls what's the full name of joke's school cos fees in university of florida seem to actually be  &lt;#&gt; k. Pls holla back\n",
      "4173\n",
      "Sorry, I'll call later\n",
      "4175\n",
      "And pls pls drink plenty plenty water\n",
      "4176\n",
      "How are you doing. How's the queen. Are you going for the royal wedding\n",
      "4180\n",
      "Can ü send me a copy of da report?\n",
      "4182\n",
      "Ok da, i already planned. I wil pick you.\n",
      "4185\n",
      "I just really need shit before tomorrow and I know you won't be awake before like 6\n",
      "4186\n",
      "I'm good. Have you registered to vote?\n",
      "4187\n",
      "Hmm ok, i'll stay for like an hour cos my eye is really sore!\n",
      "4189\n",
      "Mm umma ask vava also to come tell him can play later together\n",
      "4190\n",
      "Well the general price is  &lt;#&gt; /oz, let me know if/when/how much you want\n",
      "4191\n",
      "Sorry, I'll call later\n",
      "4192\n",
      "Each Moment in a day,has its own value-Morning brings hope,afternoon brings faith,Evening brings luv,Night brings rest,Wish u find them all today.Good Morning\n",
      "4193\n",
      "&lt;#&gt;  w jetton ave if you forgot\n",
      "4195\n",
      "Can not use foreign stamps in this country.\n",
      "4197\n",
      "Sorry, it's a lot of friend-of-a-friend stuff, I'm just now about to talk to the actual guy who wants to buy\n",
      "4201\n",
      "I will come tomorrow di\n",
      "4202\n",
      "Wylie update: my weed dealer carlos went to freedom and had a class with lunsford\n",
      "4203\n",
      "Are you happy baby ? Are you alright ? Did you take that job ? I hope your fine. I send you a kiss to make you smile from across the sea ... *kiss* *kiss*\n",
      "4205\n",
      "How are you enjoying this semester? Take care brother.\n",
      "4208\n",
      "Lets use it next week, princess :)\n",
      "4209\n",
      "Or i go home first lar ü wait 4 me lor.. I put down my stuff first..\n",
      "4210\n",
      "I want kfc its Tuesday. Only buy 2 meals ONLY 2. No gravy. Only 2 Mark. 2!\n",
      "4211\n",
      "No da:)he is stupid da..always sending like this:)don believe any of those message.pandy is a mental:)\n",
      "4212\n",
      "Oi when you gonna ring\n",
      "4214\n",
      "I attended but nothing is there.\n",
      "4215\n",
      "Ard 530 like dat lor. We juz meet in mrt station then ü dun haf to come out.\n",
      "4216\n",
      "No dear i was sleeping :-P\n",
      "4217\n",
      "Er mw im filled tuth is aight\n",
      "4219\n",
      "Actually i'm waiting for 2 weeks when they start putting ad.\n",
      "4221\n",
      "U free on sat rite? U wan 2 watch infernal affairs wif me n darren n mayb xy?\n",
      "4224\n",
      "Stupid auto correct on my phone\n",
      "4225\n",
      "Double eviction this week - Spiral and Michael and good riddance to them!\n",
      "4227\n",
      "Ok thats cool. Its , just off either raglan rd or edward rd. Behind the cricket ground. Gimme ring when ur closeby see you tuesday.\n",
      "4230\n",
      "Have you bookedthe hut? And also your time off? How are you by the way?\n",
      "4231\n",
      "And several to you sir.\n",
      "4232\n",
      "U really pig leh sleep so much. My dad wake me up at 10 smth 2 eat lunch today.\n",
      "4233\n",
      "I'm at home. Please call\n",
      "4234\n",
      "My love ... I hope your not doing anything drastic. Don't you dare sell your pc or your phone ...\n",
      "4239\n",
      "Lol wtf random. Btw is that your lunch break\n",
      "4244\n",
      "Okie...\n",
      "4246\n",
      "Is toshiba portege m100 gd?\n",
      "4247\n",
      "Well welp is sort of a semiobscure internet thing\n",
      "4250\n",
      "Loosu go to hospital. De dont let it careless.\n",
      "4252\n",
      "Omg Joanna is freaking me out. She's looked thru all my friends to find photos of me. And then she's asking about stuff on my MySpace which I haven't even logged on in like a year. :/\n",
      "4253\n",
      "Send ur birthdate with month and year, I will tel u ur LIFE PARTNER'S name. and the method of calculation. Reply must.\n",
      "4257\n",
      "Aah! A cuddle would be lush! I'd need lots of tea and soup before any kind of fumbling!\n",
      "4259\n",
      "I am late. I will be there at\n",
      "4261\n",
      "Are you plans with your family set in stone ?\n",
      "4264\n",
      "Den only weekdays got special price... Haiz... Cant eat liao... Cut nails oso muz wait until i finish drivin wat, lunch still muz eat wat... \n",
      "4265\n",
      "She just broke down a list of reasons why nobody's in town and I can't tell if she's being sarcastic or just faggy\n",
      "4266\n",
      " &lt;DECIMAL&gt; m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla\n",
      "4267\n",
      "The greatest test of courage on earth is to bear defeat without losing heart....gn tc\n",
      "4268\n",
      "SORRY IM STIL FUCKED AFTER LAST NITE WENT TOBED AT 430 GOT UP 4 WORK AT 630\n",
      "4269\n",
      "Hey so whats the plan this sat? \n",
      "4270\n",
      "Beauty sleep can help ur pimples too.\n",
      "4271\n",
      "Great. Hope you are using your connections from mode men also cos you can never know why old friends can lead you to today\n",
      "4273\n",
      "Where to get those?\n",
      "4274\n",
      "Kind of. Just missed train cos of asthma attack, nxt one in half hr so driving in. not sure where to park.\n",
      "4277\n",
      "Can you please send me my aunty's number\n",
      "4278\n",
      "I'm glad. You are following your dreams.\n",
      "4282\n",
      "Wn u r hurt by d prsn who s close 2 u, do fight wit dem. Coz somtimes dis fight saves a relation bt being quiet leaves nothin in a relation.. Gud eveB-)\n",
      "4285\n",
      "Yes. I come to nyc for audiitions and am trying to relocate.\n",
      "4286\n",
      "I pocked you up there before\n",
      "4288\n",
      "I wud never mind if u dont miss me or if u dont need me.. But u wil really hurt me wen u need me &amp; u dont tell me......... Take care:-)\n",
      "4290\n",
      "Okay, good, no problem, and thanx!\n",
      "4292\n",
      "Call me when u're done...\n",
      "4300\n",
      "Hurt me... Tease me... Make me cry... But in the end of my life when i die plz keep one rose on my grave and say STUPID I MISS U.. HAVE A NICE DAY BSLVYL\n",
      "4303\n",
      "Aiyar hard 2 type. U later free then tell me then i call n scold n tell u.\n",
      "4306\n",
      "Yo come over carlos will be here soon\n",
      "4308\n",
      "I guess it is useless calling u 4 something important.\n",
      "4309\n",
      "Ha ha - had popped down to the loo when you hello-ed me. Hello!\n",
      "4313\n",
      "I keep ten rs in my shelf:) buy two egg.\n",
      "4314\n",
      "I wasn't well babe, i have swollen glands at my throat ... What did you end up doing ?\n",
      "4315\n",
      "Is ur changes 2 da report big? Cos i've already made changes 2 da previous report.\n",
      "4316\n",
      "Captain is in our room:)\n",
      "4317\n",
      "I can't speak, bcaz mobile have problem. I can listen you but you cann't listen my voice. So i calls you later.\n",
      "4318\n",
      "HIYA STU WOT U UP 2.IM IN SO MUCH TRUBLE AT HOME AT MOMENT EVONE HATES ME EVEN U! WOT THE HELL AV I DONE NOW? Y WONT U JUST TELL ME TEXT BCK PLEASE LUV DAN \n",
      "4320\n",
      "Are you still playing with gautham?\n",
      "4321\n",
      "Hey mr  and I are going to the sea view and having a couple of gays I mean games! Give me a bell when ya finish \n",
      "4322\n",
      "K, jason says he's gonna be around so I'll be up there around  &lt;#&gt;\n",
      "4323\n",
      "Sorry . I will be able to get to you. See you in the morning.\n",
      "4324\n",
      "Aight well keep me informed\n",
      "4325\n",
      "I am not having her number sir\n",
      "4326\n",
      "Am only searching for good dual sim mobile pa.\n",
      "4330\n",
      "1Apple/Day=No Doctor. 1Tulsi Leaf/Day=No Cancer. 1Lemon/Day=No Fat. 1Cup Milk/day=No Bone Problms 3 Litres Watr/Day=No Diseases Snd ths 2 Whom U Care..:-)\n",
      "4331\n",
      "i thought we were doing a king of the hill thing there.\n",
      "4333\n",
      "ALSO TELL HIM I SAID HAPPY BIRTHDAY\n",
      "4336\n",
      "Now u sound like manky scouse boy steve,like! I is travelling on da bus home.wot has u inmind 4 recreation dis eve?\n",
      "4337\n",
      "Fyi I'm taking a quick shower, be at epsilon in like  &lt;#&gt;  min\n",
      "4341\n",
      "I want to sent  &lt;#&gt; mesages today. Thats y. Sorry if i hurts\n",
      "4342\n",
      "Ü all write or wat..\n",
      "4345\n",
      "Hi:)did you asked to waheeda fathima about leave?\n",
      "4346\n",
      "Enjoy urself tmr...\n",
      "4347\n",
      "You still around? I could use a half-8th\n",
      "4349\n",
      "You give us back my id proof and  &lt;#&gt;  rs. We wont allow you to work. We will come to your home within days\n",
      "4350\n",
      "Ü bot notes oredi... Cos i juz rem i got...\n",
      "4351\n",
      "Yes. Rent is very expensive so its the way we save.\n",
      "4352\n",
      "Night has ended for another day, morning has come in a special way. May you smile like the sunny rays and leaves your worries at the blue blue bay. Gud mrng\n",
      "4353\n",
      "Hows the pain dear?y r u smiling?\n",
      "4356\n",
      "Sorry, I can't help you on this.\n",
      "4358\n",
      "HELLOGORGEOUS, HOWS U? MY FONE WAS ON CHARGE LST NITW WEN U TEXD ME. HOPEU AD A NICE WKEND AS IM SURE U DID LOOKIN 4WARD 2 C-IN U 2MRW LUV JAZ\n",
      "4360\n",
      "Ü only send me the contents page...\n",
      "4362\n",
      "Don't Think About \"What u Have Got\" Think About \"How to Use It That You Have Got\" gooD ni8\n",
      "4363\n",
      "I can't right this second, gotta hit people up first\n",
      "4364\n",
      "Evry Emotion dsn't hav Words.Evry Wish dsn't hav Prayrs.. If u Smile,D World is wit u.Othrwise even d Drop of Tear dsn't lik 2 Stay wit u.So b happy.. Good morning, keep smiling:-)\n",
      "4365\n",
      "So what about you. What do you remember\n",
      "4366\n",
      "Ujhhhhhhh computer shipped out with address to sandiago and parantella lane. Wtf. Poop.\n",
      "4367\n",
      "Mm yes dear look how i am hugging you both. :-P\n",
      "4369\n",
      "1 I don't have her number and 2 its gonna be a massive pain in the ass and i'd rather not get involved if that's possible\n",
      "4370\n",
      "Anytime lor...\n",
      "4372\n",
      "Purity of friendship between two is not about smiling after reading the forwarded message..Its about smiling just by seeing the name. Gud evng\n",
      "4374\n",
      "Me fine..absolutly fine\n",
      "4375\n",
      "K and you're sure I don't have to have consent forms to do it :V\n",
      "4378\n",
      "How much is torch in 9ja.\n",
      "4379\n",
      "Doing nothing, then u not having dinner w us?\n",
      "4381\n",
      "Done it but internet connection v slow and can‘t send it. Will try again later or first thing tomo.\n",
      "4382\n",
      "Mathews or tait or edwards or anderson\n",
      "4383\n",
      "yeah sure thing mate haunt got all my stuff sorted but im going sound anyway promoting hex for .by the way who is this? dont know number. Joke\n",
      "4384\n",
      "No need lar i go engin? Cos my sis at arts today...\n",
      "4385\n",
      "Thanks honey but still haven't heard anything I will leave it a bit longer so not 2 crowd him and will try later - great advice thanks hope cardiff is still there!\n",
      "4387\n",
      ", im .. On the snowboarding trip. I was wondering if your planning to get everyone together befor we go..a meet and greet kind of affair? Cheers, \n",
      "4388\n",
      "S.i'm watching it in live..\n",
      "4389\n",
      "see you then, we're all christmassy here!\n",
      "4391\n",
      "Do you know why god created gap between your fingers..? So that, One who is made for you comes &amp; fills those gaps by holding your hand with LOVE..!\n",
      "4392\n",
      "The greatest test of courage on earth is to bear defeat without losing heart....gn tc\n",
      "4393\n",
      "what are your new years plans?\n",
      "4396\n",
      "Only just got this message, not ignoring you. Yes, i was. Shopping that is\n",
      "4399\n",
      "Can you tell Shola to please go to college of medicine and visit the academic department, tell the academic secretary what the current situation is and ask if she can transfer there. She should ask someone to check Sagamu for the same thing and lautech. Its vital she completes her medical education in Nigeria. Its less expensive much less expensive. Unless she will be getting citizen rates in new zealand.\n",
      "4401\n",
      "Juz go google n search 4 qet...\n",
      "4404\n",
      "Just getting back home\n",
      "4405\n",
      "Sorry, I'll call later  &lt;#&gt; mins\n",
      "4412\n",
      "Ya but it cant display internal subs so i gotta extract them\n",
      "4414\n",
      "Sad story of a Man - Last week was my b'day. My Wife did'nt wish me. My Parents forgot n so did my Kids . I went to work. Even my Colleagues did not wish.\n",
      "4416\n",
      "Yeah I should be able to, I'll text you when I'm ready to meet up\n",
      "4417\n",
      "V skint too but fancied few bevies.waz gona go meet &othrs in spoon but jst bin watchng planet earth&sofa is v comfey; If i dont make it hav gd night\n",
      "4418\n",
      " says that he's quitting at least5times a day so i wudn't take much notice of that. Nah, she didn't mind. Are you gonna see him again? Do you want to come to taunton tonight? U can tell me all about !\n",
      "4419\n",
      "When you get free, call me\n",
      "4420\n",
      "How have your little darlings been so far this week? Need a coffee run tomo?Can't believe it's that time of week already …\n",
      "4423\n",
      "MMM ... Fuck .... Merry Christmas to me\n",
      "4425\n",
      "Update your face book status frequently :)\n",
      "4426\n",
      "Just now saw your message.it k da:)\n",
      "4427\n",
      "Was it something u ate?\n",
      "4428\n",
      "So what did the bank say about the money?\n",
      "4429\n",
      "Aiyar dun disturb u liao... Thk u have lots 2 do aft ur cupboard come...\n",
      "4430\n",
      "Hey they r not watching movie tonight so i'll prob b home early...\n",
      "4431\n",
      "Yar lor... How u noe? U used dat route too?\n",
      "4432\n",
      "2mro i am not coming to gym machan. Goodnight.\n",
      "4434\n",
      "Can u look 4 me in da lib i got stuff havent finish yet.\n",
      "4435\n",
      "Sounds great! Im going to sleep now. Have a good night!\n",
      "4437\n",
      "House-Maid is the murderer, coz the man was murdered on  &lt;#&gt; th January.. As public holiday all govt.instituitions are closed,including post office..understand?\n",
      "4438\n",
      "How come u got nothing to do?\n",
      "4439\n",
      "Nothing will ever be easy. But don't be looking for a reason not to take a risk on life and love\n",
      "4440\n",
      "i want to grasp your pretty booty :)\n",
      "4441\n",
      "I've got it down to a tea. not sure which flavour\n",
      "4442\n",
      "I'm going 2 orchard now laready me reaching soon. U reaching?\n",
      "4444\n",
      "You know my old Dom I told you about yesterday ? His name is Roger? He got in touch with me last night and wants me to meet him today at 2 pm\n",
      "4445\n",
      "COME BACK TO TAMPA FFFFUUUUUUU\n",
      "4446\n",
      "2 celebrate my bday, y else?\n",
      "4447\n",
      "Merry christmas to u too annie!\n",
      "4449\n",
      "I sent them. Do you like?\n",
      "4451\n",
      "Awesome, be there in a minute\n",
      "4453\n",
      "I've told you everything will stop. Just dont let her get dehydrated.\n",
      "4454\n",
      "Or I guess  &lt;#&gt;  min\n",
      "4455\n",
      "I'm home. Ard wat time will u reach?\n",
      "4456\n",
      "Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@\n",
      "4457\n",
      "If you want to mapquest it or something look up \"usf dogwood drive\", that's the tiny street where the parking lot is\n",
      "4459\n",
      "Die... I accidentally deleted e msg i suppose 2 put in e sim archive. Haiz... I so sad...\n",
      "4462\n",
      "Thanks again for your reply today. When is ur visa coming in. And r u still buying the gucci and bags. My sister things are not easy, uncle john also has his own bills so i really need to think about how to make my own money. Later sha.\n",
      "4463\n",
      "Sorry I flaked last night, shit's seriously goin down with my roommate, what you up to tonight?\n",
      "4464\n",
      "He said i look pretty wif long hair wat. But i thk he's cutting quite short 4 me leh.\n",
      "4466\n",
      "CHEERS FOR CALLIN BABE.SOZI CULDNT TALKBUT I WANNATELL U DETAILS LATER WENWECAN CHAT PROPERLY X\n",
      "4467\n",
      "Hey u still at the gym?\n",
      "4469\n",
      "Much better now thanks lol\n",
      "4471\n",
      "Lemme know when I can swing by and pick up, I'm free basically any time after 1 all this semester\n",
      "4472\n",
      "Wa... U so efficient... Gee... Thanx...\n",
      "4476\n",
      "We will meet soon princess! Ttyl!\n",
      "4477\n",
      "I'll pick you up at about 5.15pm to go to taunton if you still want to come.\n",
      "4478\n",
      "Oh :-)only 4 outside players allowed to play know\n",
      "4479\n",
      "I anything lor.\n",
      "4480\n",
      "Erutupalam thandiyachu\n",
      "4481\n",
      "Y cant u try new invention to fly..i'm not joking.,\n",
      "4482\n",
      "No..its ful of song lyrics..\n",
      "4483\n",
      "What do u reckon as need 2 arrange transport if u can't do it, thanks\n",
      "4484\n",
      "True lov n care wil nevr go unrecognized. though somone often makes mistakes when valuing it. but they will definitly undrstnd once when they start missing it.\n",
      "4485\n",
      "Shopping? Eh ger i toking abt syd leh...Haha\n",
      "4486\n",
      "What not under standing.\n",
      "4487\n",
      "have * good weekend.\n",
      "4488\n",
      "Miss call miss call khelate kintu opponenter miss call dhorte lage. Thats d rule. One with great phone receiving quality wins.\n",
      "4489\n",
      "Call me when you get the chance plz &lt;3\n",
      "4490\n",
      "The new deus ex game comin early next yr\n",
      "4492\n",
      "My friend, she's studying at warwick, we've planned to go shopping and to concert tmw, but it may be canceled, havn't seen  for ages, yeah we should get together sometime!\n",
      "4493\n",
      "Probably a couple hours tops\n",
      "4496\n",
      "Hope this text meets you smiling. If not then let this text give you a reason to smile. Have a beautiful day.\n",
      "4498\n",
      "Ok\n",
      "4504\n",
      "Stupid.its not possible\n",
      "4505\n",
      "She told to hr that he want posting in chennai:)because i'm working here:)\n",
      "4508\n",
      "He neva grumble but i sad lor... Hee... Buy tmr lor aft lunch. But we still meetin 4 lunch tmr a not. Neva hear fr them lei. Ü got a lot of work ar?\n",
      "4509\n",
      "Not able to do anything.\n",
      "4510\n",
      "Ü takin linear algebra today?\n",
      "4511\n",
      "This weekend is fine (an excuse not to do too much decorating)\n",
      "4512\n",
      "Sorry I missed you babe. I was up late and slept in. I hope you enjoy your driving lesson, boytoy. I miss you too ... *teasing kiss*\n",
      "4513\n",
      "Now project pa. After that only i can come.\n",
      "4515\n",
      "Sure, whenever you show the fuck up &gt;:(\n",
      "4516\n",
      "That was random saw my old roomate on campus. He graduated\n",
      "4519\n",
      "That sucks. So what do you got planned for your yo valentine? I am your yo valentine aren't I?\n",
      "4521\n",
      "What to think no one saying clearly. Ok leave no need to ask her. I will go if she come or not\n",
      "4524\n",
      "Actually I decided I was too hungry so I haven't left yet :V\n",
      "4525\n",
      "I've sent ü my part..\n",
      "4526\n",
      "Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
      "4528\n",
      "Understand. his loss is my gain :) so do you work? School?\n",
      "4529\n",
      "HOW ARE U? I HAVE MISSED U! I HAVENT BEEN UP 2 MUCH A BIT BORED WITH THE HOLIDAY WANT 2 GO BAK 2 COLLEGE! SAD ISNT IT?xx\n",
      "4531\n",
      "Don't forget though that I love you .... And I walk beside you. Watching over you and keeping your heart warm.\n",
      "4533\n",
      "Ok both our days. So what are you making for dinner tonite? Am I invited?\n",
      "4535\n",
      "I have no money 4 steve mate! !\n",
      "4536\n",
      "IM LATE TELLMISS IM ON MY WAY\n",
      "4537\n",
      "Never blame a day in ur life. Good days give u happiness. Bad days give u experience. Both are essential in life! All are Gods blessings! good morning.:\n",
      "4538\n",
      "Normally i use to drink more water daily:)\n",
      "4539\n",
      "Dare i ask... Any luck with sorting out the car?\n",
      "4546\n",
      "Never y lei... I v lazy... Got wat? Dat day ü send me da url cant work one...\n",
      "4547\n",
      "Never try alone to take the weight of a tear that comes out of ur heart and falls through ur eyes... Always remember a STUPID FRIEND is here to share... BSLVYL\n",
      "4549\n",
      "Hope you are having a good week. Just checking in\n",
      "4550\n",
      "Haha, my friend tyler literally just asked if you could get him a dubsack\n",
      "4551\n",
      "Hey! do u fancy meetin me at 4 at cha  hav a lil beverage on me. if not txt or ring me and we can meet up l8r. quite tired got in at 3 v.pist ;) love Pete x x x\n",
      "4554\n",
      "Sun ah... Thk mayb can if dun have anythin on... Thk have to book e lesson... E pilates is at orchard mrt u noe hor...  \n",
      "4555\n",
      "Try to do something dear. You read something for exams\n",
      "4557\n",
      "Gettin rdy to ship comp\n",
      "4559\n",
      "PISS IS TALKING IS SOMEONE THAT REALISE U THAT POINT THIS AT IS IT.(NOW READ IT BACKWARDS)\n",
      "4560\n",
      "Think + da. You wil do.\n",
      "4562\n",
      "Good afternoon my boytoy. How goes that walking here and there day ? Did you get that police abstract? Are you still out and about? I wake and miss you babe\n",
      "4567\n",
      "Should i buy him a blackberry bold 2 or torch. Should i buy him new or used. Let me know. Plus are you saying i should buy the  &lt;#&gt; g wifi ipad. And what are you saying about the about the  &lt;#&gt; g?\n",
      "4568\n",
      "But you were together so you should be thinkin about him\n",
      "4569\n",
      "hiya hows it going in sunny africa? hope u r avin a good time. give that big old silver back a big kiss from me.\n",
      "4570\n",
      "At WHAT TIME should i come tomorrow\n",
      "4572\n",
      "CHA QUITEAMUZING THATSCOOL BABE,PROBPOP IN & CU SATTHEN HUNNY 4BREKKIE! LOVE JEN XXX. PSXTRA LRG PORTIONS 4 ME PLEASE \n",
      "4575\n",
      ":( but your not here....\n",
      "4576\n",
      "Not directly behind... Abt 4 rows behind ü...\n",
      "4580\n",
      "No plans yet. What are you doing ?\n",
      "4585\n",
      "Noooooooo please. Last thing I need is stress. For once in your life be fair.\n",
      "4593\n",
      "Right it wasnt you who phoned it was someone with a number like yours!\n",
      "4594\n",
      "It's ok i wun b angry. Msg u aft i come home tonight.\n",
      "4595\n",
      "I had a good time too. Its nice to do something a bit different with my weekends for a change. See ya soon\n",
      "4596\n",
      "Yo sorry was in the shower sup\n",
      "4598\n",
      "Full heat pa:-) i have applyed oil pa.\n",
      "4599\n",
      "I'm stuck in da middle of da row on da right hand side of da lt... \n",
      "4600\n",
      "Have you laid your airtel line to rest?\n",
      "4601\n",
      "Hi did u decide wot 2 get 4 his bday if not ill prob jus get him a voucher frm virgin or sumfing \n",
      "4603\n",
      "Hey j! r u feeling any better, hopeSo hunny. i amnow feelin ill & ithink i may have tonsolitusaswell! damn iam layin in bedreal bored. lotsof luv me xxxx\n",
      "4604\n",
      "And I don't plan on staying the night but I prolly won't be back til late\n",
      "4605\n",
      "THANX 4 PUTTIN DA FONE DOWN ON ME!!\n",
      "4606\n",
      "I need an 8th but I'm off campus atm, could I pick up in an hour or two?\n",
      "4609\n",
      "We live in the next  &lt;#&gt; mins\n",
      "4610\n",
      "Y de asking like this.\n",
      "4611\n",
      "Just glad to be talking to you.\n",
      "4612\n",
      "Wat time ü finish?\n",
      "4613\n",
      "Sorry da. I gone mad so many pending works what to do.\n",
      "4614\n",
      "How much you got for cleaning\n",
      "4615\n",
      "hows my favourite person today? r u workin hard? couldn't sleep again last nite nearly rang u at 4.30\n",
      "4617\n",
      "Ü called dad oredi...\n",
      "4618\n",
      "Good. do you think you could send me some pix? I would love to see your top and bottom...\n",
      "4619\n",
      "Nvm... I'm going to wear my sport shoes anyway... I'm going to be late leh.\n",
      "4621\n",
      "THIS IS A LONG FUCKIN SHOWR\n",
      "4622\n",
      "Received, understood n acted upon!\n",
      "4623\n",
      "They finally came to fix the ceiling.\n",
      "4625\n",
      "Jus finish blowing my hair. U finish dinner already?\n",
      "4627\n",
      "Lol ... I knew that .... I saw him in the dollar store\n",
      "4630\n",
      "Only saturday and sunday holiday so its very difficult:)\n",
      "4632\n",
      "Got hella gas money, want to go on a grand nature adventure with galileo in a little bit?\n",
      "4635\n",
      "These won't do. Have to move on to morphine\n",
      "4636\n",
      "How come i din c ü... Yup i cut my hair...\n",
      "4639\n",
      "Captain vijaykanth is doing comedy in captain tv..he is drunken :)\n",
      "4641\n",
      "Do you hide anythiing or keeping distance from me\n",
      "4644\n",
      "Sorry i din lock my keypad.\n",
      "4645\n",
      "Did u got that persons story\n",
      "4646\n",
      "Are you planning to come chennai?\n",
      "4649\n",
      "We are okay. Going to sleep now. Later\n",
      "4650\n",
      "Please protect yourself from e-threats. SIB never asks for sensitive information like Passwords,ATM/SMS PIN thru email. Never share your password with anybody.\n",
      "4651\n",
      "Finally it has happened..! Aftr decades..! BEER is now cheaper than PETROL! The goverment expects us to \"DRINK\". . . But don't \"DRIVE \"\n",
      "4653\n",
      "Where r e meeting tmr?\n",
      "4654\n",
      "Lol yes. But it will add some spice to your day.\n",
      "4655\n",
      "Hope you are having a great day.\n",
      "4656\n",
      "Our Prasanth ettans mother passed away last night. Just pray for her and family.\n",
      "4657\n",
      "K, I'll work something out\n",
      "4659\n",
      "This message is from a great Doctor in India:-): 1) Do not drink APPY FIZZ. It contains Cancer causing age\n",
      "4661\n",
      "You call him and tell now infront of them. Call him now.\n",
      "4662\n",
      "Ok no prob...\n",
      "4664\n",
      "No. Yes please. Been swimming?\n",
      "4665\n",
      "Mum not going robinson already.\n",
      "4666\n",
      "Ok set let u noe e details later...\n",
      "4668\n",
      "I send the print  outs da.\n",
      "4670\n",
      "When I was born, GOD said, \"Oh No! Another IDIOT\". When you were born, GOD said, \"OH No! COMPETITION\". Who knew, one day these two will become FREINDS FOREVER!\n",
      "4672\n",
      "Probably not, I'm almost out of gas and I get some cash tomorrow\n",
      "4674\n",
      "I forgot 2 ask ü all smth.. There's a card on da present lei... How? Ü all want 2 write smth or sign on it?\n",
      "4675\n",
      "I'm leaving my house now.\n",
      "4677\n",
      "Ü ready then call me...\n",
      "4680\n",
      "Sry da..jst nw only i came to home..\n",
      "4681\n",
      "That's cool he'll be here all night, lemme know when you're around\n",
      "4682\n",
      "Are you staying in town ?\n",
      "4683\n",
      "Haha yeah, 2 oz is kind of a shitload\n",
      "4684\n",
      "Ok u can take me shopping when u get paid =D\n",
      "4685\n",
      "My life Means a lot to me, Not because I love my life, But because I love the people in my life, The world calls them friends, I call them my World:-).. Ge:-)..\n",
      "4686\n",
      "Alright we'll bring it to you, see you in like  &lt;#&gt;  mins\n",
      "4687\n",
      "But pls dont play in others life.\n",
      "4688\n",
      "Eatin my lunch...\n",
      "4689\n",
      "Hmmm.but you should give it on one day..\n",
      "4690\n",
      "Didn't try, g and I decided not to head out\n",
      "4691\n",
      "Ok no prob\n",
      "4692\n",
      "Surly ill give it to you:-) while coming to review.\n",
      "4693\n",
      "By march ending, i should be ready. But will call you for sure. The problem is that my capital never complete. How far with you. How's work and the ladies\n",
      "4694\n",
      "Tessy..pls do me a favor. Pls convey my birthday wishes to Nimya..pls dnt forget it. Today is her birthday Shijas\n",
      "4697\n",
      "A guy who gets used but is too dumb to realize it.\n",
      "4698\n",
      "Okey dokey, i‘ll be over in a bit just sorting some stuff out.\n",
      "4699\n",
      "Don no da:)whats you plan?\n",
      "4700\n",
      "Yes fine \n",
      "4702\n",
      "I liked the new mobile\n",
      "4704\n",
      "Mmmmmmm *snuggles into you* ...*deep contented sigh* ... *whispers* ... I fucking love you so much I can barely stand it ...\n",
      "4705\n",
      "Yar but they say got some error.\n",
      "4707\n",
      "Wow so healthy. Old airport rd lor. Cant thk of anything else. But i'll b bathing my dog later.\n",
      "4708\n",
      "Wif my family booking tour package.\n",
      "4709\n",
      "Did you say bold, then torch later. Or one torch and 2bold?\n",
      "4711\n",
      "Ya i knw u vl giv..its ok thanks kano..anyway enjoy wit ur family wit 1st salary..:-);-)\n",
      "4712\n",
      "Huh so slow i tot u reach long ago liao... U 2 more days only i 4 more leh...\n",
      "4714\n",
      "Big brother‘s really scraped the barrel with this shower of social misfits\n",
      "4715\n",
      "Oops i thk i dun haf enuff... I go check then tell ü..\n",
      "4716\n",
      "S:)8 min to go for lunch:)\n",
      "4717\n",
      "Hey. What happened? U switch off ur cell d whole day. This isnt good. Now if u do care, give me a call tomorrow.\n",
      "4718\n",
      "K will do, addie &amp; I are doing some art so I'll be here when you get home\n",
      "4719\n",
      "My uncles in Atlanta. Wish you guys a great semester.\n",
      "4720\n",
      "Aiyo... Her lesson so early... I'm still sleepin, haha... Okie, u go home liao den confirm w me lor...\n",
      "4721\n",
      "Forgot to tell ü smth.. Can ü like number the sections so that it's clearer..\n",
      "4723\n",
      "I'm home, my love ... If your still awake ... *loving kiss*\n",
      "4727\n",
      "Jason says it's cool if we pick some up from his place in like an hour\n",
      "4730\n",
      "I've reached already.\n",
      "4731\n",
      "I dont know ask to my brother. Nothing problem some thing that. Just i told .\n",
      "4732\n",
      "K:)eng rocking in ashes:)\n",
      "4733\n",
      "Wat time r ü going to xin's hostel?\n",
      "4737\n",
      "Not for possession, especially not first offense\n",
      "4738\n",
      "Nt only for driving even for many reasons she is called BBD..thts it chikku, then hw abt dvg cold..heard tht vinobanagar violence hw is the condition..and hw ru ? Any problem?\n",
      "4739\n",
      "I bought the test yesterday. Its something that lets you know the exact day u ovulate.when will get 2u in about 2 to 3wks. But pls pls dont fret. I know u r worried. Pls relax. Also is there anything in ur past history u need to tell me?\n",
      "4741\n",
      "I keep seeing weird shit and bein all \"woah\" then realising it's actually reasonable and I'm all \"oh\"\n",
      "4742\n",
      "Many more happy returns of the day. I wish you happy birthday.\n",
      "4743\n",
      "Ya very nice. . .be ready on thursday\n",
      "4747\n",
      "Orh i tot u say she now still dun believe.\n",
      "4748\n",
      "When you just put in the + sign, choose my number and the pin will show. Right?\n",
      "4750\n",
      "Thanx u darlin!im cool thanx. A few bday drinks 2 nite. 2morrow off! Take care c u soon.xxx\n",
      "4751\n",
      "If you're still up, maybe leave the credit card so I can get gas when I get back like he told me to\n",
      "4753\n",
      "Well boy am I glad G wasted all night at applebees for nothing\n",
      "4761\n",
      "I'm home. Doc gave me pain meds says everything is fine.\n",
      "4764\n",
      "Prepare to be pleasured :)\n",
      "4765\n",
      "Hi.:)technical support.providing assistance to us customer through call and email:)\n",
      "4767\n",
      "Whens your radio show?\n",
      "4769\n",
      "I'm not sure if its still available though\n",
      "4771\n",
      "CHEERS LOU! YEAH WAS A GOODNITE SHAME U NEVA CAME! C YA GAILxx\n",
      "4772\n",
      "Hi..i got the money da:)\n",
      "4773\n",
      "Hi, Mobile no.  &lt;#&gt;  has added you in their contact list on www.fullonsms.com It s a great place to send free sms to people For more visit fullonsms.com\n",
      "4774\n",
      "Ok then u tell me wat time u coming later lor.\n",
      "4775\n",
      "U repeat e instructions again. Wat's e road name of ur house?\n",
      "4776\n",
      "So many people seems to be special at first sight, But only very few will remain special to you till your last sight.. Maintain them till life ends.. Sh!jas\n",
      "4778\n",
      "Sorry completely forgot * will pop em round this week if your still here?\n",
      "4779\n",
      "U R THE MOST BEAUTIFUL GIRL IVE EVER SEEN. U R MY BABY COME AND C ME IN THE COMMON ROOM\n",
      "4780\n",
      "O we cant see if we can join denis and mina? Or does denis want alone time\n",
      "4781\n",
      "Sen told that he is going to join his uncle finance in cbe\n",
      "4782\n",
      "Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \n",
      "4783\n",
      "Call me, i am senthil from hsbc.\n",
      "4784\n",
      "Especially since i talk about boston all up in my personal statement, lol! I woulda changed that if i had realized it said nyc! It says boston now.\n",
      "4790\n",
      "We're on the opposite side from where we dropped you off\n",
      "4791\n",
      "Yup. Izzit still raining heavily cos i'm in e mrt i can't c outside.\n",
      "4792\n",
      "Send me your resume:-)\n",
      "4793\n",
      "Gd luck 4 ur exams :-)\n",
      "4794\n",
      "Or u ask they all if next sat can a not. If all of them can make it then i'm ok lor.\n",
      "4795\n",
      "Sorry that was my uncle. I.ll keep in touch\n",
      "4796\n",
      "Saw Guys and Dolls last night with Patrick Swayze it was great\n",
      "4799\n",
      "Just come home. I don't want u to be miserable\n",
      "4800\n",
      "I dont know why she.s not getting your messages\n",
      "4801\n",
      "its cool but tyler had to take off so we're gonna buy for him and drop it off at his place later tonight. Our total order is a quarter, you got enough?\n",
      "4803\n",
      "Reverse is cheating. That is not mathematics.\n",
      "4804\n",
      "How do you plan to manage that\n",
      "4805\n",
      "Er, hello, things didn‘t quite go to plan – is limping slowly home followed by aa and with exhaust hanging off\n",
      "4806\n",
      "Sorry for the delay. Yes masters\n",
      "4807\n",
      "Call me when u finish then i come n pick u.\n",
      "4809\n",
      "What's up my own oga. Left my phone at home and just saw ur messages. Hope you are good. Have a great weekend.\n",
      "4810\n",
      "Don't worry though, I understand how important it is that I be put in my place with a poorly thought out punishment in the face of the worst thing that has ever happened to me. Brb gonna go kill myself\n",
      "4812\n",
      "E admin building there? I might b slightly earlier... I'll call u when i'm reaching...\n",
      "4814\n",
      "i can call in  &lt;#&gt;  min if thats ok\n",
      "4816\n",
      "Ü no home work to do meh... \n",
      "4819\n",
      "How's ur paper?\n",
      "4824\n",
      ":-) :-)\n",
      "4825\n",
      "Not thought bout it... || Drink in tap & spile at seven. || Is that pub on gas st off broad st by canal. || Ok?\n",
      "4829\n",
      "Lol no. Just trying to make your day a little more interesting\n",
      "4832\n",
      "Po de :-):):-):-):-). No need job aha.\n",
      "4836\n",
      "OH RITE. WELL IM WITH MY BEST MATE PETE, WHO I WENT OUT WITH 4 A WEEK+ NOW WERE 2GEVA AGAIN. ITS BEEN LONGER THAN A WEEK.\n",
      "4837\n",
      "Yay can't wait to party together!\n",
      "4838\n",
      "....photoshop makes my computer shut down.\n",
      "4840\n",
      "That's one of the issues but california is okay. No snow so its manageable\n",
      "4842\n",
      "Hmmm.... Mayb can try e shoppin area one, but forgot e name of hotel...\n",
      "4844\n",
      "I need details about that online job.\n",
      "4846\n",
      "Missing you too.pray inshah allah\n",
      "4847\n",
      "Pls help me tell Ashley that i cant find her number oh\n",
      "4848\n",
      "I am in escape theatre now. . Going to watch KAVALAN in a few minutes\n",
      "4850\n",
      "either way works for me. I am  &lt;#&gt;  years old. Hope that doesnt bother you.\n",
      "4851\n",
      "Maybe you should find something else to do instead???\n",
      "4852\n",
      "Gain the rights of a wife.dont demand it.i am trying as husband too.Lets see\n",
      "4853\n",
      "I liked your new house\n",
      "4854\n",
      "I'm fine. Hope you are also\n",
      "4855\n",
      "Also north carolina and texas atm, you would just go to the gre site and pay for the test results to be sent.\n",
      "4857\n",
      "yes baby! I need to stretch open your pussy!\n",
      "4859\n",
      "Ok...\n",
      "4861\n",
      "Response is one of d powerful weapon 2 occupy a place in others 'HEART'... So, always give response 2 who cares 4 U\"... Gud night..swt dreams..take care\n",
      "4862\n",
      "Nokia phone is lovly..\n",
      "4865\n",
      "Sorry da..today i wont come to play..i have driving clas..\n",
      "4867\n",
      "Oh! Shit, I thought that was your trip! Loooooool ... That just makes SO much more sense now ... *grins* and the sofa reference was ... The \"sleep on a couch\" link you sent me ... Wasn't that how you went on your trip ? Oh ... And didn't your babe go with you for that celebration with your rents?\n",
      "4868\n",
      "Okey dokey swashbuckling stuff what oh.\n",
      "4869\n",
      "Watching cartoon, listening music &amp; at eve had to go temple &amp; church.. What about u?\n",
      "4870\n",
      "1. Tension face 2. Smiling face 3. Waste face 4. Innocent face 5.Terror face 6.Cruel face 7.Romantic face 8.Lovable face 9.decent face  &lt;#&gt; .joker face.\n",
      "4871\n",
      "Dip's cell dead. So i m coming with him. U better respond else we shall come back.\n",
      "4872\n",
      "Well. You know what i mean. Texting\n",
      "4873\n",
      "Hi dis is yijue i would be happy to work wif ü all for gek1510...\n",
      "4874\n",
      "Lol! Oops sorry! Have fun. \n",
      "4875\n",
      "Wat happened to the cruise thing\n",
      "4876\n",
      "I know dat feelin had it with Pete! Wuld get with em , nuther place nuther time mayb?\n",
      "4878\n",
      "The world's most happiest frnds never have the same characters... Dey just have the best understanding of their differences...\n",
      "4880\n",
      "Yeah just open chat and click friend lists. Then make the list. Easy as pie\n",
      "4881\n",
      "alright tyler's got a minor crisis and has to be home sooner than he thought so be here asap\n",
      "4883\n",
      "As usual u can call me ard 10 smth.\n",
      "4884\n",
      "New Theory: Argument wins d SITUATION, but loses the PERSON. So dont argue with ur friends just.. . . . kick them &amp; say, I'm always correct.!\n",
      "4885\n",
      "For many things its an antibiotic and it can be used for chest abdomen and gynae infections even bone infections.\n",
      "4886\n",
      "Poor girl can't go one day lmao\n",
      "4887\n",
      "Or just do that 6times\n",
      "4889\n",
      "You have to pls make a note of all she.s exposed to. Also find out from her school if anyone else was vomiting. Is there a dog or cat in the house? Let me know later.\n",
      "4892\n",
      "Its worse if if uses half way then stops. Its better for him to complete it.\n",
      "4893\n",
      "Miserable. They don't tell u that the side effects of birth control are massive gut wrenching cramps for the first 2 months. I didn't sleep at all last night.\n",
      "4894\n",
      "Send me the new number\n",
      "4895\n",
      "Convey my regards to him\n",
      "4897\n",
      "2 and half years i missed your friendship:-)\n",
      "4898\n",
      "I cant pick the phone right now. Pls send a message\n",
      "4899\n",
      "Oh for fuck's sake she's in like tallahassee\n",
      "4900\n",
      "Haha, that was the first person I was gonna ask\n",
      "4902\n",
      "Taka lor. Wat time u wan 2 come n look 4 us?\n",
      "4904\n",
      "I;m reaching in another 2 stops.\n",
      "4905\n",
      "no, i *didn't* mean to post it. I wrote it, and like so many other times i've ritten stuff to you, i let it sit there. it WAS what i was feeling at the time. I was angry. Before i left, i hit send, then stop. It wasn't there. I checked on my phone when i got to my car. It wasn't there. You said you didn't sleep, you were bored. So why wouldn't THAT be the time to clean, fold laundry, etc.? At least make the bed?\n",
      "4907\n",
      "Will you come online today night\n",
      "4909\n",
      "I'm in solihull, | do you want anything?\n",
      "4910\n",
      "Will do. Have a good day\n",
      "4911\n",
      "WE REGRET TO INFORM U THAT THE NHS HAS MADE A MISTAKE.U WERE NEVER ACTUALLY BORN.PLEASE REPORT 2 YOR LOCAL HOSPITAL 2B TERMINATED.WE R SORRY 4 THE INCONVENIENCE\n",
      "4913\n",
      "I am on the way to tirupur.\n",
      "4915\n",
      "You've already got a flaky parent. It'snot supposed to be the child's job to support the parent...not until they're The Ride age anyway. I'm supposed to be there to support you. And now i've hurt you. unintentional. But hurt nonetheless.\n",
      "4919\n",
      "Sitting in mu waiting for everyone to get out of my suite so I can take a shower\n",
      "4920\n",
      "Re your call; You didn't see my facebook huh?\n",
      "4921\n",
      "G says you never answer your texts, confirm/deny\n",
      "4922\n",
      "Its so common hearin How r u? Wat r u doing? How was ur day? So let me ask u something different. Did u smile today? If not, do it now.... Gud evng.\n",
      "4926\n",
      "Ok... Let u noe when i leave my house.\n",
      "4929\n",
      "Just hopeing that wasn‘t too pissed up to remember and has gone off to his sisters or something!\n",
      "4932\n",
      "Good morning, my boytoy! How's those yummy lips ? Where's my sexy buns now ? What do you do ? Do you think of me ? Do you crave me ? Do you need me ?\n",
      "4933\n",
      "Match started.india  &lt;#&gt;  for 2\n",
      "4935\n",
      "Hey do you want anything to buy:)\n",
      "4937\n",
      "K..k.:)congratulation ..\n",
      "4938\n",
      "G wants to know where the fuck you are\n",
      "4939\n",
      "No it was cancelled yeah baby! Well that sounds important so i understand my darlin give me a ring later on this fone love Kate x\n",
      "4940\n",
      "Tomarrow i want to got to court. At  &lt;DECIMAL&gt; . So you come to bus stand at 9.\n",
      "4941\n",
      "Ü go home liao? Ask dad to pick me up at 6...\n",
      "4942\n",
      "Omg you can make a wedding chapel in frontierville? Why do they get all the good stuff?\n",
      "4944\n",
      "Check mail.i have mailed varma and kept copy to you regarding membership.take care.insha allah.\n",
      "4946\n",
      "Anyway I don't think I can secure anything up here, lemme know if you want me to drive down south and chill\n",
      "4950\n",
      "I am in bus on the way to calicut\n",
      "4951\n",
      "Hi its me you are probably having too much fun to get this message but i thought id txt u cos im bored! and james has been farting at me all night\n",
      "4954\n",
      "I lost 4 pounds since my doc visit last week woot woot! Now I'm gonna celebrate by stuffing my face!\n",
      "4956\n",
      "Doing my masters. When will you buy a bb cos i have for sale and how's bf\n",
      "4959\n",
      "Why didn't u call on your lunch?\n",
      "4961\n",
      "I want  &lt;#&gt;  rs da:)do you have it?\n",
      "4963\n",
      "Yup ok...\n",
      "4964\n",
      "I want to see your pretty pussy...\n",
      "4969\n",
      "Future is not what we planned for tomorrow.....! it is the result of what we do today...! Do the best in present... enjoy the future.\n",
      "4970\n",
      "I will cme i want to go to hos 2morow. After that i wil cme. This what i got from her dear what to do. She didnt say any time\n",
      "4971\n",
      "We are supposed to meet to discuss abt our trip... Thought xuhui told you? In the afternoon. Thought we can go for lesson after that\n",
      "4972\n",
      "Hey come online! Use msn... We are all there\n",
      "4973\n",
      "I'm fine. Hope you are good. Do take care.\n",
      "4975\n",
      "Aiyo u so poor thing... Then u dun wan 2 eat? U bathe already?\n",
      "4976\n",
      "Yar... I tot u knew dis would happen long ago already.\n",
      "4977\n",
      "You are gorgeous! keep those pix cumming :) thank you!\n",
      "4978\n",
      "A boy was late 2 home. His father: \"POWER OF FRNDSHIP\"\n",
      "4980\n",
      "Spending new years with my brother and his family. Lets plan to meet next week. Are you ready to be spoiled? :)\n",
      "4981\n",
      "So what u doing today?\n",
      "4983\n",
      "Slept? I thinkThis time ( &lt;#&gt;  pm) is not dangerous\n",
      "4988\n",
      "No rushing. I'm not working. I'm in school so if we rush we go hungry.\n",
      "4989\n",
      "Which channel:-):-):):-).\n",
      "4990\n",
      "So your telling me I coulda been your real Valentine and I wasn't? U never pick me for NOTHING!!\n",
      "4992\n",
      "We made it! Eta at taunton is 12:30 as planned, hope that‘s still okday?! Good to see you! :-xx\n",
      "4993\n",
      "I'm hungry buy smth home...\n",
      "4994\n",
      "HEY KATE, HOPE UR OK... WILL GIVE U A BUZ WEDLUNCH. GO OUTSOMEWHERE 4 ADRINK IN TOWN..CUD GO 2WATERSHD 4 A BIT? PPL FROMWRK WILL BTHERE. LOVE PETEXXX.\n",
      "4997\n",
      "Happy new year. Hope you are having a good semester\n",
      "4998\n",
      "Esplanade lor. Where else...\n",
      "4999\n",
      "Can you talk with me..\n",
      "5001\n",
      "Well its not like you actually called someone a punto. That woulda been worse.\n",
      "5002\n",
      "Nope. Since ayo travelled, he has forgotten his guy\n",
      "5006\n",
      "Guess which pub im in? Im as happy as a pig in clover or whatever the saying is! \n",
      "5007\n",
      "ILL B DOWN SOON\n",
      "5009\n",
      "Go fool dont cheat others ok\n",
      "5010\n",
      "My mobile number.pls sms ur mail id.convey regards to achan,amma.Rakhesh.Qatar\n",
      "5011\n",
      "By the way, 'rencontre' is to meet again. Mountains dont....\n",
      "5013\n",
      "U attend ur driving lesson how many times a wk n which day?\n",
      "5014\n",
      "Uncle G, just checking up on you. Do have a rewarding month\n",
      "5015\n",
      "Hello boytoy ! Geeee ... I'm missing you today. I like to send you a tm and remind you I'm thinking of you ... And you are loved ... *loving kiss*\n",
      "5016\n",
      "I think the other two still need to get cash but we can def be ready by 9\n",
      "5017\n",
      "Hey gals...U all wanna meet 4 dinner at nìte? \n",
      "5022\n",
      ":-( sad puppy noise\n",
      "5024\n",
      "Anyway holla at me whenever you're around because I need an excuse to go creep on people in sarasota\n",
      "5025\n",
      "Where you. What happen\n",
      "5026\n",
      "I was gonna ask you lol but i think its at 7\n",
      "5032\n",
      "Hey... Very inconvenient for your sis a not huh?\n",
      "5034\n",
      "* Was really good to see you the other day dudette, been missing you!\n",
      "5035\n",
      "I want to go to perumbavoor\n",
      "5036\n",
      "How many times i told in the stage all use to laugh. You not listen aha.\n",
      "5038\n",
      "(You didn't hear it from me)\n",
      "5039\n",
      "Thanks for being there for me just to talk to on saturday. You are very dear to me. I cherish having you as a brother and role model.\n",
      "5040\n",
      "Pls clarify back if an open return ticket that i have can be preponed for me to go back to kerala.\n",
      "5042\n",
      "She ran off with a younger man. we will make pretty babies together :)\n",
      "5045\n",
      "Dunno, my dad said he coming home 2 bring us out 4 lunch. Yup i go w u lor. I call u when i reach school lor...\n",
      "5048\n",
      "Hmmm.still we dont have opener?\n",
      "5049\n",
      "Yeah so basically any time next week you can get away from your mom &amp; get up before 3\n",
      "5050\n",
      "Edison has rightly said, \"A fool can ask more questions than a wise man can answer\" Now you know why all of us are speechless during ViVa.. GM,GN,GE,GNT:-)\n",
      "5052\n",
      "With my sis lor... We juz watched italian job.\n",
      "5054\n",
      "Lmao you know me so well...\n",
      "5056\n",
      "Am on a train back from northampton so i'm afraid not! I'm staying skyving off today ho ho! Will be around wednesday though. Do you fancy the comedy club this week by the way?\n",
      "5057\n",
      "Goodnight da thangam I really miss u dear.\n",
      "5059\n",
      "Geeeee ... Your internet is really bad today, eh ?\n",
      "5062\n",
      "Sorry, I'll call you  later. I am in meeting sir.\n",
      "5063\n",
      "Havent stuck at orchard in my dad's car. Going 4 dinner now. U leh? So r they free tonight?\n",
      "5065\n",
      "I dunno lei... Like dun haf...\n",
      "5069\n",
      "Talk to g and x about that\n",
      "5070\n",
      "Hai dear friends... This is my new &amp; present number..:) By Rajitha Raj (Ranju)\n",
      "5072\n",
      "As in different styles?\n",
      "5074\n",
      "Gud ni8 dear..slp well..take care..swt dreams..Muah..\n",
      "5077\n",
      "Well, i'm glad you didn't find it totally disagreeable ... Lol\n",
      "5078\n",
      "Guy, no flash me now. If you go call me, call me. How madam. Take care oh.\n",
      "5082\n",
      "Amazing : If you rearrange these letters it gives the same meaning... Dormitory = Dirty room Astronomer = Moon starer The eyes = They see Election results = Lies lets recount Mother-in-law = Woman Hitler Eleven plus two =Twelve plus one Its Amazing... !:-)\n",
      "5084\n",
      "Hey happy birthday...\n",
      "5085\n",
      "Sorry i missed your call. Can you please call back.\n",
      "5086\n",
      "Omg if its not one thing its another. My cat has worms :/ when does this bad day end?\n",
      "5087\n",
      "Good morning, im suffering from fever and dysentry ..will not be able to come to office today.\n",
      "5088\n",
      "I wont do anything de.\n",
      "5091\n",
      "No no. I will check all rooms befor activities\n",
      "5093\n",
      "Gokila is talking with you aha:)\n",
      "5094\n",
      "Hi Shanil,Rakhesh here.thanks,i have exchanged the uncut diamond stuff.leaving back. Excellent service by Dino and Prem.\n",
      "5097\n",
      "Sorry about that this is my mates phone and i didnt write it love Kate\n",
      "5101\n",
      "Nope thats fine. I might have a nap tho! \n",
      "5103\n",
      "In other news after hassling me to get him weed for a week andres has no money. HAUGHAIGHGTUJHYGUJ\n",
      "5104\n",
      "A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8\n",
      "5105\n",
      "I wnt to buy a BMW car urgently..its vry urgent.but hv a shortage of  &lt;#&gt; Lacs.there is no source to arng dis amt. &lt;#&gt; lacs..thats my prob\n",
      "5106\n",
      "Ding me on ya break fassyole! Blacko from londn\n",
      "5107\n",
      "I REALLY NEED 2 KISS U I MISS U MY BABY FROM UR BABY 4EVA\n",
      "5108\n",
      "The sign of maturity is not when we start saying big things.. But actually it is, when we start understanding small things... *HAVE A NICE EVENING* BSLVYL\n",
      "5109\n",
      "Oh you got many responsibilities.\n",
      "5111\n",
      "I've reached sch already...\n",
      "5113\n",
      "U definitely need a module from e humanities dis sem izzit? U wan 2 take other modules 1st?\n",
      "5114\n",
      "Argh why the fuck is nobody in town ;_;\n",
      "5116\n",
      "Thanks. Fills me with complete calm and reassurance! \n",
      "5117\n",
      "Aslamalaikkum....insha allah tohar beeen muht albi mufti mahfuuz...meaning same here....\n",
      "5118\n",
      "Are you driving or training?\n",
      "5119\n",
      "Lol for real. She told my dad I have cancer\n",
      "5121\n",
      "Oops I did have it,  &lt;#&gt; ?\n",
      "5122\n",
      "NOT ENUFCREDEIT TOCALL.SHALL ILEAVE UNI AT 6 +GET A BUS TO YOR HOUSE?\n",
      "5123\n",
      "Hi Chikku, send some nice msgs\n",
      "5126\n",
      "To the wonderful Okors, have a great month. We cherish you guys and wish you well each day. MojiBiola\n",
      "5127\n",
      "Cuz ibored. And don wanna study\n",
      "5129\n",
      "Rose for red,red for blood,blood for heart,heart for u. But u for me.... Send tis to all ur friends.. Including me.. If u like me.. If u get back, 1-u r poor in relation! 2-u need some 1 to support 3-u r frnd 2 many 4-some1 luvs u 5+- some1 is praying god to marry u.:-) try it....\n",
      "5130\n",
      "Any way where are you and what doing.\n",
      "5131\n",
      "That sucks. I'll go over so u can do my hair. You'll do it free right?\n",
      "5132\n",
      "it's still not working. And this time i also tried adding zeros. That was the savings. The checking is  &lt;#&gt; \n",
      "5135\n",
      "Sorry * was at the grocers.\n",
      "5136\n",
      "There are some nice pubs near here or there is Frankie n Bennys near the warner cinema?\n",
      "5139\n",
      "Oh shut it. Omg yesterday I had a dream that I had 2 kids both boys. I was so pissed. Not only about the kids but them being boys. I even told mark in my dream that he was changing diapers cause I'm not getting owed in the face.\n",
      "5140\n",
      "Yeah I imagine he would be really gentle. Unlike the other docs who treat their patients like turkeys.\n",
      "5142\n",
      "Now that you have started dont stop. Just pray for more good ideas and anything i see that can help you guys i.ll forward you a link.\n",
      "5143\n",
      "Hi darlin im on helens fone im gonna b up the princes 2 nite please come up tb love Kate\n",
      "5148\n",
      "K..then come wenever u lik to come and also tel vikky to come by getting free time..:-)\n",
      "5150\n",
      "Happy new year to u and ur family...may this new year bring happiness , stability and tranquility to ur vibrant colourful life:):)\n",
      "5151\n",
      "No problem with the renewal. I.ll do it right away but i dont know his details.\n",
      "5152\n",
      "Idk. I'm sitting here in a stop and shop parking lot right now bawling my eyes out because i feel like i'm a failure in everything. Nobody wants me and now i feel like i'm failing you.\n",
      "5153\n",
      "Haven't left yet so probably gonna be here til dinner\n",
      "5154\n",
      "Like  &lt;#&gt; , same question\n",
      "5155\n",
      "MY NEW YEARS EVE WAS OK. I WENT TO A PARTY WITH MY BOYFRIEND. WHO IS THIS SI THEN HEY\n",
      "5156\n",
      "Sir, I need Velusamy sir's date of birth and company bank facilities details.\n",
      "5158\n",
      "I will come with karnan car. Please wait till 6pm will directly goto doctor.\n",
      "5159\n",
      "No but the bluray player can\n",
      "5161\n",
      "Lol no. I just need to cash in my nitros. Hurry come on before I crash out!\n",
      "5162\n",
      "Just send a text. We'll skype later.\n",
      "5163\n",
      "Ok leave no need to ask\n",
      "5166\n",
      "Y she dun believe leh? I tot i told her it's true already. I thk she muz c us tog then she believe.\n",
      "5167\n",
      "Oh did you charge camera\n",
      "5168\n",
      "I‘ve got some salt, you can rub it in my open wounds if you like!\n",
      "5169\n",
      "Now i'm going for lunch.\n",
      "5171\n",
      "Oh k. . I will come tomorrow\n",
      "5172\n",
      "Aight, text me tonight and we'll see what's up\n",
      "5173\n",
      "U 2.\n",
      "5174\n",
      "Water logging in desert. Geoenvironmental implications.\n",
      "5175\n",
      "Raji..pls do me a favour. Pls convey my Birthday wishes to Nimya. Pls. Today is her birthday.\n",
      "5177\n",
      "Very strange.  and  are watching the 2nd one now but i'm in bed. Sweet dreams, miss u \n",
      "5179\n",
      "Hi hope u r both ok, he said he would text and he hasn't, have u seen him, let me down gently please \n",
      "5180\n",
      "Babe! I fucking love you too !! You know? Fuck it was so good to hear your voice. I so need that. I crave it. I can't get enough. I adore you, Ahmad *kisses*\n",
      "5183\n",
      "Fuuuuck I need to stop sleepin, sup\n",
      "5184\n",
      "I'm in town now so i'll jus take mrt down later.\n",
      "5185\n",
      "I just cooked a rather nice salmon a la you\n",
      "5186\n",
      "I uploaded mine to Facebook\n",
      "5194\n",
      "Ok... Take ur time n enjoy ur dinner...\n",
      "5195\n",
      "Darren was saying dat if u meeting da ge den we dun meet 4 dinner. Cos later u leave xy will feel awkward. Den u meet him 4 lunch lor.\n",
      "5202\n",
      "WOT STUDENT DISCOUNT CAN U GET ON BOOKS?\n",
      "5203\n",
      "Me fine..absolutly fine\n",
      "5206\n",
      "I will reach ur home in  &lt;#&gt;  minutes\n",
      "5207\n",
      "Babe, I'm answering you, can't you see me ? Maybe you'd better reboot YM ... I got the photo ... It's great !\n",
      "5208\n",
      "Hi.what you think about match?\n",
      "5209\n",
      "I know you are thinkin malaria. But relax, children cant handle malaria. She would have been worse and its gastroenteritis. If she takes enough to replace her loss her temp will reduce. And if you give her malaria meds now she will just vomit. Its a self limiting illness she has which means in a few days it will completely stop\n",
      "5211\n",
      "It is only yesterday true true.\n",
      "5212\n",
      "K.k.how is your business now?\n",
      "5213\n",
      "3 pa but not selected.\n",
      "5216\n",
      "I am late. I will be there at\n",
      "5217\n",
      "Well thats nice. Too bad i cant eat it\n",
      "5218\n",
      "I accidentally brought em home in the box\n",
      "5219\n",
      "Pls she needs to dat slowly or she will vomit more.\n",
      "5220\n",
      "I have to take exam with in march 3\n",
      "5221\n",
      "Jane babes not goin 2 wrk, feel ill after lst nite. Foned in already cover 4 me chuck.:-)\n",
      "5224\n",
      "OH FUCK. JUSWOKE UP IN A BED ON A BOATIN THE DOCKS. SLEPT WID 25 YEAR OLD. SPINOUT! GIV U DA GOSSIP L8R. XXX\n",
      "5226\n",
      "Prabha..i'm soryda..realy..frm heart i'm sory\n",
      "5230\n",
      "Nope, I'm still in the market\n",
      "5231\n",
      "I realise you are a busy guy and i'm trying not to be a bother. I have to get some exams outta the way and then try the cars. Do have a gr8 day\n",
      "5233\n",
      "Hey what how about your project. Started aha da.\n",
      "5235\n",
      "Am on the uworld site. Am i buying the qbank only or am i buying it with the self assessment also?\n",
      "5236\n",
      "Your opinion about me? 1. Over 2. Jada 3. Kusruthi 4. Lovable 5. Silent 6. Spl character 7. Not matured 8. Stylish 9. Simple Pls reply..\n",
      "5238\n",
      "Yeah I can still give you a ride\n",
      "5240\n",
      "Gud gud..k, chikku tke care.. sleep well gud nyt\n",
      "5241\n",
      "Its a part of checking IQ\n",
      "5242\n",
      "Hmm thinking lor...\n",
      "5243\n",
      "Of course ! Don't tease me ... You know I simply must see ! *grins* ... Do keep me posted my prey ... *loving smile* *devouring kiss*\n",
      "5244\n",
      "thanks for the temales it was wonderful. Thank. Have a great week.\n",
      "5245\n",
      "Thank you princess! I want to see your nice juicy booty...\n",
      "5246\n",
      "Haven't eaten all day. I'm sitting here staring at this juicy pizza and I can't eat it. These meds are ruining my life.\n",
      "5247\n",
      "Gud ni8 dear..slp well..take care..swt dreams..Muah..\n",
      "5248\n",
      "U come n search tat vid..not finishd..\n",
      "5249\n",
      "K I'm leaving soon, be there a little after 9\n",
      "5251\n",
      "Yeah work is fine, started last week, all the same stuff as before, dull but easy and guys are fun!\n",
      "5252\n",
      "You do your studies alone without anyones help. If you cant no need to study.\n",
      "5253\n",
      "Please tell me not all of my car keys are in your purse\n",
      "5255\n",
      "Ok... Sweet dreams...\n",
      "5257\n",
      "As usual..iam fine, happy &amp; doing well..:)\n",
      "5260\n",
      "If anyone calls for a treadmill say you'll buy it. Make sure its working. I found an ad on Craigslist selling for $ &lt;#&gt; .\n",
      "5263\n",
      "Pls speak with me. I wont ask anything other then you friendship.\n",
      "5264\n",
      "Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@\n",
      "5265\n",
      "Gud ni8.swt drms.take care\n",
      "5268\n",
      "ER, ENJOYIN INDIANS AT THE MO..yeP. SaLL gOoD HehE ;> hows bout u shexy? Pete Xx\n",
      "5270\n",
      "Did u fix the teeth?if not do it asap.ok take care.\n",
      "5273\n",
      "Its too late:)but its k.wish you the same.\n",
      "5274\n",
      "Hi. Hope ur day * good! Back from walk, table booked for half eight. Let me know when ur coming over.\n",
      "5276\n",
      "Dunno leh cant remember mayb lor. So wat time r we meeting tmr?\n",
      "5277\n",
      "Best msg: It's hard to be with a person, when u know that one more step foward will make u fall in love.. &amp; One step back can ruin ur friendship.. good night:-) ...\n",
      "5279\n",
      "Helloooo... Wake up..! \"Sweet\" \"morning\" \"welcomes\" \"You\" \"Enjoy\" \"This Day\" \"with full of joy\".. \"GUD MRNG\".\n",
      "5280\n",
      "Vikky, come around  &lt;TIME&gt; ..\n",
      "5281\n",
      "And how you will do that, princess? :)\n",
      "5284\n",
      "Sent me ur email id soon\n",
      "5286\n",
      "I'm still pretty weak today .. Bad day ?\n",
      "5287\n",
      "Hey ! Don't forget ... You are MINE ... For ME ... My possession ... MY property ... MMM ... *childish smile* ...\n",
      "5289\n",
      "Hey! Congrats 2u2. id luv 2 but ive had 2 go home!\n",
      "5290\n",
      "Dear where you. Call me\n",
      "5291\n",
      "Xy trying smth now. U eat already? We havent...\n",
      "5293\n",
      "I donno its in your genes or something\n",
      "5295\n",
      "Alex says he's not ok with you not being ok with it\n",
      "5297\n",
      "My darling sister. How are you doing. When's school resuming. Is there a minimum wait period before you reapply? Do take care\n",
      "5298\n",
      "I.ll hand her my phone to chat wit u\n",
      "5302\n",
      "About  &lt;#&gt; bucks. The banks fees are fixed. Better to call the bank and find out.\n",
      "5303\n",
      "I can. But it will tell quite long, cos i haven't finish my film yet...\n",
      "5306\n",
      "Ill be at yours in about 3 mins but look out for me\n",
      "5307\n",
      "What you did in  leave.\n",
      "5308\n",
      "I'm coming back on Thursday. Yay. Is it gonna be ok to get the money. Cheers. Oh yeah and how are you. Everything alright. Hows school. Or do you call it work now\n",
      "5309\n",
      "Jolly good! By the way,  will give u tickets for sat eve 7.30. Speak before then x\n",
      "5310\n",
      "yeah, that's what I was thinking\n",
      "5311\n",
      "K.k:)i'm going to tirunelvali this week to see my uncle ..i already spend the amount by taking dress .so only i want money.i will give it on feb 1\n",
      "5315\n",
      "Hahaha..use your brain dear\n",
      "5316\n",
      "Jus finish watching tv... U?\n",
      "5318\n",
      "Good morning, my Love ... I go to sleep now and wish you a great day full of feeling better and opportunity ... You are my last thought babe, I LOVE YOU *kiss*\n",
      "5319\n",
      "Kothi print out marandratha.\n",
      "5320\n",
      "But we havent got da topic yet rite?\n",
      "5322\n",
      "Thanks, I'll keep that in mind\n",
      "5324\n",
      "Dear Sir,Salam Alaikkum.Pride and Pleasure meeting you today at the Tea Shop.We are pleased to send you our contact number at Qatar.Rakhesh an Indian.Pls save our Number.Respectful Regards.\n",
      "5325\n",
      "Gal n boy walking in d park. gal-can i hold ur hand? boy-y? do u think i would run away? gal-no, jst wana c how it feels walking in heaven with an prince..GN:-)\n",
      "5327\n",
      "Wishing you a wonderful week.\n",
      "5328\n",
      "Sweet heart how are you?\n",
      "5329\n",
      "Sir, waiting for your letter.\n",
      "5333\n",
      "Neither [in sterm voice] - i'm studying. All fine with me! Not sure the  thing will be resolved, tho. Anyway. Have a fab hols\n",
      "5336\n",
      "Sounds better than my evening im just doing my costume. Im not sure what time i finish tomorrow but i will txt you at the end.\n",
      "5338\n",
      "So when do you wanna gym?\n",
      "5339\n",
      "You'd like that wouldn't you? Jerk!\n",
      "5341\n",
      "And of course you should make a stink!\n",
      "5343\n",
      "No go. No openings for that room 'til after thanksgiving without an upcharge.\n",
      "5344\n",
      "When you guys planning on coming over?\n",
      "5345\n",
      "Wat ü doing now?\n",
      "5346\n",
      "My Parents, My Kidz, My Friends n My Colleagues. All screaming.. SURPRISE !! and I was waiting on the sofa.. ... ..... ' NAKED...!\n",
      "5347\n",
      "No sir. That's why i had an 8-hr trip on the bus last week. Have another audition next wednesday but i think i might drive this time.\n",
      "5348\n",
      "Do I? I thought I put it back in the box\n",
      "5350\n",
      "No one interested. May be some business plan.\n",
      "5351\n",
      "Yup it's at paragon... I havent decided whether 2 cut yet... Hee...\n",
      "5352\n",
      "Good morning princess! Have a great day!\n",
      "5353\n",
      "Guai... Ü shd haf seen him when he's naughty... Ü so free today? Can go jogging...\n",
      "5355\n",
      "Living is very simple.. Loving is also simple.. Laughing is too simple.. Winning is tooo simple.. But, Being 'SIMPLE' is very difficult...;-) :-)\n",
      "5358\n",
      "Hmm. Shall i bring a bottle of wine to keep us amused? Just joking! I'll still bring a bottle. Red or white? See you tomorrow\n",
      "5359\n",
      "This is ur face test ( 1 2 3 4 5 6 7 8 9  &lt;#&gt;  ) select any number i will tell ur face astrology.... am waiting. quick reply...\n",
      "5360\n",
      "Hey, iouri gave me your number, I'm wylie, ryan's friend\n",
      "5361\n",
      "Yep get with the program. You're slacking.\n",
      "5362\n",
      "I'm in inside office..still filling forms.don know when they leave me.\n",
      "5363\n",
      "I think your mentor is , but not 100 percent sure.\n",
      "5367\n",
      "Just trying to figure out when I'm suppose to see a couple different people this week. We said we'd get together but I didn't set dates\n",
      "5369\n",
      "Hi mom we might be back later than  &lt;#&gt; \n",
      "5372\n",
      "Ok., is any problem to u frm him? Wats matter?\n",
      "5373\n",
      "K I'll head out in a few mins, see you there\n",
      "5374\n",
      "Do u konw waht is rael FRIENDSHIP Im gving yuo an exmpel: Jsut ese tihs msg.. Evrey splleing of tihs msg is wrnog.. Bt sitll yuo can raed it wihtuot ayn mitsake.. GOODNIGHT &amp; HAVE A NICE SLEEP..SWEET DREAMS..\n",
      "5375\n",
      "I cant pick the phone right now. Pls send a message\n",
      "5376\n",
      "I don't want you to leave. But i'm barely doing what i can to stay sane. fighting with you constantly isn't helping.\n",
      "5379\n",
      "Somebody set up a website where you can play hold em using eve online spacebucks\n",
      "5380\n",
      "Its sunny in california. The weather's just cool\n",
      "5383\n",
      "Good day to You too.Pray for me.Remove the teeth as its painful maintaining other stuff.\n",
      "5386\n",
      "I'm at work. Please call\n",
      "5390\n",
      "Nt joking seriously i told\n",
      "5392\n",
      "Ooooooh I forgot to tell u I can get on yoville on my phone\n",
      "5393\n",
      "All done, all handed in. Don't know if mega shop in asda counts as celebration but thats what i'm doing!\n",
      "5398\n",
      "Hi. Hope you had a good day. Have a better night.\n",
      "5399\n",
      "And he's apparently bffs with carly quick now\n",
      "5405\n",
      "So how many days since then?\n",
      "5406\n",
      "Dear are you angry i was busy dear\n",
      "5408\n",
      "... Are you in the pub?\n",
      "5411\n",
      "I ask if u meeting da ge tmr nite...\n",
      "5412\n",
      "Gr8. So how do you handle the victoria island traffic. Plus when's the album due\n",
      "5413\n",
      "Nite nite pocay wocay luv u more than n e thing 4eva I promise ring u 2morrowxxxx\n",
      "5414\n",
      "East coast\n",
      "5415\n",
      "You should get more chicken broth if you want ramen unless there's some I don't know about\n",
      "5416\n",
      "My slave! I want you to take 2 or 3 pictures of yourself today in bright light on your cell phone! Bright light!\n",
      "5418\n",
      "So how are you really. What are you up to. How's the masters. And so on.\n",
      "5419\n",
      "I'm at bruce &amp; fowler now but I'm in my mom's car so I can't park (long story)\n",
      "5421\n",
      "Hi elaine, is today's meeting confirmed?\n",
      "5422\n",
      "Ok k..sry i knw 2 siva..tats y i askd..\n",
      "5423\n",
      "Sorry, I'll call later\n",
      "5424\n",
      "U horrible gal... U knew dat i was going out wif him yest n u still come n ask me...\n",
      "5425\n",
      "Otherwise had part time job na-tuition..\n",
      "5428\n",
      "You didnt complete your gist oh.\n",
      "5431\n",
      "If I was I wasn't paying attention\n",
      "5432\n",
      "Thanx a lot 4 ur help!\n",
      "5433\n",
      "You're gonna have to be way more specific than that\n",
      "5435\n",
      "I'm wif him now buying tix lar...\n",
      "5437\n",
      "Am slow in using biola's fne\n",
      "5438\n",
      "What are youdoing later? Sar xxx\n",
      "5439\n",
      "Hey i've booked the 2 lessons on sun liao...\n",
      "5441\n",
      "By the way, make sure u get train to worc foregate street not shrub hill. Have fun night x\n",
      "5444\n",
      "Good morning. At the repair shop--the ONLY reason i'm up at this hour.\n",
      "5445\n",
      "And that's fine, I got enough bud to last most of the night at least\n",
      "5446\n",
      "I am back. Good journey! Let me know if you need any of the receipts. Shall i tell you like the pendent?\n",
      "5447\n",
      "So that takes away some money worries\n",
      "5451\n",
      "Just sing HU. I think its also important to find someone female that know the place well preferably a citizen that is also smart to help you navigate through. Even things like choosing a phone plan require guidance. When in doubt ask especially girls.\n",
      "5452\n",
      "What???? Hello wats talks email address?\n",
      "5453\n",
      "Except theres a chick with huge boobs.\n",
      "5454\n",
      "Im just wondering what your doing right now?\n",
      "5455\n",
      "Wishing you a beautiful day. Each moment revealing even more things to keep you smiling. Do enjoy it.\n",
      "5458\n",
      "Sorry, I'll call later\n",
      "5463\n",
      "U GOIN OUT 2NITE?\n",
      "5465\n",
      "Shall I bring us a bottle of wine to keep us amused? Only joking! I‘ll bring one anyway\n",
      "5470\n",
      "I thought slide is enough.\n",
      "5473\n",
      "Ok lor ü reaching then message me.\n",
      "5474\n",
      "Where's mummy's boy ? Is he being good or bad ? Is he being positive or negative ? Why is mummy being made to wait? Hmmmm?\n",
      "5475\n",
      "Dhoni have luck to win some big title.so we will win:)\n",
      "5476\n",
      "Yes princess! I want to please you every night. Your wish is my command...\n",
      "5477\n",
      "What Today-sunday..sunday is holiday..so no work..\n",
      "5478\n",
      "No probably  &lt;#&gt; %.\n",
      "5480\n",
      "Have you seen who's back at Holby?!\n",
      "5484\n",
      ", ,  and  picking them up from various points | going 2 yeovil | and they will do the motor project 4 3 hours | and then u take them home. || 12 2 5.30 max. || Very easy\n",
      "5486\n",
      "Ofcourse I also upload some songs\n",
      "5489\n",
      "Oh thanks a lot..i already bought 2 eggs ..\n",
      "5490\n",
      "K. I will sent it again\n",
      "5491\n",
      "U studying in sch or going home? Anyway i'll b going 2 sch later.\n",
      "5493\n",
      "I think if he rule tamilnadu..then its very tough for our people.\n",
      "5494\n",
      "Cool, we shall go and see, have to go to tip anyway. Are you at home, got something to drop in later? So lets go to town tonight! Maybe mum can take us in.\n",
      "5495\n",
      "Good afternoon, my love ... How goes your day ? How did you sleep ? I hope your well, my boytoy ... I think of you ...\n",
      "5496\n",
      "Yes... I trust u to buy new stuff ASAP so I can try it out\n",
      "5499\n",
      "Now get step 2 outta the way. Congrats again.\n",
      "5503\n",
      "Perhaps * is much easy give your account identification, so i will tomorrow at UNI\n",
      "5505\n",
      "What i told before i tell. Stupid hear after i wont tell anything to you. You dad called to my brother and spoken. Not with me.\n",
      "5507\n",
      "I want to be inside you every night...\n",
      "5508\n",
      "Machan you go to gym tomorrow,  i wil come late goodnight.\n",
      "5509\n",
      "Lol they were mad at first but then they woke up and gave in.\n",
      "5510\n",
      "I went to project centre\n",
      "5512\n",
      "Just making dinner, you ?\n",
      "5515\n",
      "You are a great role model. You are giving so much and i really wish each day for a miracle but God as a reason for everything and i must say i wish i knew why but i dont. I've looked up to you since i was young and i still do. Have a great day.\n",
      "5516\n",
      "Ya, i'm referin to mei's ex wat... No ah, waitin 4 u to treat, somebody shld b rich liao...So gd, den u dun have to work frm tmr onwards...\n",
      "5518\n",
      "By the way, i've put a skip right outside the front of the house so you can see which house it is. Just pull up before it.\n",
      "5521\n",
      "You are a big chic. Common. Declare\n",
      "5522\n",
      "Thats cool. I want to please you...\n",
      "5523\n",
      "Going to join tomorrow.\n",
      "5528\n",
      "Its just the effect of irritation. Just ignore it\n",
      "5529\n",
      "What about this one then.\n",
      "5531\n",
      "Compliments to you. Was away from the system. How your side.\n",
      "5532\n",
      "happened here while you were adventuring\n",
      "5534\n",
      "Ok which your another number\n",
      "5535\n",
      "I know you are thinkin malaria. But relax, children cant handle malaria. She would have been worse and its gastroenteritis. If she takes enough to replace her loss her temp will reduce. And if you give her malaria meds now she will just vomit. Its a self limiting illness she has which means in a few days it will completely stop\n",
      "5538\n",
      "I can't believe how attached I am to seeing you every day. I know you will do the best you can to get to me babe. I will go to teach my class at your midnight\n",
      "5541\n",
      "Yeah it's jus rite...\n",
      "5542\n",
      "Armand says get your ass over to epsilon\n",
      "5543\n",
      "U still havent got urself a jacket ah?\n",
      "5544\n",
      "I'm taking derek &amp; taylor to walmart, if I'm not back by the time you're done just leave the mouse on my desk and I'll text you when priscilla's ready\n",
      "5545\n",
      "Hi its in durban are you still on this number\n",
      "5546\n",
      "Ic. There are a lotta childporn cars then.\n",
      "5548\n",
      "No, I was trying it all weekend ;V\n",
      "5550\n",
      "Cool, what time you think you can get here?\n",
      "5551\n",
      "Wen did you get so spiritual and deep. That's great\n",
      "5552\n",
      "Have a safe trip to Nigeria. Wish you happiness and very soon company to share moments with\n",
      "5553\n",
      "Hahaha..use your brain dear\n",
      "5555\n",
      "Yeh. Indians was nice. Tho it did kane me off a bit he he. We shud go out 4 a drink sometime soon. Mite hav 2 go 2 da works 4 a laugh soon. Love Pete x x\n",
      "5557\n",
      "No. I meant the calculation is the same. That  &lt;#&gt; units at  &lt;#&gt; . This school is really expensive. Have you started practicing your accent. Because its important. And have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\n",
      "5560\n",
      "Anything lor. Juz both of us lor.\n",
      "5561\n",
      "Get me out of this dump heap. My mom decided to come to lowes. BORING.\n",
      "5562\n",
      "Ok lor... Sony ericsson salesman... I ask shuhui then she say quite gd 2 use so i considering...\n",
      "5564\n",
      "Why don't you wait 'til at least wednesday to see if you get your .\n",
      "5565\n",
      "Huh y lei...\n",
      "5570\n",
      "The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\n",
      "5571\n",
      "Rofl. Its true to its name\n"
     ]
    }
   ],
   "source": [
    "for v in (train_df.loc[train_df['class'] =='ham']).iterrows():\n",
    "    print(v[0])\n",
    "#     print(v[1]['category_name'])\n",
    "    print(v[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28294\n",
      "8846 8879 8861\n"
     ]
    }
   ],
   "source": [
    "def read_words(words_file):\n",
    "    return [line for line in open(words_file, 'r') ]\n",
    "\n",
    "words = read_words(\"blacklist.txt\")\n",
    "\n",
    "spam = [word.strip('-_\\n/') for word in words]\n",
    "spam = [word.replace('-',' ') for word in spam]\n",
    "print(len(spam))\n",
    "\n",
    "\n",
    "# l1 = []\n",
    "# l2 = []\n",
    "# l3 = []\n",
    "# for i,w in enumerate(spam):\n",
    "#     if(i%3==0):\n",
    "#         l1.append(w)\n",
    "#     elif(i%3==1):\n",
    "#         l2.append(w)\n",
    "#     else:\n",
    "#         l3.append(w)\n",
    "        \n",
    "l1 = set()\n",
    "l2 = set()\n",
    "l3 = set()\n",
    "for i,w in enumerate(spam):\n",
    "    if(i%3==0):\n",
    "        l1.add(w)\n",
    "    elif(i%3==1):\n",
    "        l2.add(w)\n",
    "    else:\n",
    "        l3.add(w)\n",
    "print(len(l1),len(l2),len(l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'mbt fora', 'packersjers', 'favourable single', 'japanese mont', 'abercrombie deutsch', '6.su', 'jordan sale', '.credopa.in', 'celine prezzi', 'herbaltincture', 'loan.co', 'wprobot', 'website..', 'laminin lpgn', 'teen cam', 'c http', 'replicachina', 'iphone/iphone', 'yourdick', 'sexcam', 'oxycontin', 'foolproof trick', '3.in', 'tattootip', 'chinascarf', 'l.uk.e.w.a.rm', 'coast petite', 'favourite justif', 'salemiami', 'sprawdzonenarzedzia', 'сети работ', 'chinesecheap', 'lululemon loca', 'porn.model', 'fifa', 'actual submit', 'jordan femme', 'belstaffcuero', 'occhiali', 'bioactivity', '.filmi', 'mac cosmetic', 'get professional', 'image/cache', 'my online', 'hollister tiendas', 'lesbos.', 'bcbg casual', 'bulgari brand', '00mg buy', 'celine paris', 'mainslancel', 'unknown.ro', 'baguebulgari', 'officialmailsite', 'cool t shirts', 'belstaff hand', 'replicaleger', 'toile vanessa', 'ženys run', 'lady porn', 'sac reutilis', 'vinter jakker', 'gmailprox', 'allimage', 'bluecap turbo', 'material stylish', 'jacket 2017', 'theguest', 'gold claim.', 'forsale.shop', 'casas en', 'shiatsu.', 'myonline', 'derniers modele', 'knowing answer', 'domination secret', 'cartieruomo', 'bots cheat', 'actually thank', 'erotic asphyx', 'chanel.', 'got.by', 'personal pc', 'prokuror či', 'curry jersey', 'serrurier paris', 'topforum', 'coupon.', 'f.j.oga', 'subject', '+spade', 'fund trend', 'heart site', 'maillot man', 'outstanding share', 'relogios replica', 'top blog', '.tatuagens', 'event.', 'usual discuss', 'zapato', 'kredyt', '@xyz.net', 'article snatch', 'valka derevev', 'r.two', 'money.cfm', 'awesoome', 'autres equip', 'b0.php', 'discounted north', 'burberry scarf', 'tarikishoe', 'clashhack', 'vetement homme', '.bots3', 'canada pharm', 'computerperform', 'cotcoinox', 'c.he.ap', 'tactical marketing', 'user', 'buysilver', 'pokemon tcg', 'ciallis', 'sunglassescheap', 'snapback.', 'carreralune', 'kors tote', 'b07.htm', 'watch seiko', 'monclerhomme', 'banners.', 'quantocost', 'discountafl', 'instant cash', 'killer post', 'jordan3', 'websiite', 'redirects.jsp', 'aktuellnews', 'easily', 'blanc pen', 'celine boston', 'ãœ', 'dieta sofrida', 'offer watch', 'tobuy.in', 'very lower', 'ジェレミースコット', 'slugy otdeloch', 'ρi', '.asses', 'money fast', 'nicepost', 'raybanout', 'snoremouth', 'go viral,', 'yourprofile', 'legersale', 'ownersinsur', 'pussy', 'hcgboost', 'stone jp', 'oо', 'mobile', 'christianloubou', 'ratno.info', 'penned write', 'photoeditingdeal', 'flagyl', 'real mastery', 'lesbian fuck', 'sex gratis', 'bikini sale', 'xtrasize', 'bvlgariout', 'wholesale soccer', 'rbstr.ru', '[url', 'successful post', 'daddy site', 'twwo minute', 'nike', 'replica china', 'gia công', 'teen sex', '?forex', 'actually dont', 'x  x.', 'magnificent inform', 'articles study', 'nufactur', 'мошенник', 'шаблоны wordpress', 'birkenstock sandal', 'duvetica aristeo', 'greatest available', 'clans ipad', 'bollywood hair', 'wholesale free', 'muscles exercis', 'торговля бинар', 'pleasant understand', 'ファッショ', 'submit web', 'еd', 'jbs.', 'pornvid', '.assjob', 'footwears', 'iphone case', 'main longchamp', 'baratos person', 'pagesscrape', 'offertecalvin', 'brandjp', 'taxdebt', 'куп свидетел', 'brands jp', 'item.htm', 'prada clutch', 'ꭼt', 'boot ugg', 'nngid.ru', '.cazino.', 'reborn article', 'novinki', 'newest news', 'hotel egypt', 'termopane', 'belstaff bota', 'shop pin', 'fifa coin', 'tnf sale', 'ликвида', 'jersey nike', 'max user', 'app nana cydia', 'networksscam', 'pro99', 'footweardisc', 'fuck my', 'click', 'akrilpanel', 'monclervente', 'library.jsp', 'y teta', 'outlet.org', 'runway sita', 'academykrs', 'blu cig', 'd.the', 'diesel jap', 'jhjhjh', 'thumb.htm', 'mbt ayakkab', 'how to.', 'sex toy', 'zapatos dc', 'obey posse', 'lauren femme', 'diet', 'gilet moncler', 'копируйте ссылку', 'bianca black', 'peutereygiub', 'obuv nach', 'canada loan', 'pokergame', 'cert line', 'newbalance.', 'growth hormone', '4 cash', 'sale now!', 'afford afford', 'shoesonline', 'tanger out', 'laranitafree', 'poke coin', 'all ahout', 'qsymia india', 'tai online', 'plasticcaserole', 'hydraulik', '.unblog.', 'abalroar dentro', 'b08.jsp', 'thanks designed', 'butwho', 'oulet online', 'smith sold', 'herrenmoncler', 'shoe+', 'wow gold', 'salefactor', 'tatuagem deu', 'polos ralph', 'yuwanshe', 'borsa moncler', 'timberlandwomen', 'takje care', 'jerseysnike', 'desyrel', 'www.barato', 'elite jers', 'felpe hollis', 'penny auction', 'mbt baridi', 'asthma.', 'gointeractive', 'sօ', 'ageless male', 'shoppingcenter.co', 'graypanther', 'pedophillie notoire', 'kors milan', 'devil1.', 'longchamp martin', 'neurontin', 'tasche longchamp', 'baratosperson', 'fake ugg', 'fradidas', 'promo store', '+loubou', 'professional2012', 'targetedtraff', '123', 'взламывать паро', 'mcm bag', 'månner puma', 'sluts', 'начинающих алматы', 'mylupus', '+women', 'สูตรบาคาร่า', '.soft ware', 'statement state', 'inrtomaf', '2012baby', '+http', 'photo/bv', 'makersnow', 'nice page', 'fish casino', 'longchamppurse', '5.in', 'uggclass', 'braves jers', 'my webpage', 'kids baby', 'hollisterit', 'online reader', 'porngay', 'forum.really', 'clubboy', 'zippo zubehor', 'cheap timber', 'firefox setting', 'onlinejp', 'nurkowemarsa', 'also eable', 'fastidious web', 'surfing online', 'buyduvetica', 'mbt vanzari', 'polo lacoste', 'rhttp', 'social bookmark', 'lovely portal', 'shoe jap', 'guerregratuit', 'chloeoutsale', 'sunglasscheap', 'jacobsinmilan', 'largetote', 'seo soluton', 'styleshq', 'longchamp tourne', 'student login', 'camicienegozi', 'casino24', 'dont cease', 'marihuana', 'pharm', 'damierazur', 'оc', 'shoe.mobi', 'good weblog', 'giubbotto prezzi', 'realestate.web', 'widget', 'meandyou', 'website', 'online out', 'rtn reg', 'collezionemoncler', 'maillotman', 'комплект документ', 'huntingtx', 'income masterclass', 'seogain', 'senukevps', '.twilight', 'imitation hermes', 'фитнес клуб', 'cgsaleca', '全球华人', 'спам', 'calorias leva', 'cheap cheap', 'от ожирения', 'jeremy scott', 'manor escort', 'sacslongchamp', '0taylor', 'excellent article', '手机', 'slugi elektrek', 'poradnik face', 'couponsense', 'freecassino', 'campaign', 'dsquaredout', 'nike free', 'htm', 'kors crossbody', 'lijst', 'b06.php', 'great good', 'mbtspecial', '１０歳', 'insanity.php', 'cure herpes', 'automaticpay', 'dieta', '4 gold', 'uggsonline', 'age|', 'freewebsite', 'sexy sex', 'salomon.', 'cybermonday', 'member.asp', 'uggs ugly', 'promos code', 'information.htm', 'a06.jsp', 'banprezzo', 'revive beauty', 'havegive', 'plus.co', 'without prescript', 'pandora online', 'medicinez', 'teamonline', 'filmsporn', 'perhaps earring', 'vuitton wallet', 'twoo minute', 'bag crash', 'licu=urn', 'neat website', '.ru/vid', 'anti psori', 'stylo montblanc', 'buy duvetica', 'vuitton deutsch', 'fantasticpage', 'pokecheat', 'метод взлома', 'birkenstock', 'fulgor fantástic', 'power proficien', 'ka.t.h.lee.n', 'mizunoshoe', '最安値', 'teenaged female', 'r.za', '+seo', '2015 popular', 'nba houston', 'autentica.', 'приватные прокси', 'official website', 'handbag distrib', 'kisslove', 'top 8', 'byo.co', 'pheaemon', 'proteindiet', 'hot tech', 'losreplica', 'videos youtube', 'bitartrate', 'chaussure loubou', 'chanel sale', 'casino', 'btn phone', '=', 'bulgarisshop', 'promo artist', 'p.y.th.on', 'cheap boot', 'pokertexas', 'jerseys whole', 'catalogues.cfm', 'media wiki', 'botanical slim', 'sex dat', 'knockoffhand', '?birkin', 'pumaout', 'availableclock', 'fix credit', 'obuv zeny', 'vidéos youtube', 'komputer', 'rightblog', 'latestmk.', 'borsalouis', 'страшный диагноз', 'includes priceless', 'sumatriptan', 'jam jord', 'jersey.', 'frame cheap', 'migliore replica', 'cigarettesbuy', 'the easiest', 'monclerparka', 'clearance on', 'online cheat', 'weekly income', 'panty pic', 'gabbanashop', 'uggs hot', 'shirtprint', 'no remov', 'españa online', 'nudevid', 'mbt out', 'com7.htm', 'koopsted', ',\"', 'fantasticsite', 'fj.o.g.a', 'celine trapeze', 'ck', 'excellent topic', 'chooboot', 'grateest', '.cause', 'mbt carpe', 'nailjp', 'handbagsfor', 'pages.', 'money adder', 'phexin.', '100mg', 'uggsclass', 'panty girl', 'form.', 'ɑc', 'losers beat', 'wholesalesoccer', 'q.', 'աa', 'femmecouche', 'hermes enamel', 'newports', 'аl', 'best online', 'bulgaribrand', 'service now!', 'update movie', 'визиток сайт', 'monindex', 'networks buzz', 'dg.', 'nordstrom moncler', 'fence produce', 'overall glance', 'mrantsneak', 'approaching article', 'recent seo', 'saleout', 'dapoxetin', 'virus hoax', 'intersting', 'company coach', 'chanelsac', 'truly thk', 'bottes en ligne', 'oь', 'blog ', 'gmail prox', 'menu.', 'adidas futbol', 'domination.', 'testrun', 'dlphone', 'misc', 'compare price', 'red ugg', 'empower', 'reports.php', 'hoodia', 'valentino shopper', 'homme rolex', 'optionbroker', 'outlet.weebly', 'www.fotograf', '.su/store', 'louboutin platform', 'mayari birkenstock', 'uggbos', '%5b%5d', 'dumps shop', 'tones way', 'hermes austr', 'hermes', 'lottery singapore', 'remarkablepost', '.info/,', 'boobs', 'moncler pour', 'mostra vid', 'videncia vidente', 'vape cloud', 'кредит', 'vestemoncler', 'gotowkowa', 'big pussy', 'invest off', 'phpbb2', '.games.pl', 'vuitton belt', 'jerking my', 'sito ugg', 'essay today', 'website1', 'tatuagens feminina', 'bluehostreview', 'spilivanie derevev', 'server 2013', 'freedownload', 'marker gaul', 'a7.htm', 'thesis penning', 'virus secur', 'whats up,', 'wayfarer barato', 'good goodie', 'kino prog', 'beamten', 'i.g.or', 'a9.cfm', 'amazon.asp', 'net viewer', 'uggs brux', 'business directory.', 'video pokie', 'oakley radar', 'zapatillas dc', '.ru/pub', 'appleunlock', 'litte more', 'best cyber', 'cheap pandora', 'fightmark', 'boners.', 'o.for', 'piuminiwoolrich', 'marijuanafact', 'cell phone', 'rating24', 'freebid', 'aquarius compat', 'rebecca jap', 'replicajack', 'seksual', '5.@', '配送', '{url}', 'page scrape', 'bestcartier', 'blocker preco', 'monroussillon', 'xxx mobile', 'ff15 shop', 'подарки', '5.htm', 'chinese dress', 'boot uk', 'handbagu', 'young porn', 'parafon forte', 'cheapbasket', 'i’v got', 'goose.cfm', 'legitimatepharm', 'tier business', 'курсов кассиров', 'replicaherve', '眉r', 'mbtpanda', 'bottesugg', 'cooker ninja', '4.su', 'executive coach', 'neatpage', 'idanmark', 'click sure', 'oneminutesite', 'truly fastidious', 'mcm hand', 'viral.club', 'borse luis', 'rekla.', 'share ssite', 'main.php', 'websolution', 'best website', 'amazing post', '911.ro', 'option broker', 'xl paket', 'medikal.co', 'baclofen', 'como utilizar', 'japannew', 'phone number lookup', 'cosmetickit', 'cigar online', 'remarkable paragraph', 'burberry wallet', 'juniors kid', 'i http', 'your submit', 'c.r.ease', 'timberland herr', 'quality blog', 'codes xbox', 'optimize enhance', 'andjourn', 'coffee erectile', 'dili optim', 'piumini moncler', 'verspiegelte ray', 'internet web', 'online sex', 'foglio prada', 'forsalejap', 'shoe sale', 'about', 'sprawdzone narzędzia', 'sexelist', 'schuhysl', 'fr/index', 'videochat', 'wonderful blog', 'highblood', 'tax settlement', 'ejaculatehelp', 'fake coach', 'boost your', 'b00.htm', 'acnetreatment', 'play online', 'te.mpte.st', 'server 2016', 'hid', '20mg buy', 'взламывать одно', 'bag.jsp', 'meilleur casino', 'coinvest', 'stimulacion sex', 'perfect writ', 'vintage erotic', 'sex blog', 'www.co', 'redsoleshoe', 'barcelona sunglass', 'exclusive.pl', 'escort', 'generator2014', 'barato', '正宗肥仔', 'nova coleção', 'a3.cfm', 'datebase', 'porn virgin', 'plombier paris', '+emerg', 'keyword1', 'louboutin sale', 'gucci2', 'k%c3%a8o b%c3%b3ng', 'anal', 'armanicost', 'freshreview', 'pioggia gucci', 'womanbikini', 'betting article', 'blog.', '=space', 'anal plug', 'online gry', 'продолжительном сроке', 'fragil emocional', 'revia med', 'laarzen schoen', 'bulk mail', 'retro jord', 'ut.ag', 'replica face', 'leger copies', 'activity.', 'miracle disc', 'kleinmujer', 'film porn', 'coach ya', 'verres ray', '100 most', 'porn prev', 'dsquared uomo', 'shoe woman', 'wikka.asp', 'thats need', 'drmarten jp', 'gucci brief', 'link directory', 'best priced', 'chloeboot', 'online 5', 'human site', 'warehouse.us', 'led_indust', 'collezione celine', 'girl jp', 'token generat', 'eroticnude', '+ppv', 'sale jap', 'cheap nike', 'shirts cheap', 'guardians hack', 'girl spouse', 'shit zombie', 'gget set', 'store', 'cheap bike', 'cassino 24', 'skque.', 'exclusive rendez', 'more eventually', 'albanian travel', 'oksunglass', '365.co', '.za/za', 'pornpic', '4 juice', 'coach jap', 'кабриолет', 'mcm purse', 'coastalva', 'hyper fb', 'metods', 'lancel pas', 'r.ro', 'coach shoe', '.kartridzhej.', 'seo host', 'mbt tembea', 'gry', 'pantysmania', 'australian pokie', 'vivierpompe', 'obuvšedá', 'pђ', '.bots0', '&#30330;&#22770;', 'nude wallpaper', 'quality weblog', 'retina247', 'oakleyactive', 'doable prog', 'и origin', 'images/table', 'dylongfa', 'os depoimento', 'patience maxi', 'chaturbate', 'ping', 'mortgagecalc', 'ура наконецто', 'сопровождение девушка', 'fixed football', 'proficiency student', 'colornude', 'pleease', 'fb traff', 'looked on line', 'shedfat', 'coachshoe', 'двери гарди', 'pips daily', 'tojp.co', 'mastery class', 'bisnis', 'less, appnana', 'runcpa', '.com3', 'k.at.hlee.n', 'monclerout', 'outsourcing compan', 'nike andy', 'chaussure femme', 'mcm', 'familiarity daily', 'aϲ', 'alviero', 'ケース', 'clearance mbt', 'appnana code', 'totehand', 'chinawhole', 'bootsget', 'acel google', 'privatproxy', 'cytotec', 'tiffanyout', 'arpels replica', 'barbour out', 'outlet2', 'men hand', '80折', 'ustanovka kanali', 'fitchout', 'uomo dsqu', 'daftar harga', 'сироп мангустина', 'filvce.ru', 'bulgari bague', 'trx', 'b1.jsp', 'игровые', 'jersey1', 'marc jacobs', 'erinvestor', 'c.r.e.ase', 'a9.php', 'images/index', 'natureto', 'bcbg dress', 'сократите риск', 'derm exclus', 'informasjon.cfm', 'binarnykh optsion', 'experience simply', 'guccireplica', 'replica femme', 'ugg animal', 'a8.jsp', 'wp content', 'говорю гово', 'philippine primar', 'profile?', 'this website;', 'moncler2015', 'b04.htm', 'forte muscle', 'cre.a.se', 'forex', 'sexy web', 'pokemongocoin', 'pokemon cheat', 'brustvergrößerung', 'ugg.asp', 'coachjap', 'nouveau maillot', 'zmir escort', 'buspirone', 'clearance+', 'famous blog', 'rich woolrich', 'celtics colour', 'go watchs', 'urlms.co', 'cheap supra', 'chaussure', 'china low', 'advert market', 'коммерческих трудност', 'zune pass', 'sua mulher', 'articles plus', 'replicarolex', 'produkcja', 'stride jack', 'stiefeldamen', 'shanghaiescort', 'fastidiousdial', 'symptoms med', 'chinese shop', 'successonline', 'oemsoftware', 'funwithwindow', 'fake oakley', 'mastery.', '.picloader.', 'fr canad', 'burberry watch', 'replicabrand', 'abssice 360', 'cheaploubou', 'moncler jassen', 'im grateful', 'diet.cfm', 'cash call', 'headset2016', 'program ppc', 'a4.htm', 'professional 2006', 'good free', 'help tax', 'finance blog', 'servicedoc', 'proficiency showed', 'chaussures loubou', 'remote desktop', 'ramipril', 'р°', 'signzodiac', 'iwc brand', 'chaussure paris', 'scarpe nike', 'mlm site', 'pillsonline', '4.in', 'tubezzz', 'prescription.htm', 'selling iphone4', 'bargain essay', 'timberland1', 'addidas', 'qsymia.', 'cigarette online', 'colournude', 'distinct website', '2013 seo', 'iphone you', 'f.joga', 'аренда видос', '1stop', 'fastidious article', 'not fake', 'low rake', 'bagcheap', '.su/load', 'free.in', 'currency trad', 'shortcode', 'louboutin sneaker', 'mbtboot', 'vaporstick', 'buzz vidéo', 'goad.k', 'sexshop', 'sites24', 'hoverboard buy', 'title', 'incredible article', 'nfr.asp', 'bonuscasino', '30mg cheap', 'zsurgical', 'nude.php', 'shoe.cfm', '.ru/catalog', '.outlet.co', 'smsh.me', 'chaussures homme', 'cheat master', 'bijsluiter tablet', 'iphone apple', 'successful article', 'post 2.', 'markejean', 'ugg france', '.gull.gull', 'site24', 'jacobs jp', 'goose running', 'twerking vid', 'timberland saldi', '2u.org', 'concerning blog', 'с°', 'internet lifestyle', 'berlinmoncler', 'eng/index', 'eroticworkout', 'photos/rolex', 'no continu', '.cool t shirts.', 'funding.in', 'man gaga', 'tits love', 'jimla.r.d', 'молочной', 'minoxidil', 'tittypic', 'energetic blog', 'vecaro', 'appnanasync', 'loanalys', 'nossos órgãos', 'lancel', 'clou replica', 'canadagoosejack', 'live odds', 'solutioninc', 'darmoweogłoszenia', 'bird hack', 'thumbs.jsp', 'best wordpress', 'tiredisc', 'furboot', 'couple watches', 'willhemp', 'refire cert', 'npn.', 'rarely directly', 'carsgame', 'uhttp', 'eyewear present', 'every info', 'ruwordpress', 'sex.htm', 'cigar.', 'server 2006', 'coach promo', 'cheap carolina', 'ebony porn', 'sac celine', 'occhiali catalog', 'bestpricebuy', 'out assim', 'heuersale', '02.jsp', 'ホリスター', 'wholesale jers', 'vapor stick', 'forsale go', 'men’s timber', '1000000$', 'men mbt', 'hollisterlogo', 'indexold', 'cassino.co', 'alphahoverboard', 'mudar dieta', 'where+to', 'serv', 'fingering my', 'rose shoe', 'celine shop', 'inheritancecash', '.supreme', '00mail', 'melhor preço', 'hgh.', 'nguoidanviet2', 'articles buzz', 'cashadvance', 'oakleyfrog', 'timberlandboot', 'noticeably plenty', 'adenosyl', 'scarpehogan', 'customdesignedshirt', 'warehouse.ro', ',on', 'moderowany', 'market prices', 'your porn', 'ecco out', 'gifts click', 'cleaning academy', 'goosesite', 'feature.php', 'wholesale clock', 'woolrichuomo', 'anal sex', 'ppvcpa', '.detophyll', 'going please', 'directorysubmit', 'a01.php', 'esthetic master', 'jerseys from', 'wille bijoux', 'baikalextreme', 'essays serv', 'moncler espa', 'porno', '婚庆礼仪', 'sexgratis', 'homes smart', 'market.in', 'immediateincome', 'prada bag', 'firm.in', 'aktuellenews', 'link bait', 'account receiv', 'herveleger', 'pliage cuir', 'leger band', '&#28608;&#23433;', 'moncler tibet', 'bitcoindonat', 'singapore.cfm', 'nbashop', 'domino poker', 'montblanc uk', ';&#x', 'magicmoncler', 'videos view', 'german lesb', 'cheap christ', 'warriors jersey', '.su/2', 'orderflower', 'jackets out', 'computer pc', '無料', 'barato online', 'chaquetascuero', 'leather levi', 'facebook lawful', 'white hermes', 'buy now', 'howtomake', 'runescape gold', 'hemp oil', 'top.asses', 'pradaclutch', 'being pput', '.su/forum', 'xxxgay', 'polo shoe', 'vagina live', 'excellentblog', 'mcqueen leopard', 'fb like', 'nude girl', 'abc', 'ass small', 'toms.', 'viral eas', 'writing helper', 'brandengage', 'dating date', 'supremacy.', 'addition reason', 'livesex', 'jordan out', 'online dat', 'website we', 'gucciinstock', 'uomogucci', 'index 3', '+kino', 'outletleouf', 'site', 'whenshe', 'pumadeutsch', '.ru/artik', 'blog (wordpress', 'download joomla', 'dollar serv', 'herpes treatment', 'mieszkania', 'szambo', 'slvrenew', 'pjure breast', 'charmsthomas', 'long long', 'девушка сопровождение', 'article sub', 'moncler vest', 'jerseys', 'revolutionjog', 'thninkig', 'vuittonbag', '。ｃoｍ', 'attachments', 'gucciparis', 'angels porn', '.in/in', 'օr', 'longchamp hobo', 'cheap', 'forte', 'dont fit me', 'stonejap', 'tinnitus tin', 'goldbarren', 'millen au', 'costume.', 'tion?i', 'armagard.co', 'ugg pas', 'onlyway', 'master.su', 'lacoste out', 'ppv traff', 'good template', 'réplica relogio', 'riga stag', 'acidreflux', 'leger sale', 'uggs out', 'jersey for', 'sneak', 'woolrich giub', 'coach legacy', 'betes2', 'dragon avail', 'filmi', 'barata ropa', 'shop bought', 'instantcash', 'religion jeans', 'suggested website', 'zero calorie', 'canadian hgh', 'nice paragraph', 'm4a player', 'this publish', 'lot its', 'pc health', 'js wing', 'наркозависи', 'blog site', 'pussy.', 'mbt exercise', 'uggs pascher', 'vigrx', 'j’ai tjrs', 'pleasant article', 'laurenonline', 'patraoonline', 'pantys child', 'fakecoach', 'lv', 'грузоперевозки', 'online betting', '.ph/user', 'leagle porn', 'latest weblog', 'porno tube', 'filosov.', 'chf iraq', 'mail.in', '%d0%b4', 'fantasia', 'blacktight', 'discount reebok', 'pp gold', 'cigsbuy', 'amazing videos', 'san xuat', 'linksexchange', '彩票網', 'link creati', 'photo/online', 'business first', 'miu sunglass', 'ҟy', 'com9.php', 'seeming vexation', 'refreshers.', 'timberlandsaldi', 'email.asp', 'beatsmonster', 'blog beast', 'produkter', 'excellent page', 'nbajers', 'uploaded', '&#116;&#105;&#109;&#098;&#101;&#114;&#108;&#097;&#110;&#100;', 'web sitte', 'gunstig kaufen', 'zapatillas asic', 'pornlive', 'video xxx', 'guccifactor', 'onlinedat', 'fotograf', 'cheapsalv', 'genital herpes', 'advert research', 'kawa strefa', 'shirts online', 'video youtube', 'replica chinese', 'guccij', 'говорим гово', 'vuitton brace', 'com/mk', 'pascher', 'chaussurembt', 'prazer sexual', 'iphone6 case', 'slugi plotnik', 'vapor cloud', 'saletrain', 'bangkok cosplay', 'silverjewel', 'woowoo house', '+asic', 'blog diaria', 'eomarketplace', 'personalinjury', 'dumps with', 'images/bv', 'eyeglasses sale', 'new car', 'cartier', 'for interact', 'business daily', 'ugg on', 'bot1.', 'эротика', 'tips unlimited', 'game online', 'kors factor', 'boutiqueschanel', 'detrol.', 'wp pad', 'coinmarket', 'teensex', 'mens train', 'boutique balenciaga', 'moncler out', '.widblog.', 'a lots', 'pantie play', 'yslfemme', '他の製品', 'ヴィクトリアシークレット', 'redirects.asp', 'ban wayfare', '15.asp', 'marketexchange', 'открыть сайт', 'default2', 'calvin klein.', 'instant loan', 'floating board', 'stighollis', 'stobuys', '锘縉', 'chemale porn', 'my page;', 'ka.th.l.een', 'coachmise', 'simonly', 'prada new', 'lechit narkomaniy', 'sozedde.in', 'purchase guilt', 'instagram in', 'japanese converse', 'and shate', 'pill cheap', 'ogrodowyparasol', 'orgazma', 'longchamp out', 'pandora top', 'canadagooses factor', 'compare', '3.php', 'casque beats', 'monclesito', 'occhiali.', 'porn star', 'my bitcoin', 'personalnatur', 'mycinonline', 'prada dress', 'www.foteczka', 'replicaprad', 'help.cfm', 'v.@', '.ph/1', 'nikeairshop', 'moncler sold', 'モンクレール', 'best pant', 'web solution', 'sacstrail', 'soldes ralph', 'винтовой нас', 'manchettehermes', 'redirect.jsp', 'youtube down', 'աu', 'assets grow', 'poco prezzo', 'cheaper', 'app.htm', 'relogioreplica', 'site:', 'supremacy', 'viraleas', 'vestesmoncler', 'asicsgel', 'funding.su', 'mbt+', 'powder review', 'www', 'ad live', 'feihuang0', 'chapa shoe', '脿', 'jacobs geldb', 'hermes belt', 'underc.', '&lt;!', 'targettedtraff', 'sabo shop', 'sledge baseball', 'proficiency lesson', 'remarkable website', 'áo gỗ', 'poker0', 'presentation however', 'casino 1', 'gifts singapore', '&#070;&#069;&#078;&#068;&#073;', 'serch engin', 'forte dsc', '%d1%80', '+mulberry', 'check.asp', 'replicas', 'vendida separadamente', 'triviatrivia', 'categories', 'ugg bailey', 'a07.cfm', 'need sex', 'アウトレット', 'tօ', 'fake sunglass', 'louboutinspas', 'hermesdisc', 'neat site', 'jordan sc', 'telecharger.', 'niue pokemon', 'thailove', 'educational paragraph', 'moncler 2015', 'x galler', 'bulgarie bijoux', 'naturalcure', ', ,', 'bags afford', 'lpgn laminin', 'sensual.', 'cheapstephen', 'bottes pas', '.quitsmoking.', 'еa', '@pro.wire', 'tmp', 'montblanc pen', 'zagorodnyy dom', '6purse', 'b01.htm', 'mulberry hand', 'behind knee', 'decent post', 'my site:', 'pens montblanc', 'purchase generic', 'buytraff', 'lookingfor', 'gia cong', 'gaga milan', 'marketingprof', 'laurencheap', 'jackenwest', 'global known', 'monclerespa', 'care maintenance', \"sta'sean\", 'b2.cfm', 'plus dating', 'web blog', '.uzblog.', 'kiss love', 'relações sexuais', 'legalhack', 'network scam', 'comment/bv', 'belizepropert', 'versace jap', 'tafil', 'mbtmen', 'blogger', '.bot3', '4 u.', 'postinfo.', 'salecanad', 'marinesgay', 'cartier fidanza', 'comment/nike', 'gucciden', 'cpaclick', 'suggested page', 'worldfuck', 'its genuine', 'boy panty', 'nato apostando', 'bestsellerbag', 'essayer cette', 'commerce whole', '?gid', 'massages erotic', 'essay intro', 'definitelly', 'wife tube', 'us/profil', ',me', 'generator 2015', 'pumasneak', 'handsome gold', 'payment nigeria', 'spicy bbw', 'ゴヤール', 'loubiton', 'porn gratuit', 'c.reas.e', '?t', 'nhl nhl', 'a2.asp', 'timberland villa', 'spammer spam', 'hoganoulet', 'venetabors', 'gay cartoon', '트위터', 'ojectx', 'acne prescript', 'stumbledupon', 'engines account', 'gamme reacute', 'casino hack', 'dangerous post', 'contoh plakat', 'canada goose.', 'replique femme', 'amazon.htm', 'football', 'best home cinema', 'ed in men', 'taboo matter', 'networks truth', 'negozi burberry', 'zaidimai', 'drugbuy', 'renda extra', '.ro/1', '.top10bestsellers.', 'wallpaper free', 'several opportune', 'sunglassok', '.ru/aa', 'bulgariout', '=kobe', 'livearticle', 'cordarone', 'cigs online', 'fine wive', 'fxprofit', 'ugg style', 'moncler pas', 'ooobrand', 'outlet italia', 'com6.php', 'ysl schuh', 'tracking a phone', 'cheap ferrag', 'porn big', 'pricetobook', 'generator2017', 'regarding blog', 'hollisterbolsos', 'sprzataniu powier', 'silverandgold', 'worldxxx', 'qry', 'okanyway', 'jointpain', 'minecraftfree', 'carb nite', 'fg xpress', 'taionline', 'stimulation sex', 'websitepost', 'mobile porn', 'goldseiko', 'site technique', 'money generat', 'shop erotic', 'plug in', 'uploads', 'sweet cum', 'zapatas dc', 'publish amazing', 'pandora', '00 preparing', 'megaculo', 'justcloud', 'tem.pte.st', 'debt solution', 'free shipping', 'cheap michael', 'venetaprezzi', 'old|', 'site backup', 'cheapred', 'jersey.cfm', 'hollister job', 'd http', 'windowsphone.co', '8.in', 'goose coat', 'textile urbain', 'très high tech', 'uggthree', 'dat online', 'crakclemovie', 'shredhd', 'lvsale', 'astounding4', 'penning this', 'cuz this', 'kitchenknife', 'ミュウ', 'reports.cfm', 'moncler site', 'astute command', 'link track', 'outplacementcompan', 'sign zodiac', 'prescription acne', 'faildrug', 'retina247.', 'cheapcoach', 'thomassabosale', 'the pokie', 'master.in', 'northfaceuk', 'augment', 'aktuel news', 'tumitumi', 'kors laptop', 'cached', 'comhttp', 'html', 'northfacepascher', 'promotecampaign', 'marketxchange', 'my homepage', 'ban jack', 'coin market', 'bag.asp', 'skjønnhetsprodukter', 'boots online', 'reverse phone', 'magasinchemise', 'offer.net', 'unique article', 'рекламу', 'furla candy', 'aktuell news', 'faux coach', 'fastidious dial', 'profesjonal', 'chaussures', 'autentico', 'xurl.es', 'sehmale movie', 'promocja', 'pantiessex', 'designer bag', 'child pantie', 'successfulpost', 'worlds most', 'vapecloud', 'griseparka', 'article pow', 'naturlig penis', 'pornolady', 'valkiriarf', 'auspicious repl', 'biżuteria', 'replicadesign', '.bloglovin.', '01.php', '4.php', 'ventures impart', 'lv replica', 'a5.asp', 'mulberry purse', 'ultimatestag', 'link seller', 'paris sécuris', 'chlorzoxazone', 'vendita', 'оформить груз', 'andcloth', 'pyt.h.on', 'vape pen', 'ligne medicament', 'your success', 'top 5', 'furla hand', 'storre penis', 'hermesfactor', '.skque.', 'replica jack', 'porn preach', '.bravesites.', 'contact', 'asphyx dvd', 'ka.th.leen', 'adidasout', 'basket chanel', 'вольво (volvo', 'pharmacies from', 'usingn s', 'mbt tunisha', '.detrol', 'addons', 'botas mujere', 'servicii profes', 'hollister sandal', 'make csgo', 'magasin chemise', 'goose parka', 'hot.php', 'gabapentin', 'longchamp 2017', '9the best', 'rusex.', 'asics paris', 'storeout', 'provider proficient', 'hack cydia', 'hommecanadagoose', 'kommissionen tager', 'freecrypto', 'image old', 'agencje', 'подробности смотри', 'campaign.', 'kamere 4k', 'résultat pmu', 'totally free', '.tafil', '?celine', 'oakleysj', 'vuittondamier', 'louboutinwedding', 'js', '+.cfm', 'moncleruomo', 'nude erotic', 'size genetic', 'uggblack', 'luminor watch', 'great bet', 'b.a.g.s', 'myweb', 'sentence word', 'даклатасвир', 'pornocomic', 'blogsys', 'afflictionmen', 'katalog', 'perfectwrit', 'nobisj.', 'me passionne', 'loosely user', 'cagoosesale', 'mulberrypurse', 'wiki/index', 'cuir vanessa', 'cheap chinese', 'porn post', 'kidpantie', 'coincheat', 'pills review', 'onlayn', 'borseluis', 'webnode.it', 'сђ', 'to gget', 'pufy', 'porn porn', 'clearance', 'butikk', 'porn tube', 'profit engine', 'vinder jakker', 'shared site', 'onlinebetting', 'sozedde.ru', 'purses online', 'clickforu', 'bags cheap', 'dudes.de', 'price to book', 'nba jers', 'shirtsembroid', 'easy naked', 'seks.ro', 'шаблоны вордпресс', 'tarifmalin', 'tomsdamen', 'www.cryptod', 'keylessremote', 'uggenfant', 'kinder moncler', ')', 'mp3la.ru', 'avental indispensavel', 'психоло', '+vti', 'investinwell', 'слот', 'latvia stag', 'understood of', 'collectif moncler', 'ženy run', 'every stuff', 'top advanture', 'custom wheel', 'cosmeticsout', 'quality article', 'parafoninfo', 'prices now!', 'hello people', 'fabulous drug', 'doku', 'woolrich', 'aff.cfm', 'ugg class', 'reliable rx', 'kors canad', 'hobo fuchsia', 'www.net', 'erotic desire', 'homme giorgio', 'jerseys3', 'juicycouture', 'ipad repair', 'pharmacy.', 'therefore he', 'nfl austr', 'zinger baseball', 'games cassino', 'wentoutside', 'camisetas', 'theyll', 'qsymia', 'internetu kvepalai', 'passrecovery', 'adsense.', 'mrwhite', 'lucrar renda', 'oakley glass', 'markergaul', 'k.a.thle.en', 'bigreplica', 'b03.php', 'blog sys', 'nike1', 'relief secret', 'amusement account', 'private proxy', 'boa qualidade', 'zenonia', 'blogs trick', 'guest book', 'grise parka', 'cheap whole', '0.asp', 'web page;', 'lesbianbdsm', 'reflux symptom', 'enhance pill', '1to1', 'serial numberul', 'pantie girl', '.louboutin', 'presentation subsequent', 'weblog format', '革の', 'jack', 'maxbet', 'ԁe', 'bestdrug', '.ru/lol', 'top 2', 'laplap', 'youtube sens', 'unlimited multilang', '.or.kr', '6cigar', 'indre pengemark', 'emails sometime', 'iphone 7 plus lock', 'vivier pompe', '100% authentic', 'tetonas mega', 'shoe sol', 'millen colour', 'images/prada', 'hemp', 'прикольные поздрав', 'agent marbella', 'anellicartier', 'bitcoin wallet', 'fact awesome', 'элитную мебель', 'derniers modèle', 'hollisterjean', 'does green', '.phexin.', 'patagonia zone', 'undeniably believe', '.property sales', 'transform vhs', 'legal porn', 'groupsex', 'gafas ray', 'mini credit', 'shopuk', 'longchamp.', 'fashionable web', 'shirts embroid', 'www.escort', 'nike mercurial', 'wonderful post', 'tutorial.htm', 'tatuagen feminina', 'mcm backpack', 'posting comment', 'cable speaker', 'teens porn', 'incessantly thought', 'currencytobuy', 'com0.php', 'armani cloth', 'details necessities', 'purses out', 'sales5@', 'abercrombiemen', 'sac vuitton', 'outletonline', 'every advert', 'a08.htm', 'thoughtful shop', 'anneed from', 'fashionista blog', 'college loan', 'nice practice', 'cracked pro', 'profil', '.ph/content', 'fashionbrace', 'pharmacy at', 'jeremyscottwing', 'x.@', 'supplement energy', 't.the', 'nude vid', 'dental guru', '.whipme', 'toryburch1', 'hot.', 'kardashian', 'adospopulaire', 'fund', 'shoesus', 'virussecur', 'swords trade', 'moncler.php', 'alienwheel', '＄８０', 'gruzowe', 'official sale', 'headset 2015', 'hogan disc', 'comment pag', 'mbt jp', 'waterprrof', 'salomon athletic', 'ray_ban', 'readers’ base', 'thedirect', 'ugg black', 'shops.', 'buyma.', 'gooseyork', 'goose york', 'marantrain', 'key', 'negozialviero', 'store serv', 'kidsnike', 'inotfmarion', 'sale.co.', 'blog look', 'carders board', 'sport interaction', 'ephedra', '.tatuagem', '50mg.', '8generic', 'hovershop', 'poker 3', 'vidéovue', 'luxury chanel', 'sims 3', 'teamworkcoach', 'bookmarks.', 'sunrize', 'fbfans', 'vapour pen', 'era nba', '７０％', '+pokie', 'ugg', 'hogan store', 'coach emboss', 'fakeray', 'jordanscheap', 'weight loss', '2016 seo', 'promotion bag', 'coach thai', 'mens watch', 'ylur business', 'seo tool', 'veste belg', 'fuck', 'cool article', 'view/pg', 'rosuvastatin', 'cigarette', 'casino game', 'adidasuomo', 'gma1l', '123.za', 'i aam', 'ok3.', 'problemowych', 'hogan oulet', 'последние новости', 'youur', 'louboutin cheap', 'showtopic', 'escort.', 'designer label', 'marketing online', 'sexy virgin', '.in/store', 'foot disc', 'factory out', 'dieseluk', 'oceanesimmo', '%d0%b7', 'artiicle', 'prom dress', 'promosys', 'system yoga', 'fajnastrona', 'marketing tip', 'runshoe', 'chanelonline', 'sandaljp', 'wayfarerbarato', 'suplementy.', 'svente enligne', '.blogzag.', 'japonesa tatuage', 'ã¤', 'b02.jsp', 'материал', 'innerestitg', 'linking issue', 'acheter dolce', 'panty kid', 'makeownhood', 'party', '.pussy', 'nike tn', '.jpg.asp', 'tokenhack', 'バスケットボール', 'звони!', 'филосовские мысли', 'cassinoonline', 'pharma from', 'personal silicone', 'b8.cfm', 'css.asp', 'russianms', ':post', 'fendi', 'maxsale', 'my btc', 'resize', '05.cfm', 'adidasjeremy', 'casino view', 'cheapbike', 'разработать игру', 'jacket2015', 'morelv', 'newport100s', 'different wordpress', 'server2010', 'vhttp', 'professional2015', 'pc stuff', 'goose online', 'camisetas personalizada', 'group xxx', 'mlsp', 'celebrity diet', 'xxx addict', '.com/%', 'malls store', '4u.co', 'virgin.porn', 'bitcoin talk', 'clean clear', 'marshall guantes', 'profitpro', '?longchamp', 'snapbacks', 'magasin robe', 'assjob.', 'cazino', 'workoutsale', 'revival beauty', 'trazodone', 'com/autocom', 'giubbotto official', 'assistance informatique', 'dumps forum', 'gratuit annonce', 'mlb.jsp', '%d1%89', '年销售额', 'rakuten.', 'mmy page', '.royal', 'статья', 'buyhoverboard', 'louiswuitton', 'nfl+', 'him delivery', 'bottega veneta', 'da man!', 'baykal otdykh', 'purple longchamp', 'kath.l.e.en', 'server2007', 'el gordo.c', 'knowledge gamer', 'установка канали', 'configure.', 'wellness ailment', 'reborn blog', 'mujer timberland', 'reborn website', 'fun with window', 'bdsm', 'feature.cfm', 'thxgamers', 'seo with', 'petroleum kohlenwasserstoff', 'big titt', 'консультаци', 'golfs', 'revenue master', 'will hemp', 'den pha led', 'burnout hack', 'galerieworld', 'benzo generat', 'mywweb', 'everyinfo', 'thatover', '.web2.', 'hgh sup', 'blog soon', 'bvlgari bzero', 'cheap travel', 'login.cfm', 'actually certainly', 'follower love', 'procedures for', '1.za', 'hardness quantity', ':adidas', 'astounding1', 'wanna say', 'nike air force', 'paragraph here', '4cheap', 'info.', 'magasinrobe', 'pra acabar', '.blog.telrock.', 'main.cfm', 'mana efficient', '.fr/,', 'виагра', 'a01.asp', 'business catalyst', '節約', '30mgcheap', 'consolidation loan', 'umschuldung', 'inbox9', 'bitcoin cassino', 'door blog', 'mbt uomo', 'armani', 'footdisc', 'admins', 'cheap jord', 'barbourjack', 'gabbana shop', 'parajumpers', 'promotion store', 'shoes', 'thumb.jsp', '.jpg.jsp', 'grossen ray', 'uae/service', 'noticias, noticia', 'табак танж', 'longchamp sac', 'duvetica.', 'целевые клиентские', 'louboutin', 'quality writing', 'son copain', 'vintovoy nas', 'invest.su', 'goed jassen', 'hɑ', 'los replica', 'my cryptocurrency', 'asian sex', 'party xxx', 'webpage', 'ing puter', 'genuine pandora', 'montblanc rollerball', 'directory.', 'боброва люб', 'starsunglass', 'sexual fantas', 'high heel', 'burberry clear', 'ÿþ', 'hermesout', 'prac licencjackich', '?doc', 'less peplum', 'sentienthealth', 'health related', 'instructorcar', 'timelines cheat', 'xanax', 'own blog', 'ff17store', 'face', 'stop snor', 'clockspart', 'raahe0', 'sms grupp', '７０歳', 'bcbg runway', 'takenwith', 'usinformed', 'control', 'ihover', 'miu miu', 'ligne médicament', 'rpgonline', 'puma sverige', 'list', 'librarys.cfm', 'you sees', 'outlet2016', 'jordan 1', 'заправка картридж', 'recommended internet', 'burberry.', 'chanel replica', '50mg cheap', 'business net', 'generatingcash', 'shift dress', 'jobs usa', 'swegway', 'replica prada', 'monster.download', 'yourblog', 'armpit.', 'nike shox', 'millen clear', 'event', 'wо', 'autopostunlimit', 'love life', 'пенная шоу', 'ugganimal', 'cheap carton', 'running obuv', 'akkuschrauber', 'b http', 'clearanceout', 'hotelbritannia', 'posttestimonial', 'sex advice', 'currency buy', 'hacking', 'truly peaked', 'videosex', 'customized dissertation', 'strong populat', 'candycrash', 'moncler andorra', 'бесплатной диаг', 'clone key', 'videos porn', 'woah this', 'sneakers', 'studios beat', 'targeted keyword', '.ph/pub', 'sp1 key', 'vuitton fx', '<a>', 'elite sample', 'shop boot', 'www.lolita', 'next articles', 'si’ve', 'informatique enligne', 'skidki otel', 'valuble', '.com9', 'dolce bag', 'giubbotti online', 'birkin bag', 'catalogue.jsp', 's.po.r.t', 'whichpag', 'lottery.', 'dwi attorney', 'iamimport', 'monclersweater', 'professionals.co', 'whoah this', 'b05.asp', 'virtualsex', 'шапка', 'porn free', '%d1%8d', 'dental that', 'r.all', 'longchamp fabric', 'produit hermes', 'birkenstock boston', 'free weblog', 'prada girl', 'videoview', 'a03.asp', 'pisanie prac', 'cnoesfs', 'roxy ugg', '.hatenadiary.', 'businessfinder', 'weekly profit', 'php.', 'outlet louis', '.ro/top', 'monclerski', 'westwoodboot', 'загородный дом', 'erotic love', 'camicie abercrom', 'milanos jp', 'isabel marant', 'effexor', 'fuckpic', 'writing he/she', 'onestime', 'jacket sunglass', 'cabin lithu', 'r.co', 'b02.asp', 'doudoune', 'help essay', 'human verification', 'schoenen seite', 'timeline hack', 'beats studio', 'withher', 'server2016', 'polohollis', 'bruno cabas', 'slippers khaki', 'substance deal', 'traffic.', 'raybansuk', 'massage massage', 'choice ok.', 'cover wholesale', '割引', 'impressive post', 'mintcoin', 'captcha snip', 'medrol4mg', 'nuestratienda', 'cheapestprice', '.ro/pub', 'online internet', 'beijing massage', 'cheap sherr', 'vuitton women', 'mydomain', 'aѕ', 'cigarettes online', 'mk us', 'storeserv', 'femdom', 'клипса антихрап', '.allocating', 'top list', 'poished', 'bestdigital', 'marant shoe', 'timberlandswede', 'natwerk', 'camisetas hollis', 'wowo site', 'pokies', 'library.asp', '|', 'sieg heil', 'filmizle', 'luxottica ray', '.bdsm', 'montre bvlgari', 'kartridja', 'скидки на', 'carinsur', 'sacchloe', 'г¤', 'prada cell', 'handbagsu', 'gaming', 'malepower', 'offerta ray', 'intelligently about', 'suspicious ensure', 'systemweb', 'maillotsligue', '4u.pl', 'enlightening post', 'budget dedicated', 'vidéo buzz', 'tiffany jewel', 'оптимизаторы удале', 'protandim', 'vestes belg', 'buycanadagoose', 'outlets online', 'gagner', 'shoes 2013', 'wedtgh', 'm88 m88', 'poker money', 'bridalshop', 'panty play', 'traceserv', 'wweb page', 'zlatehory', 'billion yuan', 'łódź', 'рулетка для', 'kosmetyc', 'new/iphone', 'blog/owner', 'forcheap', 'montaj montaj', 'bat pro', 'montblancpen', 'video tube', 'rss.htm', 'articleplus', 'adremusviral', 'monclerpour', 'soeasy', 'commenting any', 'b9.htm', 'obtain consol', 'pg/post', 'коптер dj', 'breed assimilat', 'vyrubka derev’yev', 'picloader.', 'bikini salg', 'outlet bag', 'blog here', 'letter template.psp', 'sharing site', 'signa zodiac', 'secure smtp', 'inspired hand', 'paulsmithsa', 'northface', 'damen timber', 'enchère max', 'web3', 'wordpress web', 'nhl', 'additionally fly', 'schuhesky', 'миллион', 'maillots ligue', 'thumbs.htm', 'best blogging', 'ecig.', 'lonikog', 'oakley cheap', 'shoes 2015', 'replica cartier', 'generation algorithm', 'metlifecare', 'maillotbundes', 'bags jap', 'globalistagenda', 'iapple', '&~', 'pokemon coin', 'gestione', 'accupril', 'russian mis', 'index 0', 'sexysex', 'hollister orig', 'voguetoms', 'reebok scarpe', 'zanotti sale', 'parafon.', 'answer.', 'jazz jersey', 'lesbian', 'outlet france', 'handbagssale', 'schuhegunstig', '.ph/page', 'available bargain', 'dehttp', '=utf8', 'skapa grupp', 'preferences 8', 'nhlnhl', 'sample@', 'eroticthrill', 'größen ray', 'monster jap', 'index.asp', 'gucci envy', 'competent essay', 'versacejp', 'dedans longchamp', 'viramune', 'chaus', 'fantastic understand', 'configura.', 'анкету ссылку', 'prix ugg', 'фото', 'fantasticentire', 'modules.php', 'millen neder', 'ニューバランス', 'bcbgsleeve', 'advertising method', 'uomo timber', 'rayban gunstig', 'bowling footwear', 'server 2010', 'healthcarehow', 'friday sale', 'latelier give', 'link pyramid', 'class web', 'tabalong', 'regarfing', 'addon', 'shopshoe', 'homeforsale', 'hentai', 'finance debt', 'sneakertrade', 'lunette oakley', 'internetlifestyle', 'xe', 'painless glad', 'vidéos buzz', 'bag gucci', 'engine ranker', '.armani', 'android hack', 'luxury replica', 'uggs bebe', 'pallet display', 'ショッ', 'create own', 'login uk', 'celeb diet', 'blahnikstore', 'apicpreate', 'tattoos cheap', 'есть скидка', 'outils', 'com1.htm', 'has cuisine', 'robertalite', 'fat quite', 'coupon sense', 'bikiniudsalg', 'blogspot2.', 'europe nfl', 'rе', 'cheap galaxy', 'legitscript', 'outstandingpost', 'playonline', 'depositcasino', 'trendsshop', 'amoxil', 'soldeloubou', 'allocating', 'uomo moncler', 'goose norge', 'cheap mulberry', 'jacobspurse', 'betor.biz', 'tjrs eu', 'amazing blog', 'goosegrise', 'funds.su', '7the best', 'gayxxx', 'game game', '1minuts', 'prematureejac', 'coins game', 'imitation chanel', 'bitcoin acc', 'yeezy boost', 'articles/suggest', 'mens bathrobe', 'gh0', 'panties kid', 'tattooscheap', 'panties sex', 'uomoscarpe', 'k  k.', 'tatuagem', 'sacs lancel', 'delivery philippine', 'mcm shop', 'jackets for men', 'jordans4', 'pignee', 'packaging printing', 'profits', 'wealthaffiliate', 'loiknog', 'fibromyalgia', '保証', 'redirector.asp', 'doktora.', 'hypothyroidism', 'webscheme', 'blahnik shop', 'dear lover', 'buy acrylic', 'louboutin8', 'skin pigment', 'jordans cheap', 'makingmoney', 'calvinklein', 'cr.e.a.s.e', 'vidéo vue', 'iphone4spy', 'tipsof', 'nbshoe', 'gooseenfant', '?utm_source=', 'bang bros', 'abercrombie man', 'maintainprevent', 'chaussures.', 'goosecheap', 'officiale moncler', 'prada out', 'cannabisoil', 'poor loan', 'timberland', 'limited edition', 'targetted visit', 'heartsite', 'teamtshirt', 'don think', 'payday loan', 'bags uk', 'enormous article', 'ssory', 'de site', 'shoponline', 'canadagoose online', 'femalehand', 'acetazolamide', 'professional2006', 'viral adremus', 'asshole', 'lauren amster', 'montblanc ballpoint', 'link build', 'armpit', 'clearance sale', 'rayban tokyo', 'buyplus', '111.ro', 'gratuiteannonce', 'manche longue', 'fridaynhl', 'pierr', 'coordintaing', 'my webb', 'search optim', 'seoagenc', '00.cfm', 'brand nfl', 'беторфирма', 'register paid', 'longchamp.asp', 'japanese swarovski', 'headset 2017', 'sobre tiara', 'iphone4 case', 'yourloan', 'buy jord', 'hairstraight', 'kors out', 'charms', 'sporttronch', 'outlet.mobi', 'kreddyt', 'fuck down', 'cent|', 'mkhand', 'go.a.dk', 'kitchen porn', 'thomas fat', 'bestpant', 'спорт на', 'equibase rate', 'b5.cfm', 'dressesbcbg', 'financecash', 'financialsolution', 'galleries porn', '7.su', 'rozszerzona', 'pornpreach', '.in/3', 'skincarereview', 'torrentmovie', 'coachpromo', 'dieseldenim', 'mariage hugo', 'dietpill', 'ugboos', 'beats pas', 'hollister jack', 'paris bors', 'exit.asp', 'facts door', 'formula blog', 'unknown.za', 'inspirfe', 'autocom', 'digi person', 'damen moncler', 'selllancel', 'imitrex', 'uploadingfile', 'nxvid', 'zapatas jord', 'aktuelnews', 'links seller', 'app nana sync', '4the best', 'dessa dieta', 'vinhobras', 'cliquant', 'moncler amster', 'insomnia tip', 'stodio beat', 'dating site', '&#1075;', 'real xxx', 'oakleycheap', 'soldesbotte', 'montblanc.', 'shoe out', 'buy fifa', 'gold coin', 'free internation', 'every light', 'ugg boot', 'costume mari', 'chinesescarf', 'web viewer', 'bbqr.me', 'not|', 'short ugg', 'europeannfl', 'sneaker trade', '.thearticle', 'laurent sandal', 'jeacoma.co', 'some functionalitie', 'zimmerman fund', 'top available', 'golfout', 'a02.htm', 'niselv.co', 'nikejers', 'outlet hermes', 'можно дешевле', 'a8.asp', 'haraka black', 'k.a.t.hl.een', 'porn ebony', 'mbt tariki', 'ꭼy', 'pradaout', '2014 seo', 'abouther', 'html new', '.disposable', 'individual stuffs', 'earthly lover', 'hair grow', 'mp3 player', 'supplements.', 'acne face', 'gmoolah', 'video vue', 'furla sac', 'golden', 'chanelhand', 'hot sale', '12.asp', '.in/content', 'successsite', 'albiongold', 'boutiquesmoncler', 'ejaculation help', 'aare feel', 'copieugg', 'comments/celine', 'jackets 2016', 'veneta port', 'to.p.ic', 'success site', 'burberryblack', 'pleasant good', 'privilege card', 'apex bionic', 'oczyszczalnia przydomow', 'biobronz', 'value server', 'handbags', 'one hundred%', '.blogspace.', 'publik seen', 'ambien', 'asics.', 'nuovohogan', 'salomon.php', 'affiliate blog', 'fashionist blog', 'manner puma', 'abercrombie', 'illusion origami', 'web0', 'flexglobal', 'gel virage', '+giub', 'pedophillie gratuit', 'pornos.', 'patriots hat', 'jacketsout', 'yougucci', 'toplist', 'highprofile', 'finite instant', 'default', 'shoesale', 'hasiltogel', 'kors tonne', 'storegucci', 'ifyou', 'ukloan', 'diflucan', 'wealthyaffiliate', 'bit koin', 'chose appnana', 'chiccassino', 'my ssite', 'tenormin', 'moncler prezzi', '.2u4.us', 'jacketcanad', 'взломать паро', 'gambling game', 'beaut bio', 'catalog.php', 'web optim', 'hatsindiana', 'pengemarked', '10mg.', 'themes sale', 'pmu poker', 'ԁu', 'onlineschuh', 'manolo blahnik', 'swaro', 'resist comment', 'abvout', 'click compan', 'lock prezzi', 'certain mastery', 'affordable', 'blog platform', 'mastery series', 'costly sufficient', 'porn angel', 'y.o.ur', 'hookup4', 'k.athl.e.en', 'pandora jewel', 'greatest option', '?prada', 'videopoker', 'scarpe basket', 'online.com', 'kors brand', 'template', 'scripts', 'circle jerk', 'kids nike', 'cartier love', 'ray bans', 'teach you every', 'downloadism', 'tips', 'notdienst', 'pdf/prada', 'louboutinpas', 'casino enligne', 'incrediblesite', 'yarosl', 'nobis.', 'chaussure supra', 'schuh ysl', 'northfce', 'sehmale vid', 'millen jack', 'sexy fantas', 'advantages', 'poke tcg', '3.cfm', 'mbt sneaker', 'tatuagens', 'gay xxx', 'twentysunglass', 'поиск лекар', 'posylka.izkitaya', 'goosejakke', 'burberrytask', 'hello admin', 'onsale.jsp', 'lv jap', 'herren moncler', 'vtxfair', 'speed proxy', 'investor.za', 'monclerpiumini', 'specific gift', 'fac visit', 'giubbotti timber', 'affiliate review', 'filpan.ro', 'recipe paleo', 'currency byte', 'furlabag', 'outlet woolrich', 'toms shoe', 'hogantime', 'uggitalia', 'foot insole', 'masalah member', 'mbttrain', 'ваши специал', 'doesnt', 'montblanc meister', 'fb sales', 'fuckhard', 'post extreme', 'nikese', '5minuts', 'essay write', 'knowledge student', 'girl eblog', '200 500.', 'extended essay', 'new mastery', '.ru/top', 'simply extremely', '.selebriti.', 'jordan store', 'porn diary', 'thanks.i', 'talknig', 'meridia', 'bitcoin', 'suggestedpage', 'poke amulet', 'ホグロフス', 'top site', 'webpage.', 'loans compan', 'navstevni kniha', 'shemael vid', 'coupon pag', 'server 2007', 'weddingreception.', 'membership', 'internet blog', 'index2', 'adult sex', 'adidasfotbal', 'nudes', 'mbtexercise', 'bayswater bag', 'girl gaga', 'mbt spaccio', 'feeds also', 'pantymania', 'pussies', 'шёлковый палант', 'tresore', 'sistemas populare', 'essayserv', 'jordans 3', 'shemale vid', 'gain weight', 'clsale', 'clck.ru', 'k.athle.e.n', 'աi', 'baby.', 'shoppers', 'polo hollis', 'ppsspp.', 'info?i', 'pantyssex', 'cpa camp', 'documents/rolex', 'clothes jack', 'you article', 'jacket jp', 'run shoe', 'установка забор', 'thumb.asp', 'nikefree', 'vendorlock', 'ppv camp', 'nailart', 'oakley frog', 'post.really', 'template.co', 'outstandingweblog', 'choo sale', 'detskie diskotek', 'exceptional blog', 'getting thoughts', 'sex search', 'photos/chanel', 'vuitton espa', 'пенная вечери', 'speed up', 'top bvlgari', 'everyone furthermore', 'com5.jsp', 'tratamente corpor', 'glucophage', 'mens jack', 'costumes', 'timberland boot', 'whois tool', 'youth jers', 'accordion hurricane', 'fashion', 'freepoker', 'porn photo', 'original devise', 'salestrain', 'article post', 'giubbotti official', 'catalogo/preview', 'greatest doc', 'this gucci', 'hollister deutsch', 'adidas super', 'cheapwhole', '.pussie.', 'christian', '3.ro', 'lupusinfo', 'www.numberone', 'bots1.', 'sharedssite', 'hogan shoe', 'onlinepoker', 'gagajp', 'fuckdown', 'vе', 'hmm it', 'write up very', 'extensive web', 'from subsequent', 'pantie pic', 'fler artiklar', 'article!', 'goose enfant', 'defiantlybrilliant', 'vapourstick', 'bukmacher', 'direct health', 'ремонт ворот', 'delete', 'comment speak', 'vests belg', 'самая крупная', 'fj.oga', 'sales factor', 'ge.tt', 'chaqueta cuero', 'ženyrun', '2.php', 'cheapchanel', 'zielona kawa', 'woolrich sito', 'sanders jers', 'linkvault', 'pokemon bux', '¥ó', 'medbaz', 'friday 2013', 'pradanew', 'online.web', 'kontakt.', 'laarzen kopen', 'ones time', 'std testing', 'shirt online', 'buy qsymia', 'uggsfor', 'frontier hack', 'island jack', 'initial traffic?', 'forhandlere', 'vasion.', 'этот сайт', 'customwheel', 'coach store', 'eurovid', 'vuitton hard', 'housesforsale', 'google mapa', 'landingpage', 'loteria download', 'loans canad', 'basketballword', 'неизлечимых медициной', 'really.. thank', 'plz help', 'jordangamma', 'oakley twenty', 'system', 'review best', 'bagscartier', 'egemtr', 'lebronshoe', 'you write up', 'dailyreview.co', 'ɍy', 'genuinepandora', '무료쿠폰', 'redirect.asp', 'bagonline', 'free prescript', 'buy movie', 'dollar jackpot', 'fifa18', 'sprinkler tune', '.su/profil', 'good homepage', 'interact internet', 'loans fast', 'replicanba', 'cheap hermes', 'stripper newcastle', 'coatsout', '.html', 'online бесплатно', 'replicas relogio', 'commerce retail', 'sacsguess', 'wifi jammer', 'press release.', 'ppsspp', 'toms price', 'cheap paper', 'scarpe hogan', 'purses', 'how to lose', 'pedophile notoire', 'woolrich out', 't', 'slut porn', 'no credit', 'wp', 'jordanmilan', '+build', 'cription.asp', 'friday 2016', 'discard.email', 'pussie', 'calitate super', 'sunglasses ray', 'puma paidat', 'profile/profil', 'best shop', 'payday.ro', 'erotic pro', 'parka france', 'rather enlightening', 'speedy product', 'shoulder tote', 'skor timber', 'ballgowns dress', 'moncler vast', 'mortgage', 'salaire instant', 'buyingfifa', 'top_bank', 'sharesite', 'gainers beat', 'shop', 'guest test', 'tattoo tip', 'arcteryx jap', 'iphone2you', 'com/page2', 'entrykey', 'говоришь гово', 'happybirthday1', 'chaquetacuero', 'fauxfemme', 'quick fast', 'kitchenknives.', 'nflofficial', 'шаблон вордпресс', 'klein prezzi', 'remont avtomat', 'sigareta.', 'woolrich arctic', 'femdomgaller', 'sincere understand', 'preferred essay', 'thai massage', 'informative blog', 'vasion', 'picload', 'getwidget', 'articles.jsp', 'essay topics', 'скандал жк', 'chirurgie plastic', 'a03.jsp', 'burberrynegozi', 'user/profil', 'websites design', 'ремонт автомат', 'pharmacyonline', 'popup', 'снижение веса', 'purse forum', 'moncler westen', 'seeking man', 'skinnys review', 'discountugg', 'injury lawyer', 'cу', 'teenvirgin', 'カジュアル', 'slongchamp', 'mercurial2016', '.proxy connect.', '1hand', 'squidoo.co', '=pandora', 'tagged', 'reebokscarpe', 'proficiency technique', 'good post', 'foreclose', 'diesel hot', 'pussy porn', 'wpimage', 'westwood online', 'royal', 'bengals store', 'giubbotto out', 'info/product', 'youyou', 'bardot lancel', 'セクシー', 'maxi va', 'membershiphack', 'collegeloan', 'timberland khaki', 'мышечной масс', 'logos', 'special.', 'canadagooseout', 'toes pain', 'без', 'spade out', 'camiseta', 'game casino', 'good}', 'clear clear', 'registrytool', 'redirecter', 'pussy eating', 'alendronate', 'бесплатная диаг', 'woolrichfield', 'free csgo', 'shoes 2016', 'cheapest ray', 'websiteusa', 'dominate seo', 'logotipo hollis', 'owtf.co.uk', 'porncomic', 'bootsoxford', 'celine sac', 'clit', 'многое другое', 'royal club', 'при дтп', 'horoscopes', 'jordans 2', 'irresistible website', 'moncler sweater', 'worldugg', 'платки батик', 'montazh kanali', 'gratuit roulette', 'internet savvy', 'com2.jsp', 'symptom med', 'footwear disc', 'phone gratuit', 'xx.ru', 'fantasticwebsite', 'turtle home', 'basketjord', 'handbag out', 'vitaminsfor', 'dtech affiliate', 'real estate', 'solde.pl', 'divulgaemail', 'marketing exchange', 'annonces gratuit', 'online.in', 'pradadress', 'bonnecopie', 'montre rolex', '¥é', 'userids', 'seohost', 'mortgage.htm', 'satchelbag', 'pliagecuir', 'free serv', 'killer blog', 'vitamin for', 'million hit', 'converse', 'level market', 'ads crack', 'pumafinland', 'proficiency badge', 'info/user', 'insurance home', 'floodai', 'tinyls.net', 'make hemp', 'trends shop', 'fagsex', 'bank24.su', 'pu貌', 'goldjewel', 'monster head', 'rug fur', 'google bind', 'goose jacka', 'style:)', '03.asp', 'metallo', 'artisanplombier', 'bitcoin depos', '.zlatehory.', 'tits', 'biofiniteskin', 'inconvenience comprehens', 'сроке службы', 'galaxy nail', 'source blossom', 'affiliate click', 'thebest', 'en private', 'pantys kid', 'erogenous picture', 'searchengine.', 'meilleur cassino', 'vuitton fanny', '.femdom', 'ugg barata', 'サッカー', 'preserveness', 'outnumbered gainer', 'professional 2012', 'demo7', '.bot0', 'financepak', 'money robot', 'skhemy', 'www.sadje', 'userupload', 'netverk', 'birkenstockaus', 'followers love', 'preferences 5', 'vicodin', 'wristband austr', 'oakleys uk', 'woolrichbolo', 'thus where', 'lauren norge', 'lululemon out', 'lublin.', 'site platform', 'big cock', 'herpes infect', 'ブが広がります', 'buying rune', 'schoenenseite', 'codesxbox', 'par/index', 'fifa ultimate', 'corneey.co', 'growing hemp', 'outlet usa', 'uggs sold', '?tumi', 'jerseys cheap', 'dominate secret', 'quitsmoking', 'coinsgame', 'viral play', 'hellothis', 'couples watches', 'do blogging', 'kors charlton', 'mp4 sex', 'whitenfl', 'ɍn', 'smaller article', 'fantastic job', 'kors bag', 'boncopie', 'résultats choquant', 'рєр', 'myssite', 'forex pro', 'instanttraff', 'runwaydress', 'hgh natural', 'nba', 'coach.', 'financial solution', 'it potency', 'аренды авто', 'protein powder', 'uggdakota', 'thankyou.htm', 'aroma paradise', 'vuitton purse', 'pornsex', 'cheat', 'clothing abercrom', 'p.yt.h.o.n', 'addict xxx', 'blogging glance', 'iphone5 case', 'situs poker', 'you viral', 'abercrombie uomo', 'zcode', 'ok0.', 'sales11@', 'cassino.', 'index5', 'considerations prior to', '深圳新闻', 'pédophile gratuit', '4 ever', 'throwback cheap', 'bonocassino', 'stewart furniture', 'статей', 'm谩s', 'windows 7 theme', '.herveleger', 'loveland.', 'sacsceline', 'pɦân phối', 'adidasblanche', '?hermes', 'injuryattorney', '+men', ',we', 'costumeshomme', 'new buzz', 'mbt uk', 'infected almost', 'haglofs', 'cheapuk', 'freembtrans', 'salomon', 'serravalle out', 'call futurity', 'burberryclear', 'poker texas', 'ff16store', 'belstaff out', 'cheapautentic', 'no excuse', 'actonel', 'օw', 'wilsonjers', 'sale', 'cl', 'cash masterclass', 'fake converse', 'coachaustr', 'hollister sale', 'качественный', 'new jord', 'oprawki ray', '40mg cheap', 'achetersac', 'uk/mulberry', 'globally known', '.adboard', 'remarkable blog', 'sale but', 'wheels.co', 'googlejava', 'hermes uk', 'masters.ro', 'porno pour', 'dlia', 'dobra kamere', 'kaepernickyouth', 'lunette', 'article plus', 'daily bloglist', 'tatuagen', '５０％', 'mail.ru', 'nail jap', '0.za', 'counter sex', 'era uk', 'najlepsze', '.supremacy', 'shirtcustom', 'your ex', 'emarketing', 'prezzi giub', 'duveticaout', 'jackets', 'tempt.est', 'showroom gucci', 'agario hack', 'parafon muscle', 'jerseyusa', 'sexyweb', 'getbitcoin', '04.jsp', 'redbottomshoe', 'mlsp suit', 'hernia support', 'make money', 'bloggers, business', 'thing way', 'massive web', 'raahe9', 'smurf account', 'fantasy', 'aktuelle news', 'burberry3', 'private.', 'tadalafil', 'tetona mega', 'codes amazon', 'hottest tech', 'dior.', 'fckt', '+cpa', 'boot sale', 'firma.asp', 'baykal extreme', 'eу', 'louisvitton', 'onlineshop', 'pricing', 'selebriti', 'beats by', 'mbt men', 'custom/en', 'authentic jord', 'ipad1', '11.htm', 'impregnacji', 'hollistersale', 'hack<', 'реальный кардинг', 'strony', 'nolvadex', 'tomsshoes', 'pradauomo', 'recommend internet', 'montblanc', '发泄物', 'points generat', 'elisa.ru', 'изготовлен', 'barbour coat', 'a2.jsp', 'media/sys', 'black friday', '.xblog.in', 'birkenstockonsale', 'lesbianporn', 'fat men', 'shoes+', 'show news', 'blogów', 'frequently', 'lifeinsur', 'asics', 'affordable hand', 'интересн', 'oakley medusa', 'video sex', 'schuhe puma', 'best.', 'paysafecard exchange', 'frebvic.za', 'empreendedor', 'passo_a_passo', 'there admin', 'hollistertiendas', 'discount bag', 'rigastag', 'data togel', 'debt help', 'tips for', 'jeanstaste', 'afl jers', '.it/?it', 'xxx fuck', 'bloglink', 'market.ru', 'spip.htm', 'splendid tiffany', 'p.yt.hon', 'hollisterpant', 'ok6.', 'ɑv', 'electronic advertising', 'zenys obuv', 'enorme fonte', '.pharmacy.', 'handbag', 'replica herve', 'tit pad', 'змусила на', 'coach kan', 'profils/profil', 'progressive jackpot', 'vulpyx.', 'farmacia.', 'linkdirectory', 'girl pantie', '4.cfm', 'foncier assurance', 'outstanding blog', 'webcamsite', 'arterrial', '.yves', 'freeiphone', 'ferra gamo', 'bestfit', 'linking serv', 'hermes abrasif', 'shoes men', 'yahoo spy', 'blog/index', 'flash slideshow', 'fast quick', 'pezones mega', 'logout widget', 'vinderburberry', 'fastidious urg', 'instant paysafecard', 'gorgeous escort', 'casinoer', 'high weblog', 'won webpage', 'トリーバーチ', 'firma sprzata', 'expertwriting', 'cheap replica', 'rom galaxy', 'airmaix', 'longchamp2014', 'nfl home', 'parafon', 'loan', 'paypalcash', 'urls fx', 'bbombr', 'jacketsforkids', 'administration', '.plotexchange.us', 'chaussures mbt', 'copy scape', 'replicaserv', 'eating hemp', 'legends hack', 'simply shared', 'ugg espa', 'cleveland jersey', 'sɦ', 'petite dress', 'facebook ad', 'bracelet replica', 'acid reflux', 'otoplasty', 'injection fact', 'sex hook', 'fulgor fantastic', 'cheap louis', '.in/serv', '+nike', 'auteurnike', 'healthcare stock', '.marant', 'shop jap', 'canadiandrug', 'アメリカンーエキスプレス', '0 easy way', 'host.ru', 'care+maintenance', 'weste jacken', 'wonderful site', 'replica7', 'chanel', 'bagsjap', 'chaacterist', 'stor punkt', 'ira kit', 'salomon run', 'outlet sale', 'epica watch', 'dynamic advertising', 'monclerweste', 'aу', 'nhl apparel', 'sunglasses.', 'herbal', 'valkiriarf.', 'balance espa', 'free cassino', 'longchamp yama', 'com0.cfm', 'm http', 'tiresdisc', 'woolrich vest', 'fastmoney', 'optionstrad', 'cash loan', 'sell iphone5', 'mcqueen shoe', 'руб гарп', 'lunarglide', 'pharmacy online', 'new seo', '.ph/katalog', 'pagina', 'woman bikini', '&#20605;&#29289;', 'lviv', 'vest moncler', 'seo', '.lesbian', '+christ', 'fashionlist', 'jacketswom', 'ferragamooutlet', 'nikeshox', 'guccisingapore', 'k.ath.le.en', 'authentic', 'a0.htm', 'b09.php', 'mulberry hobo', 'jerseys soccer', 'bigcock', 'pcsex', 'jpmonster', 'bulk clock', 'moncler homme', 'ugg shear', 'directmailsupport', '2014x.', 'examjple', 'leder jack', 'byte coin', 'medrol 2mg', 'кино позитив', 'instant', '인터넷카지노', 'where sell', 'insurance auto', 'raidershat', 'xtra size', '.kartridja.', 'pantys mania', 'death erotic', 'scarpe mbt', 'stig online', 'these however', 'mentalprocess', 'enormous page', 'loanfast', 'sex show', 'plussize', 'online shop', 'floxacin', 'download', 'brunosac', 'options info', 'thc review', 'jogging jord', 'prada dany', 'moncle sito', '+hermes', 'frebvic.pl', 'sexy.jsp', 'obuv run', 'quality page', 'headset2013', 'supra online', 'china cheap', 'промо', 'tratamente faci', 'pharmacy', 'xxxbest', 'orologi uomo', 'ティンバ', 'handbags whole', 'sexscan', '.rar.', 'miami escort', 'sacs trail', 'very imparted', 'abercrombiewom', 'crypto curreny', 'wpadmin', 'subscribe link', 'canada drug', 'deutschlandonline', 'freeslotmachine', 'key gen', 'fauxbvlgari', 'meilleurs cassino', 'precisely lots', 'succeed online', '.ro/member', 'cɑ', 'hermes wallet', 'tastegood.co', 'legit graph', 'orderparafon', 'bansunglass', 'choose calorie', '(nofollow)', 'jackets2016', 'ugg disc', 'baratos ropa', 'аренда пенна', 'dentalveneer', 'фирму', 'signo zodiac', 'neew user', 'canadagoosebanff', 'shopping harmony', 'ghdstraight', 'methodeargent', 'best tummy', 'fraudcenter', 'generic', 'ff15store', 'salejp', 'ametuer', 'colin kaepernick', '2017 popular', 'video pop', '.com wild', 'homepage', 'phttp', 'sparkle ugg', 'gstar jean', 'clock_kit', 'tattoo cheap', 'graysonbag', 'uk cig', 'carrera lune', 'mlb home', 'halt8hack', 'videos rank', '2015 prospect', 'health how', 'адекватного муж', 'friday nba', 'jswing', 'costumes homme', 'impressive blog', 'insdier', 'domen', 'powerball', 'giubbotto uomo', 'secret generous', '4.@', 'asos coupon', 'erotic wallpaper', 'gruppo gucci', 'fifaut', 'kamagra', 'market samurai', '貿易', 'coin.', 'pokeronline', 'book marks', 'strippers newcastle', 'sale+', 'xx.in', 'valise louis', '.pl/catalog', 'proizvodstvo', 'resultats choquant', 'guia completos', '05.php', 'answertraff', 'server.download', 'snore stop', 'chaussures ski', 'supra schuh', 'bestad.', 'prozac', 'set/chanel', 'systemdb', 'burberry bors', 'allpay', 'insane workout', '10.php', 'dresssale', 'quinoa stomach', 'реестр специал', 'mackage jack', '.rivulet.co', 'most effective', 'linkman', 'westwood necklace', '.ph/,', 'jordans for', 'worth bookmarking', 'helllo', 'installvirtual', 'boutiquembt', 'kazino', 'uggsale', 'outlet 2015', 'stag.weekend', 'teta mega', 'chettelongchamp', 'borsa ital', 'instant traff', 'chaussurespascher', 'videoyoutube', 'tutoringand', 'hack fb', 'beijingescort', 'smart hoverboard', 'weeklyprofit', 'niketotal', 'cheapadobe', 'online free', 'oakleyvault', 'remarkable practice', 'linkseller', 'jean kaufen', 'memberlist.php', 'article.pl', 'soldebotte', 'galleryporn', 'ожерелье', 'membership hack', '@gmai.', 'брелки', 'outlet kopen', 'secrets', 'revenue masterclass', 'coin foot', '2.ro', 'internet poker', 'pussyporn', 'legal bud', 'emagrecer podem', 'bxox.in', 'unwanted hair', 'unlock iphone', 'seek man', 'rehab', 'hderotic', 'music viral', 'become an expert', '%bb%bf', 'hommesac', 'fresh page', 'lingerie', 'валка деревьев', 'skidki na', 'fastidious post', 'прелестный сайт', 'goo.gl', 'sponsoring secret', 'profit margin', 'drmartensuk', 'finance serv', 'killer site', 'couture avec', 'fiverr method', 'clicks4', 'acyclovir', 'dresses bcbg', '买烟', 'incrediblewebsite', 'cheap youth', 'iphone6 spy', 'styles', '?u', 'bluray', 'fuckshard', 'party shirt', 'shop makeup', 'jeans kaufen', 'llet alone', 'onlinemed', '111.in', 'commercesale', 'moncler padova', 'mizuno shoe', 'porn galler', 'blog before', 'poker chip', 'shemael porn', 'bytecoin mining', 'ugg elsey', 'lululemonloca', 'newblance', 'officialugg', '+review', 'web ddress', 'im please', 'cosmatic store', 'traffic bot', 'baratas new', '.su/serv', 'jacken online', 'uggbarato', 'boner', 'fashionable page', 'fifa15', 'blog', 'price replica', 'publica foto', '2015baby', 'prada uomo', 'turystyka', 'mbt', 'menseekingwom', 'dolphins jers', 'рулетка джекпот', 'chaussures pascher', 'customized essay', '猫v', 'levelmarket', 'sản xuất', 'zozo.', 'consultation lawyer', 'prepaid credit', 'dragons avail', 'financpak', 'conquista baixar', 'viencare', 'go0gle', 'themesforwindows', 'baza dannyh', 'card dumps', 'infos', 'fencing produce', 'nfr.jsp', 'накрутка подписчик', 'tracingserv', 'official ugg', '=mcm', 'おざ', 'limited jers', 'product hermes', 'videnciavidente', 'woolrichcoupon', 'caches', 'clusivevaca', 'whipme', 'montrefemme', 'features.php', 'gold ingot', 'profil?', 'chrome=', 'длительное лечени', '2016 longchamp', 'agents marbella', 'insomnia journal', '+promo', 'hollistershop', 'itemnotfound', 'best evance', 'jersey 3', 'spyder jack', 'milanosjap', 'g.oa.d.k', 'օm', 'drink iconic', 'redwingjp', 'lauren homme', 'no cache', 'femmesac', 'hermesxl', 'vegas hotel', 'seo plug', 'webpeople', 'cazino.', 'northface jack', 'paxil', 'sex scan', 'puma nice', 'a04.cfm', 'ropa belstaff', 'weightnatur', 'raybanrb', 'знающие люди', 'loanscanad', 'bvlgari bijoux', 'fantastic blog', '+info', 'tax book', 'montre bulgari', \"!?'\", 'bieding laarzen', 'outlet 4', 'pédophillie notoire', 'cheap dumps', 'preference 9', 'latisse generic', 'seznamce', 'uggshoe', 'artiklar snart', '.com6', 'maestro', 'martini cagliari', 'blog centre', 'discount ugg', 'girlopop', 'tremendous writings', 'farmacia', 'caellis', 'for wholesale', 'globalagenda', 'mixappnana', 'پازل باند', 'accident lawyer', 'hoteles madrid', '%ef%bb', 'bazooka app', 'italy shop', 'host seller', 'magnificent article', 'mijn profiel', 'borsa luis', 'lі', 'blog entry', 'deposit casino', 'feature christs', 'hgh', 'elitemen', 'privatelabel', 'thnx u', 'симптом', 'cheapprada', 'unono chaus', 'hoolgain', 'chloe sunglass', 'ԝe', 'wedding reception.', 'egypt jersey', 'gkhk', 'handbags+', 'anyone folk', 'angel porn', 'canadagoose ca', 'ugg bebe', 'millen uk', 'casub.co', 'quitsmoking.', 'cosmetic review', 'faux bulgari', 'purchase tiffany', 'world porn', '.in/member', 'buy hoverboard', 'sales.co.', 'belstaffleder', 'wigs human', 'boardsale', 'sigarety.', 'essay serv', 'daintree residence', 'seekingman', 'oakley straight', 'chanclas hollis', 'chaussures adidas', 'panties boy', 'document', 'cг', 'moncleramster', 'привлекательный', 'milanouk', 'nba cappelli', 'wholesalechina', 'darrellbox', 'post', 'ferragamotie', '眼鏡│', 'kid baby', 'investors.ro', 'buy blade', 'outstanding site', 'kors diaper', 'goosesale', 'cache', 'raybanglass', 'site google', 'free real', 'distinct about', 'ventures profit', 'skilled blog', 'advanture game', 'giubbotti prezzi', 'medieval cost', 'millen dress', 'istanbul escort', 'registrator', '1st comment', 'cheapoakley', 'kjoleudsalg', 'outlet 2014', 'eyes porn', 'woolrich donn', 'rolex market', 'generator 2013', 'louboutinfr', 'sac gucci', 'for sale direct', 'songs reaches', 'pantiemania', 'lover.', 'korsout', 'seoplug', 'your bloog', '+cassino', '.su,', 'booty galler', 'mezzo louis', 'proficiency most', 'tele gratuit', 'bollywood moi', 'couple gratuit', 'девушка перм', 'a6.cfm', 'granny porn', 'nikeause', 't.em.ptest', 'faux', 'basket jord', 'femme rolex', 'username!', 'personal exper', 'xzzy.pl', 'frames cheap', 'korsglass', 'affliction jean', 'いい', 'epo doping', 'moncler uomo', 'belstaff espa', 'free pokecoin', 'interesting blog', 'fanshome', 'hoody cheap', 'slugi santehnik', 'jerseys nike', 'frlongchamp', 'remember must', 'server download', 'webtherefore', 'gaga jap', 'incredible topic', 'unlockiphone', 'labyboy', 'hermes official', '.doorblog', 'com/ysl', 'com8.jsp', 'reviot', 'sitemap.', 'mondayugg', 'blog viewer', 'paradisiaque', 'sessuali', 'read smaller', 'phoneadvert', 'uggs boot', 'watches.', 'zapatosasic', 'what youre', 'spaccio woolrich', '10.cfm', 'french escort', 'community.atom', 'mein sofa', 't.empte.st', 'vvfree', 'imitationhermes', 'bracelet store', 'baccarat view', 'infonetcom', 'com/htm', 'biz.pl', 'cassino online', '#file', 'gpswificlock', 'site officiel', 'xxx', 'topstop', 'citalopram price', 'first class web', 'hello world 2', 'outletjap', 'radiocarpea', 'abouts.', 'bons ganhos', 'titties', 'luxuryreplica', 'boutiques ugg', 'permonth.co', 'moncler barato', 'reviews best', 'bvlgaribzero', 'loveme.za', 'outlet 2017', 'you:', 'namacalnie zalega', 'sales8@', '.blog2', 'drecheap', 'trik selalu', 'handbagdistrib', 'cellskin', 'rizatriptan', 'sacs louis', 'pornhot', 'karpaczwsieci', 'exchange link', 'plated watch', 'mendating', 'cash advance', 'jibsjab', 'dinner soup', 'bizstore', 'ä±', 'polnocno', 'www.seo', 'much utile', 'kors austr', '.farmacia.', '+temp', 'dieseljean', 'mastery might', 'shemale porn', 'antipsori', '2sale', 'hookup1', 'facebooku', 'pantiesmania', '.ppsspp', 'cosmetiques sale', 'anynihtg', 'huaytoday', 'strick jack', '10mg cheap', 'loan compan', 'ugggünstig', 'advertising and marketing', 'members', 'uggsbay', 'wonderfulread', 'zdev', 'gold.ru', 'star sunglass', 'financial', 'boutique mbt', 'fashion women', 'theme sale', 'conf.', 'veston class', 'collection moncler', 'sky pharmacy', 'oԁ', 'social networking site', 'most expensive', 's.two', '.asp?tab', 'docid=', 'supreme essay', 'weight gain', 'prada sac', 'onlpy', 'malaysia casino', 'asianpic', 'software.za', 'khttp', 'домены', 'escorts.', 'popularna', 'face sale', 'forsale jp', 'face vest', 'salesfactor', 'fajna fotka', 'queenchiffon', 'related contents', 'chemale movie', 'seriously entice', 'jjl', 'レスポートサック', 'splendid placing!', '+downtime', 'pornotube', 'quality option', 'goose.php', 'adlive', 'borsa chanel', 'rayban.', 'gucci bors', 'finance emerg', 'settings2', 'files/new', 'muzyczny', '.ru/load', 'uggs bailey', 'net/turtle', 'finest blog', 'все подробно', 'friend', '.executive', 'untrasdend', 'cheapsupra', 'zhottie', 'pefcret', 'brothershit', 'information?i', 'timberland slipper', 'webanal', 'confira agora', 'mcm.co', 'libraries.htm', 'essay creat', '%titel%', 'mequinol', '13.php', 'pins dump', 'uggglove', 'slugy plotnik', 'tags:', 'generatecash', 'frutaplanta', 'timeshare', 'streaming site', 'stevewynnloan', 'fantastic weblog', '!! you', 'shopping ugg', 'share site', 'veneta out', 'покер', 'giantsshop', 'kneepain', 'project video game', 'great doc', 'all about the', 'features.cfm', 'casino site', 'giubbottomoncler', 'com3.php', 'partyxxx', 'baccaratsite', 'longchampsold', 'moore about', 'credit score', 'anonymousvpn', 'promotion shop', 'coachlegacy', 'lottery expert', 'seowith', 'friday2016', 'docter dre', '‡o', 'bagstore', 'b3.htm', 'burberrysale', 'vaporizer pen', 'review.asp', 'ukclsale', 'healingindu', 'calvinmujer', 'legitimate graph', 'cheesefruit', 'indeutsch', 'modules.cfm', '.ph/sold', 'coach best', 'coingame', 'b4.jsp', 'term dinner', 'ï¿½', 'up till', 'swissreplica', 'nba nba', 'доска объявлеий', 'fausse', 'korscanad', '.blogs0', 'cheap autentic', 'jackets jp', '=read', 'uggmocha', 'comreview', '.za/?entry.', 'receive carried', 'ka.thlee.n', 'pullover t shirt', 'g star jean', 'face denali', 'adbeatdistrib', 'wetting pant', 'prada.', 'uggspas', 'creditcard.org', 'forsale uk', 'cutting machine', 'specials', 'jerking.', 'loteria expert', '.blogs3', 'laurenshirt', 'loving natural', 'com6.cfm', 'soldes.su', '.postbit.', 'autentico ugg', 'journal/item', 'recovery now', 'nude share', 'penisblog', 'uploadfile', 'impressive homepage', 'matthewsjers', 'b09.cfm', 'wholesale bag', 'mass face', 'broncos hat', 'slotmachine', 'awsome', 'nicewebsite', 'порно', '.ph/2.', 'nice site', 'video.htm', 'norge canad', 'suppliers that’', 'luggagetote', 'vibro love', 'goose retail', 'price.', 'rok.su', 'poker 9', 'cosmeticsale', 'kе', 'poker6', 'greece holid', 'land hack', 'consalt', '.myblog.de', 'shoe dior', 'elementary entry', 'adultgirl', '.simpsite.', 'burberrywatch', 'attraction market', 'exchanging link', 'dre beat', 'favorite weblog', 'a3.php', 'bag out', 'informatik', 'great post', 'russian stalker', 'hermes evelyne', 'movies zone', 'escortzone', 'index css', 'longchamp.jsp', 'cosmetiquewhole', 'obuwia', 'theuniversity', 'nba utah', 'cher moncler', 'cheap lebron', 'fastidioussite', 'viewpag', 'ofargument', 'съемка', 'sacs chloe', 'moncler jack', 'ban handle', 'member.jsp', 'zero cost.', 'shoe women', 'logcabins', 'maillot ligue', 'medrol2.', 'we hqve', 'quality post', 'zapatillasasic', 'buyjord', 'galdi rebecka', 'fund.ru', 'bluetoothjammer', 'freehub', 'favourable web', 'lauren out', '|porn', 'kaepernick youth', '~7', 'burchout', 'damen kaufen', 'whatblog', '+shoe', 'free', 'uggs femme', '12.jsp', 'jagody acai', 'ventures pleas', 'ҟe', 'giubbottouomo', 'epub mobi', '.azmya.', 'yoyodiet', 'case study writer', 'jacobs dk', 'forum.ro', 'bracelets replica', 'excellentpost', 'mbtwomen', 'жк центральны', 'стратегия продвиж', 'culo mega', 'everysock', 'promotionalshop', 'moncler.cfm', 'facevest', 'cashextend', 'pursesout', 'job mom', 'benzoylmethyl', 'cheap fotbal', 'korsmessenger', 'blogbeast', 'suuccess', 'vuittonpascher', 'lesbian school', 'woolrichsite', '&#12488;&#12522;&#12540;&#12496;&#12540;&#12481;', 'предла', 'canadagoose', 'milanojap', 'opinion.in', 'outletstore', 'solde.su', 'blolg', 'lifepharm laminin', 'curcumin dose', '.propertysales', 'gucci.', 'sexteen', 'te.m.ptest', 'the best treatment', '鈥檃', 'hfgfh', 'avto.', 'peuterey roma', 'nү', 'paypal money', 'lunettesoakley', 'coatout', 'monclear', 'strona', 'shirt cheap', 'mlsp.', 'bracelet manchette', 'coachninnki', 'canadian loan', 'giubbotto moncler', 'lesspeplum', 'soft ware', '.pl/,', 'foot care', 'обрезка деревьев', 'fridaymoncler', 'giubbotto woolrich', 'millen shop', ',an', 'ã§', 'relativa media', 'site provid', 'financial debt', 'iphone4s case', 'excellent website', 'internet article', 'day.did', '.za/store', 'chaussuresfemme', 'vodstreaming', 'rentinsur', 'vuittonuk', 'gladreading', 'phonelookup', 'sportbook', '100 300.', '1.htm', 'site.co', 'quincy femme', 'vinterburberry', 'blackjack pit', 'ahaa,', 'lamisil', 'desmomelt', 'dorascrum', 'chanel out', 'host.in', 'cock', 'tees hollis', 'proficiency course', 'website look', 'gapscent', 'macha slim', 'hogan time', '¤¤', 'nikedesigna', 'mulberryfashion', 'һu', 'avental indispensável', 'webog post', 'r4i.', 'hollister gr', 'fendidonna', 'parafon info', 'rank increase', 'marketingtip', 'uggustra', 'result choquant', 'платок батик', 'vpn 4 free', '123.kinja', 'affliction new', 'weusecoin', 'common redaction', 'essay producing', 'tumi', 'jackets 2017', 'longchamp online', 'longchampmoncler', 'prada occhiali', 'ultimate stag', 'bestsuppl', 'informative weblog', 'it rite', 'parchet triplu', 'ugg short', 'jpcity.co', 'spotpoker', 'business know', 'streetsaw', '專業光碟', 'casino online', 'q http', 'ganadora clasica', 'http', 'market online', 'bijoux', 'coach wrist', 'mine day', 'grsentas', 'outletmart', 'visit my', 'pucci dress', 'giubbottidsqu', 'poker9', 'outstanding page', 'php?article', 'demoniakk', 'pantiesex', 'sex mama', 'acai diet', 'pornstream', 'pics, expensive', 'moncler quincy', 'причина пад', '&#12496;&#12483;&#12464;', 'femme sac', 'фирма бетор', 'links', 'z.in', 'saudável perder', 'ոi', 'free chat', 'красивая девушка', 'peso saudável', 'yours.', 'pass prefix', 'dealsus', 'aflreplica', 'borsechanel', 'get rid of', 'sitemap0', 'odejda', 'денег', 'cinture gucci', 'market research', 'premium out', 'jordan.', '2012 popular', 'zonejp', 'stag weekend', 'group review', 'dsquared shoe', 'core fuck', 'outlet store', 'medrol16.', 'mega teta', 'totosite', 'increasetraff', 'skins bet', 'buy plus', 'asd@', 'xomizz', 'australia boot', 'nude sharing', '９０歳', 'realtor promo', 'topnew', 'unlim pay', 'password generat', '.zero cost.', 'ripoffreport', 'diesel online', 'web sie', 'ウェブサイト', 'bears urlacher', 'mechpromo.', 'chic cassino', 'soldes.pl', 'top option', 'a09.jsp', 'marketresearch', 'ofhuman', 'gemorroya', '2@2', '1,0mg', 'marketing hero', 'asics deutsch', 'isotretinoine', 'insurance.', 'th th.', 'available now!', 'kinopozitiv', 'propertysales', '3taylor', 'us informed', 'meilleurs casino', 'futbol barcelona', 'unlimitedpay', 'promotions', 'findmewom', 'девушки сопровождение', 'scarpe', '.rusex', 'woolrichpiumini', 'analysis sport', 'poker', 'review.htm', 'pleasure secret', 'анифигасебе вылечили', 'quality available', 'loans2', 'niceinfo', 'yourwebpage', 'クラシック', 'golf access', '.in/wiki', 'spain jers', 'hoc sao', 'how to teach', 'itzshipd', 'lowestrake', 'woolrich scarf', 'secure', 'technolgy news', 'iҟ', 'cheats', 'affiliate ppc', 'pantie kid', 'widespread sorts', 'fucks your', 'plus.za', 'jordans1', 'jp/shop', 'your budget', 'ϳu', 'pills cheap', 'child panty', 'co.l.l.ect', 'york sportiv', 'kino online', 'zanotti online', '6 generic', 'breasts porn', 'seo pick', 'pages phone', 'design own', 'social media', 'adult vid', 'temp.tes.t', 'totaldns.in', 'boners', 'hemp braid', 'youtube: how', '.youblog.', 'iг', 'online cassino', '！', 'oakley sale', 'drupal theme', 'outstanding weblog', 'destiny power', 'baykal rest', 'girlescort', 'universal key', '?youtube', 'authenticstephen', 'discounts coupon', 'africancanadianmodel', 'jerseyfrom', 'dissertation writing', 'online.', 'supply', 'hhttp', 'dialsreplace', 'holistic health', 'magasin asic', 'templerun1', 'handbag uk', 'soleilchanel', 'legal steroid', 'shortcodes', 'remarkable site', '.clit.', 'evelyne bag', 'songs thousand', 'brainabundance', 'food cooking', 'νi', '99 online', 'mobileporn', 'b03.cfm', 'sluts.', 'wholesale iphone', 'gooseital', 'burberry coat', 'cheap jers', 'timberlandinneder', 'model.ru', 'how they can', 'topics.really', 'vestbelg', '10minute', 'boots get', 'shoe mbt', 'taille cost', '123.ro', 'tummytuck', 'outlet offer', 'outlet 3', 'onne minute', 'internet site', 'learn help', 'giubbotti woolrich', 'p.y.t.ho.n', 'uk mall', 'love lv', '4sale', 'guccigucci', 'tabletz', 'favourable blog', 'raahe6', 'panthers merch', '.pl/top', 'masters.za', 'finasteride', 'a00.asp', 'hoga out', 'laserowe', 'oakley polar', 'chaussures asic', 'timberlandmen', 'uggsusa', 'kobeshoe', 'buyingrune', 'seovps', 'dem tien', 'legitdrug', 'good for', 'afl.cfm', 'cam2cam', 'montblanc franc', 'giubbottiprezzi', 'models.za', 'heel', 'goose jakke', 'negozio hollis', 'link submit', 'greatarticle', 'secret beautiful', 'canadagoosesout', 'kors hand', 'herbal tincture', 'cig buy', 'mix appnana', 'muži bílý', 'woolrichamster', 'outlet', 'zapravka kartrid', '+.php', 'coach suto', 'lamborghini hover', 'north+face', 'pyth.o.n', 'fifa 16', 'temp1', 'vests moncler', 'camiseta personalizada', 'longchamp tote', '(buzzfeed)', 'suggestedwebsite', 'discount jord', 'paymobile', 'harga plakat', 'banner', 'montblanckuge', 'іn', 'joselyn sleeve', 'pokies online', 'getjoy', 'reseaux sociaux', '49ersjers', 'systems.in', 'russian mulberry', 'black www', 'cosmetic sale', 'mercurial 2013', 'superior resource', 'zaklady', 'kiss casino', 'personal pupil', 'оформить сайт', 'truly very', 'bumpant', 'with pics', 't.for', 'press release', 'exelon', 'mcqueen silk', 'mechanism kit', 'hppp', 'xxxparty', 'mint cash', 'for|', 'japan swarovski', 'cheatmaster', 'verkoop timber', 'gayredtube', 'win8pro', 'voyance<', 'loterias pela', 'remarkable weblog', 'business train', 'blahnikreplica', 'privat.', 'nike jord', 'blog:', 'enhancement pill', 'clickaffiliate', 'registration paid', 'superdryoutlet', 'www.shoes', 'liabaleles', 'favorable web', 'b05.jsp', 'tattoo sneak', 'blog’s post', 'bulgari store', '1@1', 'find a top', 'xxx free', '7hand', 'www.e market', 'baratas online', 'lotto.', '.pl/image', 'shoesmart', 'mcmbelt', 'fluoxetine', 'select your', '.ru/2.', 'amour gratuit', 'beneficial', 'repeated galdi', 'write subsequent', 'ユーチューブ', 'messageinabottle', 'представляем новы', 'cartoonporn', 'darmowe ogłoszenia', 'uggonline', 'xeloda', '2 the best', 'favourite weblog', 'men mizuno', 'laurenamster', 'eemai1', 'monclerjassen', 'pandora brace', 'buying insta', 'gamma blue', 'handbags for', 'lend direct', 'swarovskijp', 'new zune', 'moncler vente', 'instagram follower', 'seo vps', 'system review', 'hemp tycoon', 'fastidious my', '2016 prospect', 'netload.in', 'buzzvideo', 'bank credit', 'really fruitful', 'abercrom', 'free gay', 'saints jers', 'через поисск', 'cambogia', '.su/#', 'euro vid', 'obuv mesh', 'cheap loubou', 'maillot psg', '通販', 'configuration.', 'bvlgari replica', 'carfiac muscle', 'world sex', 'gamble online', 'bustier.', 'chik casino', 'sɑ', 'ulkotours', 'kensington parka', 'porn app', 'replica nba', 'dsquaredgiub', 'dissertation making', 'healthcare suppl', 'o http', 'watch hdonline', 'youtubesens', 'amateurs.', 'great low', 'h http', 'mbt sandal', 'articles.cfm', 'pursecheap', 'article.net', 'ϳa', 'poker 6', '3minute', 'keywords3', 'tongueretain', 'the first', 'bountiful with', 'message in a bottle', 'woriing', 'valuable website', 'oakleypolar', 'install', 'multilang portable', 'dobrucki.co', 'cosmetique whole', 'allbestmed', 'associate link', 'models.in', '.cool t shirt.', '.lipoly.ru', 'whereget', 'shoe america', 'dietary status', 'online business', 'golf plaza', 'systems.za', '@ma1l', 'vuitton glass', 'οs', 'images.', 'интим досуг', '.za/3', 'bulgari out', 'fantastic homepage', 'customer care', 'cheap virtual', '?opt', 'descriptive post', 'sexpapa', 'cinco jers', 'pandoracharm', '4gold', '2013 prospect', 'skortimber', '3@3', 'cheap soccer', 'diesel watch', '男同志 movie', 'aint goin', 'hermes replica', '＄５０', 'celineprix', 'azriaout', 'sexmama', 'dresses herve', 'fantastic paragraph', 'paneraiclock', 'property.', 'mhttp', 'groups', 'mutuelles sante', 'moczanowa', '花火', 'pgconcern', 'eϲ', '１０％', 'breast porn', 'loans.in', 'preference 0', 'knockoff hand', 'australia clearance', 'goose canad', 'coachya', 'gucci out', 'buy dump', 'loansfast', 'jogos', 'camisetaspolo', 'employee engage', 'ubezpieczenia', 'docteurmarten', 'buying traff', 'adidas adizero', 'raybanschwar', '49ers online', 'ff16 shop', 'link exchange', 'money site', 'family member', 'chinesewhole', 'story cheat', 'superstar class', 'friday mlb', 'serieserotic', 'sohbet', 'anuncios', 'parkas france', 'bdsm.', 'lunettes de soleil', 'ugg silver', 'һe', 'wasbusiness', 'oakley canad', 'asianteen', '+disc', 'milendress', 'mlmsite', 'vkews', 'affiliat blog', 'super compan', 'shopjap', 'away your', 'ipadrepair', \"valka derev'yev\", 'vidéos vue', 'hogan out', '?/pag', 'cheap prada', 'canadaout', 'sex papa', 'free movie', 'hollistercloth', 'aһ', 'muzi modry', 'massa muscular', 'disorder quiz', 'repliquefemme', 'buy likes', 'drugscheap', 'protein review', 'authentic cheap', 'u4nba', 'lunetteoakley', 'premium', 'north face', 'donna donn', 'chaquetas cuero', 'chung cư', 'thisgucci', 'dincob', 'm4aplayer', 'zwrot podatku', 'images/nike', '99 free', 'profit review', 'æ÷', '%u2019', 'frauenpuma', 'silver ingot', 'podcast.', 'arab sex', 'hoverboard scoot', 'huge great', 'female hand', 'exclusive.za', '30mg buy', 'online 2', 'boobs tumblr', 'adult erotic', 'women+', 'bookmarksweb', 'celineparis', 'zanotti sneak', 'php?tag', 'fast cash', 'www.howmuch', 'windows8theme', 'shoesjap', 'fr/botte', 'uggslipper', 'bags jean', 'titansmerch', 'adult story', 'aufsatz hilfe', 'osobistości', 'dreamproxi', 'plus.ro', 'mens coach', 'bestjers', 'sexy.co', 'coach austr', 'fastidious think', 'private amateur', 'top new', 'gamecasino', 'diesel', 'cigars cuba', 'mlb', 'web site', 'globalnpn', 'pages/page', 'snoringexpert', '00mgcheap', 'free shipping.', 'cheap basket', '+adidas', 'investors.za', 'goldgeek', 'jerseys 2', 'excedllent', 'other', 'titspic', 'naturalovarian', 'networks', 'bitkoin', 'kitchenworktop', 'wallpapers download', '.in/,', 'hair transplant', 'informativepost', 'casolete plastic', 'kath.le.en', 'outletbag', 'hats carolina', '試，', 'ray ban.', 'aswenr', 'expensivehaver', 'heuer', '.ro/shop', 'com5.asp', 'ban sunglass', 'mbtschuh', 'f_4', 'diet solution', 'index.php', 'firme sprzata', 'neatweblog', 'storyfunny', 'imap.to', 'legend hack', 'iphones apple', 'shoessale', 'hd muscle', 'dress shoe', 'fb ads', 'anonymity!', '2hand', 'australiaboot', '400 500.', 'nba%202k', 'zoe boot', 'chinese scarve', 'should statement', 'choise top', 'coming article', 'гo', '8 the best', 'body erotic', 'coach ninnki', 'zapata dc', 'selfgoogle', 'anilinovyye kraski', 'ejaculate help', 'ipod repair', 'wheretoget', 'gooseparka', 'sex shop', 'bags out', 'server2013', 'jassendames', 'unlimited crystal', 'roulette.', 'tomsforwomen', 'lisseur pascher', 'inc', 'x rumer', 'seo.', 'your website', 'pezone mega', 'uksonline', 'pictures blog', 'austria asic', 'articlestag', 'pleasant designed', 'hollister polo', 'ferragamo sale', 'ysl chaus', 'celular', 'sex xxx', 'in delicious', 'raybanuk', 'celine prix', 'bikinioutlet', 'healthkart', 'ferragamo tie', 'louboutin homme', 'preference 3', 'phonesforsale', 'vivekkunwar', 'coins fut', 'websites online', 'money.php', 'google', 'peer your', 'things we love', 'arsenal shirt', 'ϲa', 'kartridj', 'hollister bras', 'www.offerte', 'iphone5c case', 'profile', 'raybanaviat', 'debtrelief', 'yourpenis', 'zafar islamov', 'grawerowanie lubin', 'annonce gratuit', 'jersey4', 'how to quick', 'gamedesigndegree', 'hadheard', 'shopper.', 'italia scarpe', '5hand', 'beta/halkomentar', 'rayban sunglass', 'collectionmoncler', 'vapour cloud', 'adscrack', 'braceletstore', 'run cpa', 'expensive vaca', 'freeproxy', 'hosted blog', 'essays empire', 'success online', 'hats oakland', 'pokiemachine', 'prixchaus', 'successacademy', 'hot love', 'grow cannabis', 'havve a', 'sex virgin', 'hamster diet', 'ware crack', 'ashematme', 'com9.cfm', 'mercati di', 'poto therapy', 'blogs post', 'backpain', 'classweb', 'furosemide', 'virus spy', 'monclerfrance', 'privatlabel', 'link.', 'nfl.htm', 'uggsboot', 'writingessay', 'outletshop', '~4', 'ff17 shop', 'vergin porn', 'バッグ', 'forless.co', 'writingserv', 'outstanding website', 'track2 dump', 'ganoolq', 'uggbamsestovler', '.net/2', '.thxgamers.', 'japanesemarcjacobs', 'animatedporn', 'fine blog', 'mkout', 'доход', 'free crypto', 'venus factor', 'монтаж канали', 'hermes comp', 't.w.it', 'удивительные', 'cartierfidanza', 'consultation attorney', 'produce article', 'prentice capital', 'b4.asp', 'breakfast coming', '９０％', 'shopcoupon', 'adult base', 'pussys', 'early signs of', 'favourable article', 'angelsporn', 'video view', 'sac trail', 'sell iphone6', 'method guy', 'chce rozwodu', 'classic shval', 'motor wholesale', 'premarin', 'westwoodshop', 'timberland uom', 'curehelp', 'trading method', '2017x.', 'bot cheat', 'beats dr dre', 'europe girls', 'professional essay', 'couponcode', 'mbt haraka', 'giubbottiofficial', 'arsenaljers', 'it veryy', 'girl jap', 'its helped', 'everthing at', 'woori casino', 'андроид', 'cryptocurrency strat', 'gucci hand', 'de luxe', 'wɦ', '2the best', 'surf to', 'zapatos asic', 'burberry out', 'how to creat', 'wx1.ru', 'ban sonnen', 'dokumenta', 'pantalone hollis', 'library', 'học sáo', 'tumi1', 'adidas origin', '0.5 mg', 'greatdoc', 'diesel jp', '02.asp', 'moncler weste', 'bigtits', 'goodtemplate', 'referral king', 'diet chart', 'hollister', 'vapour stick', 'military friendly', 'natural estimulant', 'hot.hot', 'mobile phone', 'anklebootie', 'продаж', 'zapatodc', '.ru/stati', 'immobilier lux', 'relax private', 'where to get', 'irc chat', 'autentic', 'authored subject', 'chaussres boutique', 'klub.in', '2013longchamp', 'pill review', 'obrezka derev’yev', 'montblancstylo', 'bono casino', '?tid', '.ru/pict', 'j.@', '0title', 'fantastic read!', 'chemale vid', 'tiffany ring', 'sibutramine', 'camisetas.', 'document/north', 'wheels', 'system pro', 'private', 'have understand', 'pohudenie', 'une moncler', 'injuriesinsur', 'prada tshirt', 'lottiemaxi', 'k.at.h.leen', 'prescript.jsp', 'allinone', 'argente', 'membership.htm', 'nice weblog', 'produithermes', 'strategii binarny', 'mains lancel', 'femaleshand', 'dg shoe', 'ington boot', 'サンダル', 'internet.in', 'поисковик', 'forwholesale', 'anonymous', '.pl/member', 'essaysserv', 'e biz', 'uggs silver', 'sexy app', 'fendi1', 'kurs yevro', 'com/brand', 'ducati tumi', 'оh', 'russian mr', 'levitra', 'fashion hair', 'message erotic', 'tw.itt.', 'финансовой информ', 'b2b', '911.za', 'hanging with friend', 'panerai watch', 'videos.htm', 'powder bene', 'reviewstv', 'brand1', 'shoes.', 'nat&#252;rlich', 'nudegirl', 'shop nfl', 'buylouis', 'sitemap6', 'cell phone.', 'afl.php', 'sale nederland', 'piscine', 'montblanc jap', 'socialbutler', 'cheap bag', 'santehnik krug', 'fbmoney', 'giftssingapore', 'hand taschen', 'autentica cheap', 'longchamp sold', 'internet owe', 'a00.jsp', 'magnificent website', 'linksubmit', 'officeautopilot', 'replica rolex', 'anelli oro', 'onlinecassino', 'replicafemme', 'nourishment treat', 'burberrystore', 'webpage (', 'gobsinfo', 'lacoste.', 'burberryau', 'enjoy more', 'отзывы', '.bcz.co', '行動電源', 'プロモ', 'collectifmoncler', 'gucciuk', 'premium.', 'czesci skody', 'naproxen', 'day diet', 'nihon', 'videos vue', 'nice blog', 'stronaautora', 'wow cuz', '01.cfm', 'beautiful wom', 'westwood store', 'wtobrand', 'in skilled', 'pantiechild', 'yeezy3', 'windows anytime', '5.su', 'fast loan', 'deliver overtly', 'website daily', 'converse jap', 'bearsurlacher', 'caindex.', 'auto.ru', 'bcbg', 'purchases pitch', 'shoe online', 'sverige online', 'alone place', 'easing.', 'gratuitos', 'menmizuno', 'bota mujere', 'polosralph', 'moviesdl', 'お店を', 'britanniahotel', 'income', 'pass generat', 'the flashboard', 'sales2@', 'gucci replica', 'hotels egypt', 'wet pussy', 'store caldle', 'w.il.lkom.men', 'affiliatelink', 'loaded vid', 'models.ro', 'dу', 'drugs cheap', 'clientarea', '0day', 'advertise market', 'online 8', 'sac louis', 'spacciogucci', '.net/3.', 'auction', 'broker', 'group2', 'y tetona', 'язык алматы', 'glassess', '.net/user', 'penis enlarg', 'алкогольн', 'estimulante sex', 'schweizonline', 'commodity', 'replicawatch', 'fauxmontre', 'videosyoutube', 'systems.ro', 'team online', 'comprasion', 'del pene', 'manning jers', 'asses', 'talent recruit', 'noir homme', 'warfarinonline', 'caching', 'hithat', 'juniors baby', '¥¢', 'шкуры животных', 'my blog;', 'oakleys sunglass', 'daytona acier', 'webexplore', 'vapor x', 'moncler berriat', 'cash extend', 'hosting deutsch', 'outletsale', 'klub.ru', 'claim yukon', 'explosive growth', 'baikal rest', 'moncler coat', 'бесплатной демо', 'schooling dissertation', 'ipad 3', 'oulu', 'imitation homme', 'im happy', 'of herpes', 'daunen weste', 'x', 'lineshake', 'your lover', 'como ganhar', 'conversejp', 'ebonyporn', 'craft our job', 'medrol 16mg', 'tomber enceinte', 'stock plus', 'создать сайт', 'kat.hle.en', 'goose toronto', 'mens sale', 'akad hilfe', 'woolriche sito', '+event', 'pokie online', 'shemale movie', 'advert', 'tatuagem feminina', 'vuitton replica', 'up=date', 'eexpense', 'panthersstore', 'любимые игры', 'replica', 'max sale', 'albion gold', 'nieuwe zonnebrillen', 'nm.ru', 'uggs usa', 'your.mail@', 'online essay', 'clockpart', '.qsymia.', 'glitter ugg', 'shoedior', 'dotcomsecret', 'hgh enhance', 'citychung', 'ambalaje plastic', '.freeblogz.', 'bloom budget', '+load', 'login.php', '.ro/image', 'ugg cheap', 'article.za', 'jeans good', 'powerful scrape', 'ebaypic', 'thailand m88', 'sac hermes', 'secureimage', 'to daylight', 'wonblog', 'atorvastatin', 'adidas f5', 'barata person', 'bookmarked web', 'dissertation posting', 'pantys play', 'germany lesb', 'loveme.ro', 'chanel replique', 'вђќ', 'autentico cheap', 'este despesa', 'shoes2015', 'kors glass', 'article buzz', 'articles.thank', \"spilivanie derev'yev\", 'day loan', 'locatecellphone', 'monclerbarato', 'milano scarpe', 'zimovane', '.pl/1', 'jerseypro', 'discount.co', 'be benefited', 'queen chiffon', 'transpiration', 'beautyreport', 'boombeachhack', 'uggpas', '4purse', 'gucci time', 'viviennewood', 'shop to you', 'cassinos', 'xxx group', 'is now available', 'cigar', '1337seo', 'com/smf/ind', 'cheapnfl', 'visitor/day', 'black tight', 'camgirl', 'on line certif', 'designerbrand', 'ooo brand', 'gelvirage', 'golf shop', 'cassino game', 'scarpe supra', 'jassen dames', 'replicastore', 'buy traff', '2014 longchamp', 'trendypurse', '9.in', 'burberry belt', 'zero cost', '\\uf8f5﨨', 'lauren short', 'baratasnew', 'nicearticle', 'cheap jack', 'promo', 'mercurial2013', 'unknown.co', 'forum.co', 'jacobscartera', '+coach', 'settlement cash', 'hollister short', 'insanity journal', 'www.porn', '.su/3.', 'cheapest pokemon', 'jewelery', 'lottery kerala', 'forteparafon', 'mpnth', 'freeoffer', '%d1%8a', 'male enhance', 'ויקיפדיה', '3.za', 'hostingdeutsch', 'maillot bundes', 'celtics color', '@123', 'moncler cloth', 'elitewom', 'hack', 'longchampsa', 'saldi online', 'essayintro', 'jordansshoe', 'spade', 'cms theme', 'view blog', 'montblanc stylo', 'tiffanysale', 'passwordprefix', 'spiritual agent', 'favorable page', 'cr.e.ase', 'decent blog', 'camisasralph', 'megasena', 'damenschuh', 'actualcelebrity', 'hoverboard360', 'shate thou', 'hollister online', 'website style', 'buy hermes', '1@seo', 'onsale.cfm', 'enlightening website', 'iphone5ccase', 'the best', 'sexo mon', 'informasjon.php', 'asicsshoe', 'cheaphermes', '.vulpyx', 'brandwto', 'rich pickings', 'beatsbest', 'party poker', 'pornebony', 'othertype', 'beenpaid', 'iphone5s spy', 'sensual', 'copyscape', 'of tthe', 'pantys sex', 'soon:', 'shopent.su', '.cgi', 'bvlgari out', 'elite model', 'cheap sale', 'supraonline', 'foot nike', 'picloader', 'cosmetiqueseye', 'fantastic page', 'burberryhand', 'vuittonwhole', 'drmarten', 'latestblog', 'spadediaper', 'askjeeve', 'topviral', 'the latest', 'estate pro', '.com0', 'clockdial', 'zenysobuv', 'chanel faux', 'fuckyour', 'sexuais sem', 'authentic ugg', 'a5.jsp', 'coskobo', 'nobis yatesy', 'porn tickl', 'սa', 'myjke wysokocis', 'hand', 'fb gold', 'afl replica', 'insuring', 'nba apparel', 'med', 'a06.asp', '2cigar', 'store.bl', 'loans', 'louboutins pas', 'are added', 'hairloss blocker', 'naked sex', 'roulette', 'candy crash', 'lvdisc', 'jp sale', 'bookmark web', 'longchampout', 'sertraline', 'best forex', 'films x', '50折', '４０％', 'girlblog', 'adidas schuh', 'snoring mouth', 'facebookcash', 'commentabout', 'organizovana zlo', 'pornmodel', 'cheapest', 'vip.', 'occhiali ray', 'woowoohouse', 'schoene seite', 'shopnfl', 'bandeira política', 'adidasshop', 'lol i', 'entries', 'solde canad', 'for business advertis', 'injuries insur', 'ломают машин', 'porn.', 'store out', 'pornangel', 'спорт открытом', 'shirtsandtshirt', 'demo4', 'gifts', 'dolce gabbana.', 'superb blog', 'aall my', 'weblog ...', 'secretgenerous', 'cher prada', 'borseprada', 'scraper free', 'sitemap3', 'girlseblog', 'dubai princess', '.kapselfibrose.', 'baratos new', 'manchesterhotel', 'the blog,', '.ru#', 'é premium', 'how to sell', 'is.gd', 'swordtrade', 'com8.asp', 'espanaonline', 'brand jp', 'steam и', 'jersey 2', 'thespacious', 'goose homme', 'детские дискотек', '.pussys.', 'boots schweiz', 'hogan shop', 'yuuguu', 'coastva', 'fotka.', 'lauren cheap', 'escorts zone', 'blog layout', 'review.php', 'registration earn', 'panty boy', 'socialite.', 'lovely just', 'psori nano', 'oakleystraight', 'mcqueen club', 'adderall', 'montres femmes', 'wardrobe application', 'monclercloth', 'scam store', 'chaussure asic', 'couchey fr', 'online invest', 'duvetica', 'goose youth', 'men jers', 'hyundai.', 'seahawksjers', 'a6.php', 'scott wing', 'woolrichjohn', 'shoe men', 'consequently styl', 'gold pill', 'o‡', 'mbt panda', 'nikestore', 'gamblinggame', 'badcredit', 'coach men', '6.in', 'dessertsfactory', 'escort zone', 'inbox0', 'timberland out', 'datingcasual', 'outlet 1', 'healthcare advis', 'shoppes', 't.@', 'hoverboardshop', 'tienda barata', 'bitcoin donat', 'birkenstock online', 'send earn', 'zyprexa', 'pokie', 'equipment manufact', 'brasilbikini', 'ultra hoverboard', 'erotic text', 'zapewniaja bezpiecz', 'boutiques mbt', 'legit pharm', 'imitation cartier', 'sofatest', '40mg.', 'cheaps ugg', 'enprivate', 'bikinier udsalg', 'strona autora', '?shop', 'oakley active', 'bulletproof merced', 'фитнесс клуб', 'func.htm', '+fifa', 'fastidious blog', 'primeessay', 'hairloss block', 'cigarsonline', 'coast shift', 'medication', '1the best', 'extremely ample', 'code xbox', 'hermes scarf', 'sex.porn', 'ꭼm', 'gry online', 'golf out', 'spy software', 'bу', 'healthstock', 'italystore', 'positively useful', 'timberland earth', 'konsultan', 'internal', 'swarovski', 'adidas orig', 'milano jp', 'shoes.asp', 'thhe best', 'moncler piumino', 'mybusiness', 'online schuh', 'discount mulberry', 'panier site', 'fake passport', 'specialty transfer', 'uch:)', 'aspiring blog', 'all incluzive', 'thesis publishing', 'pitburner', 'edge rumor', 'schuhe', '&x7', 'camisahollis', '222.ro', 'principallongchamp', 'sáo trúc', 'shoe cloth', 'boots oxford', 'longchamphobo', 'celebrities nude', 'buy lol', 'bag.sh', 'product sale', 'montaj kanali', 'features', 'ghd pascher', 'uggs elsey', 'designershoe', 'nailsjap', 'iphonecase', 'yo penis', 'actual effort', 'pharmo', 'superb site', 'oakleytokyo', 'botte femme', 'коррупци', 'www.gambl', '.za/content', 'a‡', 'clearclear', '=com', 'men gaga', 'shoes jp', 'basket mbt', 'fuck pic', 'com/log', 'poker strateg', 'cholesterol', 'байкал отдых', 'casino.', 'lululemon', ',in', 'utilized use', 'phone advert', 'marketingblog', 'zboard.htm', 'alluring escort', 'viralstorie', 'invest.pl', 'oakley cross', 'coachfactor', 'zapatillasjord', '００歳', 'brrip', 'please click', '야동', 'laurenuk', 'g00gle', 'boots sale', 'fresh review', 'rayban', 'longchamp doctor', 'promotionshop', 'oherhand', 'viagera', 'boot queen', 'sac a main', 'green smoker', 'fresh seo', 'perfect interest', 'where get', 'replica design', 'chinadress', 'texans jers', 'uggs sale', '.pl/profil', 'quite photo', 'sunglass cheap', 'bioactives', 's.all', 'start.', 'новейшую дам', 'outlets.net', 'ban aviat', 'wp list', 'emails vip', 'nice annd', 'oakley out', 'promo sys', 'volume muscular', 'ralphlauren.', 'ebook reader test', 'video.asp', 'virus infect', 'mulberry bag', 'offerte calvin', 'tattoos sneak', 'that isnt', 'cosmetic eye', 'enchere max', 'robe du mariage', 'dating direct', 'meble gabinetowe', 'cannabis grow', 'bootonline', 'cpa traff', 'only deal', 'coach hand', 'pucciout', 'femme', 'kopejassen', 'advertising market', 'myspace', '7.in', '||', 'reborn site', '百家乐', 'are desirous', 'uggs', 'คาสิโน', 'louboutin wedding', 'phone free', 'b.u.y', 'bestxxx', 'featuring christs', 'x adidas', 'nail_art', 'reach me down', 'arcteryxjap', 'hacker', 'longchamp 2014', 'zealous of', 'porn comic', 'porn serch', 'coupons.', 'belt replica', 'xyz專', 'winter jassen', 'bdsm sex', 'fantastic site', 'easynaked', 'laurenhome', 'sehmale sex', 'iphone 6', 'peopleand', 'ppv click', 'recipes name', 'firstclass web', 'bootss', 'drmartenjp', 'схема лечения', 'adbeat distrib', 'wholesale cheap', 'ageless female', 'jacket 2014', 'ugg=', 'secret advant', 'amorti dynamique', 'за таблетку', 'gucci sale', 'seekman', '.iapple.', '>.}', 'junior kid', 'charlescave', 'models.co', 'ka.t.hle.en', 'internet link', 'hack generator', '100% plagiaris', 'article dude', 'purse.', 'barato ropa', 'kors runway', 'cheap red', 'professional 2015', 'lionsjers', 'dealongamento', 's.p.o.rt', 'клинические рекомен', 'хочется уз', 'vardenafil', 'virushoax', 'drug cheap', 'su0.ru', 'pantie.', 'dresshop', 'actualsubmit', 'panties child', 'favorable blog', 'porn mobile', 'zenyobuv', 'cigarette<', 'salomon.cfm', 'vapor ix', 'стили деко', 'велпатасвир', 'yoour', 'moncler piumini', 'kabriolet', 'vidéo porn', 'cigonline', 'capabilities also', 'part mariage', 'penis adv', 'versace belg', 'kors vancouver', 'forum.za', 'iphone.htm', 'remarkableunderstand', 'hartmann repeated', 'creditrepair', 'palavras chave:', 'purposeful wonder', '块钱', 'dumps seller', 'temptes.t', 'dunjakke', 'onlinebuy', 'cheat web', 'marketing advertising', 'everythink', 'базу данны', 'marlboro gold', 'encherisseur', 'headset 2014', 'sо', '+lacoste', 'fashiontrend', 'homme couche', 'vapourix', 'men sneaker', 'searchporn', 'jackets 2013', 'hats denver', 'trendy bag', '4 sale', 'rugsfur', 'jewelry/watch', 'mbtgun', '?l2', 'elway jers', 'kjole udsalg', 'functions', 'sk8 hi', 'gold unobtain', 'abercrombiefitch', 'doorblog.', '3 generic', 'montazh/montazh', 'medshop', 'goyard bag', 'r http', 'factory coach', 'numerousnumer', 'websitestyle', 'kapselfibrose', 'www.zapato', 'bluefin trad', 'prezzibors', 'china shop', 'loteria pro', 'server 2015', 'dental quote', 'download gratis', 'paris model', '365.za', 'google gain', 'trempe sous', 'webmaster', 'forumcoach', '%d0%ba', 'investoff', 'offerta occhia', 'oral sex', \"cann't imagine\", 'masterclass', 'финансового информ', 'training online', 'get money for', 'magasin longchamp', 'goose femme', '００％', 'preferences 2', 'redirector.jsp', 'remarkable article', 'straightface', 'onlineeurax', 'uggssilver', 'cruise vaca', 'post serv', 'googlebind', 'crave of', 'ламинин', 'hoteli', '.picload.', 'betes5', 'here different', 'server 2009', 'urlacherjers', 'poker3', 'expert suggestion', 'keywords.txt', 'cagoose jack', 'チャンルー', 'upload', 'sexy moment', 'tetas mega', 'celineshop', 'xzzy.su', 'bookmarked', 'homme mariage', 'buy cig', 'bloghome', 'barntimber', 'sacs celine', 'gucci gucci', 'review', 'аренда свето', 'insuranceauto', 'veterinarnaya', 'roger vivier', 'content material', 'kor out', 'pantys boy', 'subscription now!', 'dedrease', 'ladda ner', 'movie/?', 'bottesfemme', 'monsterjp', 'aaa.', 'seo source', 'gift click', 'uk sale', 'couplesgratuit', 'acnecyst', 'web website', 'generating money', 'es.iodress', 'factthat', 'splitting announce', 'installment loan', 'commentsyou', 'variant3', 'supplement.', 'woolrich cloth', 'whorebutt', 'vape', 'cutt.us', 'knee joint', 'online bag', 'good blog', 'ugg hot', 'timelines hack', 'reviewsbest', 'btc wallet', '7 easy way', 'yօ', 'kino winterthur', '.in/1.', 'impressive share', 'toeshoes.co', 'sex', 'hotlove', 'moncler giub', '+jers', 'gucci uomo', 'paid google', 'ka.thl.een', '14.htm', 'suplementy', 'search porn', '?b', 'sneakerchef', 'rastreadores', 'asses.', 'forex valu', 'pradabag', 'longchampdoctor', 'style mbt', 'sactrail', 'hellodress', 'internetowe', 'catalogues.php', 'goose grise', 'malware.', 'bvlgari brand', 'jersey cheap', 'millencolour', 'ipodsuppl', 'excellent written', 'video magnif', 'py.th.on', 'bobet link', 'chaussuresmbt', 'hack pass', 'software secret', 'zapatilla', 'apple wedding', 'birkin cheap', 'monster beat', 'vagina online', 'wowosite', 'farming secret', 'awesome weblog', 'guccinose', 'lohan porn', 'yslbag', 'bags online', 'myresume', 'montblancfranc', 'muscular abdomen', '.za/1.', 'drmartens uk', 'established blog', 'lasart.es', 'sexualfantas', 'financial blog', 'great writ', 'uggbrown', 'bancshare', 'ventolin inhale', 'moncler 2013', 'pnc speed', 'reirect', 'antivirussen', 'brand4', 'for u.', 'goosecalgary', 'kat.h.le.en', 'hemp milk', 'admin5', 'webnode.cz', 'kussinhartmann', 'hookup7', 'newwebsite', 'ninja sword', 'legal', 'marketing prof', 'sexy women', 'muzimodry', 'bankowoz', 'ing/bak', '.onesmablog.', 'virtual sex', 'coachsuto', 'mcm london', 'fakewatch', 'duchess satin', 'url fx', 'maillot foot', 'didrex online', 'timberland bambi', 'bcbgcasual', 'birkenstock sale', 'fresh blog', 'munch measure', '3the best', 'foreclosure', 'nike blazer', '03.jsp', 'ντετέκτιβ', 'cheap atlanta', 'excellent post', 'dbase', 'cool t shirt', 'bulgari bulgari', 'loyalty today', 'financialdebt', 'mlm lead', 'site offer', 'аt', '輸入', 'origamilesson origami', 'progs.in', 'prescriptionacne', '9.su', 'jersey soccer', 'coach rouge', 'gooseonline', '.com/2', 'sneakers trade', 'schuh', 'newoakley', 'privat label', 'in gogle', 'prescript.asp', 'lunettes.', 'incredible page', '2013 longchamp', 't.witt.', 'sadehap', 'blazer chaus', 'upgradekey', 'manteau karen', 'free bid', 'methode pour', 'infected crash', 'free visit', 'scontati', 'com/publi', 'eroticerotic', 'amateure xx', 'boots uk', 'now.in', 'backup', 'energetic post', 'panties mania', 'rugfur', 'autentica hermes', 'calzaturembt', '+swaro', 'dental veneer', 'world capital advis', '.roulette', 'nba.htm', 'cannabis buy', 'cheap afl', 'saclouis', 'sabosale', 'link seo', 'puma ferrari', 'areshade', 'siege hack', 'casinoonly', 'image/layout', 'fitch out', 'xrumer', 'page/view', 'bvlgari shop', 'foreclosed', 'sexvirgin', 'img2', 'bookmarked :)', '.top/gal', 'uг', 'bluetooth head', 'f_1', 'adcrack', 'choice top', 'newhong', 'viral vid', 'letter template.cfm', 'рф', 'lebron', 'strategic advertising', 'crush candy', 'ワンランク上の', 'chik cassino', 'myyspace', 'stivali pioggia', 'control diet', '2 easy way', 'hoganshop', 'css', '100mg.', 'knowledge training', 'スワロフスキー', 'tiki index', 'formoney', 'voyancegratuit', 'com/doc', 'hit mp3', 'лечение зависимости', 'healthhow', 'comments/north', 'monday ugg', 'online store', 'monclerman', 'excellent task', 'readyto', 'giubbino moncler', 'nikemercurial', 'ugg uk', 'difficulties thus', 'porno girl', 'hatssnap', 'helpful method', 'change your life', 'fashion store', 'read smaler', 'недорогой отель', 'replica birk', 'euille lancel', 'sucette', 'longchamp', 'b7.jsp', 'iniciar produzindo', 'girls blog', 'losers outnumbered', 'fetishxxx', 'kontakt', 'laurent femme', 'klein mujer', 'moncler france', '履', 'adipex', 'bvlgari store', 'uggs wom', 'kartridzhej', 'taschen longchamp', 'cheaper essay', 'celine bag', 'topbulgari', 'yoga out', 'mcmpurse', 'hollister jp', 'androidmp3', 'wholesale copy', 'chandler baseball', 'marketonline', 'instalmentloan', 'mare steady', 'shanghai escort', 'psychclinic', 'куча видео', 'salvia.', 'mbt shoe', 'nike total', 'drug.', 'bisapp', '8the best', 'informativewebsite', '&#12509;&#12540;&#12523;&#12473;&#12511;&#12473;', 'config.', 'sigareta', 'faux homme', 'xxx pic', 'airport', 'my wweb', 'diazepam', 'kleidungjack', 'sweetblog', 'zinger bat', 'for gucci', 'seo barn', 'discussion made', 'エスパドリーユ', '3hand', 'negozi milan', 'wedding jewel', 'di droga', 'jibajabs', 'coach tokyo', 'free laranita', 'michael kors', 'kat.hl.e.en', 'get widget', 'значительный сайт', 'blue_pill', 'p.@', 'fanatic shop', 'jerseys+', 'germany jers', 'gold celebrit', 'qhttp', 'whatsblog', 'groomingneed', 'top tier', '.ru/1', 'khám nam', 'docteur dre', 'businessnetwork', 'diet pill', 'make backup', 'buy gig', 'luisvitton', 'ownblog', 'ce billet', 'schlgsseldienste', 'free ipad', 'goyard online', 'replica top', 'boutiquebalenciaga', 'makecsgo', 'web users;', '.blog5', 'dating service', 'metode ociepl', 'clash hack', 'site position', 'wallpaper erotic', 'mirror femme', 'menthol dunhill', 'certainly pronoun', 'order parafon', 'b08.asp', 'legitimate script', 'channels obtain', 'blogs', 'panty', 'replica stella', '.designertoblog.', 'xxx sex', '5the best', '.ro/2.', 'nurkowaniemarsa', 'empire hack', 'mbtsko', 'месяц назад', 'success academy', 'ingenieurs rni', 'restoril', 'farther high', '8.su', 'goose ital', 'дизайн дизайн', 'com4.htm', 'www.videos', 'longchamp planet', 'high website', 'meisterstuckpen', 'ꭼp', 'nos molestó', 'particular article', 'ugg paris', 'article much', 'greatkeep', 'odszkodowania', 'big.naturals', 'demo1', 'pokegocheat', 'jogar loteria', 'augmenting', 'very}', 'bailey ugg', 'blog post', 'quickfast', 'outnumbered loser', 'costume ermen', 'kors+', 'bijouxbest', 'cigs', 'shoeonline', 'camiseta nfl', 'alive cheat', 'studylook', 'perhaps tutorial', '%d0%bd', 'phexin', 'borsa prada', 'belstaff chaqueta', 'forums.pl', 'a07.php', '?top', 'bulgari shop', 'borsaout', 'payday.co', 'japan dr', 'codes promo', 'page1.', 'couchsofa', 'powfleul', 'link market', 'discount nhl', '.del pene', 'penis.pl', 'large longchamp', 'cassinoenligne', 'dolcegabbana.', '.ru/image', 'invest+', 'firm.su', 'guarantee', 'rebecka charger', 'cv meaning', 'windows7key', 'tiendabarata', 'scam.net', '1.@', 'occhialiray', 'cigarettes', 'ingugg', 'louboutinshoe', 'toryburchten', 'reviews.asp', 'chaussrespascher', 'uomo man', 'pdf/bv', 'saldifootwear', 'vinho braz', 'image/rolex', 'xxx vid', '99club', 'chanel paris', 'boottenngoku', 'shoes announce', 'pro medical', '2016longchamp', 'jerseysforyou', 'express index', 'timberland men', 'borsaceline', 'ecom success', 'fb cash', 'sacs hermes', 'ɦe', 'camisetas nba', 'remarkable page', '9cigar', '2017 seo', 'магаз', 'bally store', 'balanceespa', '.games.su', 'medrol24mg', 'ontheir', '00 pips', 'dress herve', '?seo', 'hollister paris', 'page/pag', 'nice website', 'articles.php', '.su/ba', 'web pharm', 'inbox6', 'both educative', 'significantly post', 'hermesbuckle', 'replica ugg', 'that', 'casual sex', 'burberry factor', 'businessdirectory.', 'poznan', '博彩', 'cabasvanessa', 'web farm', 'credit direct', 'porn lesb', 'web anal', 'find sex', 'videosport', '.mihanblog.', 'articlesdude', 'moneyrobot', 'awesome blog', 'piuminomoncler', 'giubbotto dsqu', 'salomon canada', 'm88ui', 'torgovye strategii', 'guantes marshall', 'вырубка деревьев', 'damenvonschuh', 'gay sex', 'hardy jean', 'posts.pw', 'holdinga', 'hyroliheze.tk', 'posts!', 'singapore.php', 'appnana mod', '13.cfm', 'article.co', 'языка алматы', 'tods jap', '911.co', 'goose paris', 'buy jap', 'iphone5scase', 'scarpe roger', 'jordan grise', '.redirecter.', 'videosvue', 'admin.htm', '網站', 'professional2009', 'premium cig', 'generator 2016', 'gull minnow.gull minnow', 'timberlandearth', '2.5mg', 'documents/bv', 'pantysex', 'ru.', 'cilios online', 'dress link', 'eng private', '4juice', 'sensual massage', 'websites visit', 'write my article', 'mustlook', 'wayfarer', 'longchamptourne', 'hjblog', 'c.ar.ol', 'web invent:', 'ganadora clásica', '.ro/catalog', 'moviezone', 'key prog', 'viewblog', '.%', 'soma.', 'date netflix', 'girliespouse', 'buygold', 'iphone4me', '111.za', '123.in', 'amateur', '24hourwrist', 'accident attorney', 'thread', 'pantie boy', 'hobo bag', 'encrypted', 'caserole plastic', 'bolsosmarc', 'daikly', 'chaussures de', 'money phone', 'mypage', 'rayban lune', 'freesalg', 'thoughtl', 'sabo out', 'pip hunter', 'purse online', 'lebron shoe', '+sjs', 'profissional opinião', 'smarthoverboard', 'b7.asp', 'ranking:', 'advice.adult', 'topamax', '222.co', 'hdporn', 'yu move', 'betweenex', 'betes8', 'fashion boot', 'maillot', 'shop for.', 'ã¥', 'wirh you', 'blogs/item', 'desire erotic', 'warez', 'freelance buyer', 'kors puffer', 'montblanc kuge', 'coach factor', 'northface sale', '3d моделе', 'bvlgari jap', 'kristilongchamp', 'enhancepill', 'longchamp2017', 'sito officiel', '15.jsp', 'рассылки сайт', 'fucks hard', 'i needs', 'новость убила', 'herentimber', 'premium key', 'raybans', 'segboard', 'stratificat.', '香水', 'programy.', 'amount food', 'chanelpurse', 'burberry in', 'com_install', 'dating casual', 'damenkaufen', 'comming from', 'vietthanhbien', 'cuirmean', 'лечить наркомани', 'newsletter serv', '.net/%', 'big replica', 'csgo free', 'hd porn', 'kreatif membuat', 'linkbait', 'replica leger', 'htmllink', 'date class', 'retailstore', 'eroticke', 'mužibílý', '4u.su', 'voyance amour', 'chanel imitation', 'sale louis', 'c|pro', 'sac chloe', 'bvlgari', 'xgmasa', 'ύo', 'gomi bet', 'скидки отели', 'rozwod', 'clothesbag', 'youtvplay', '.freeoda.co', 'usefuyl', 'pc windows', 'gamescasino', 'brazil bikini', 'com/user', '18xbox', 'zeny obuv', 'f.j.o.g.a', 'broncoshat', 'scam.htm', 'neat post', 'personal blogroll', 'coin offering', '?kat', 'app.php', 'costumemari', 'directhealth', 'somedias', 'assjob', 'spammerlink', 'achat medica', 'nanas hack', 'douchingteen', 'chaussureadidas', 'dostinex', 'mlm business', 'tabak fumar', 'mensbathrobe', 'mega pezone', '222.za', 'post.best', 'uggsneder', 'sunchannel', 'wanna state', 'home loan', 'celineboutique', 'mar skin', '2minute', 'porno live', 'rescator', ':)!', 'mcqueendress', 'remedio caseiro', 'sex teen', 'shirtandtshirt', 'basketball word', 'hoverboard for', 'oakleymedusa', 'pain relief.', 'cardy boot', 'linknum', 'gucci seller', 'www.foto', 'birkenstocksandal', '+scarpe', 'labetalol', 'результаты лечени', 'possess', 'alcohol rehab', 'handbags out', 'schuh günstig', 'supercompan', 'sluts porn', 'pantychild', 'zapatadembt', 'uggs paris', '?ducati', 'zlinks', 'zimmerman hedge', 'atonemen’s tip', 'heil hitler', 'roshe run', 'bookmarks web', '.ro/?entry.', 'dalle scarpe', 'kugelbahn', 'fastidious, my', 'jersey whole', 'wholesalefree', 'probesaloka', 'chanelbag', 'ji.ml.ard', 'blogviewer', 'credit', '.pl/2.', 'test1.', 'we vigorous', 'sweet page', 'giubbotti invernali', 'wiith', 'anfangsposition', 'nfl nfl', 'homessmart', 'marijuana bene', '.su/vip', 'dream proxi', 'monclerquincy', 'puma deutsch', 'splendid depart', 'va ballgown', 'cheaptoms', 'slugy stekol', 'obuv seda', 'pips hunter', 'loubutin', 'voltaren', 'sexyfantas', '.hello people.', 'mbtunono', 'ban lage', 'neatly favor', 'nhlshop', 'appnanacydia', 'com3.cfm', 'discountmbt', 'with thhe', 'biz news', 'writing person', 'site theme', 'stalker rp', 'the shock', 'pagina web', '+gucci', 'fjog.a', 'custom jersey', 'kabriolet.', 'millenout', 'blogsoon', 'еt', '＄２０', 'afl shop', 'shoes america', 'yoursuccess', 'moncler ski', 'xxxtube', 'ebookreadertest', 'cheap wed', 'menspuma', 'перманентный макияж', 'gucchi', 'kathl.e.en', 'moncler jura', 'craiglist', 'cheapjord', 'marvelous post', 'p.ytho.n', 'blog<', 'model.in', 'wholesalepolo', 'pleasant vehicle', 'ugg mocha', 'forzest', 'vuitton out', 'facesale', 'domination seo', 'aaabbb', 'it oakley', 'big smart', 'anus live', 'no hassle', 'giubbottiwoolrich', 'china.', 'get_started', 'wang bag', 'giubbotti dsqu', '.tatuagen', 'data tools', 'porn model', 'decent site', '2.cfm', 'crossof', 'dsquared online', 'signifiant', 'twerkvid', 'cintureout', '+title', '.retina247', '.sg/propert', 'proper ugg', 'teeniepant', 'poker machine', 'professional 2009', 'avto', 'stodios beat', 'unlimited autopost', 'lotterypro', 'jim.lar.d', 'teen.sex', 'direct guideline;', 'welcometonginx', 'you ssay', 'ご了承', 'lol thanks', '+oakley', 'hollisteronline', 'friday2013', 'telepon', 'enormous post', 'cheap stephen', 'topsearch', 'wallpapers erotic', 'onlines.co', 'article.jsp', 'frozencloth', '4less', 'dvd asphyx', 'goedkoop', 'großen ray', 'helpful blog', 'gluten', 'casinohack', 'firma.jsp', 'новостного сайт', 'test.ca', 'thanh nhac', 'сайт рассылки', 'bcbg sleeve', 'seosource', 'hottest update', 'stivali timber', 'timberland schoene', 'телефона узнать', '2.0 mg', 'lesbian milf', 'build', 'アバクロ', 'propranolol', 'casinosite', 'it ordeno', 'stockscreener', 'mediawiki', 'korsstraw', 'com.com', 'bag jp', 'autenticougg', 'k.a.t.hleen', '+lebron', 'talentmanage', 'cheapjack', 'iphone5 spy', 'producing provider', 'ugg brown', 'ok9.', '2014 popular', 'ads', 'my web site', 'ysl damen', 'soleil ray', 'stiefel schweiz', 'online med', 'uggstyle', '1 generic', '4 the best', 'yoigucci', 'ωi', 'okfreeshipping', 'koop dsqu', '![]', 'sabocharm', 'sauvegarde extern', 'zawieszki', 'cheapgold', 'co jap', 'empower network', 'macmakeup', 'lenjerie', 'outletusa', 'uggsilver', 'raybanj', 'dsquared jean', 'neat article', 'sitemap9', 'celine luggage', 'help.php', 'coach niho', 'celine', 'gta online', '医師', 'brittny battle', 'ban online', 'cheapest hockey', 'coresex', 'asian lesbian', 'ルイヴィトン', 'fav to', 'mariage orig', 'nhl.asp', 'package printing', 'plansare', 'свой сайт', 'mlbmlb', 'org/log', 'extremley', 'lifestyle video', 'kat.h.l.e.en', 'buy gold', 'pliage shop', 'prada', '4 purse', 'online gambl', 'memberlist.cfm', 'wallpapers nude', 'marketing blog', 'tattoos', 'inbox3', 'brand engage', '.bisnis', 'german love', 'джекпот лотерея', 'www.xx', 'brilliant blog', 'cassino bonus', 'hacks1', 'opt in promotion', '.ru/serv', 'menboost', 'mercurial 2016', 'peso saudavel', 'plagorism', 'cheap proclip', 'onlineout', 'b06.cfm', 'монтаж элект', 'website visit', 'sales14@', 'value comment', 'nocredit', 'gallery world', 'mallstore', 'impoirtant', 'gucci factor', 'goodgame', '0.jsp', 'medical marijuana', 'longchamp purse', 'guestgoo', '+ugg', 'magazini', 'htaccrss', 'adwokacka', 'ugg.jsp', 'soldes canad', 'wholesalemac', 'dɑ', '9 generic', 'zapatilladc', 'laurenralph', '123 movie', 'media marketing', '5 easy way', 'secondgrade', 'soft secret', 'secretadvant', 'ύa', 'sac.', 'catalogs.htm', 'articlespost', 'gangprofil', 'tiffany gemstone', 'von ysl', 'birkenstocks.', 'louboutin uk', 'wantbitcoin', 'our link', 'thing thats', 'girl blog', 'scar repair', 'offering free', 'maxazria', 'hollister pant', 'gucci italia', 'sex pc', 'detophyll.', 'b0.cfm', 'index.htm', 'adsplus', 'com2.asp', 'video buzz', 'pandorauk', 'arabgirl', 'onlinegambl', 'remarkableinfo', 'plus date', 'grafica preço', 'ダコタ', 'salomonfr', 'cassino', 'marketing serv', 'articles.ro', 'jersey.php', 'vzloma zaparolennogo', 'san pham', 'tɦ', 'dunhill fine', '~1', 'выбрать лучшие', 'panties play', 'cappelli new', 'goshop.', 'euro million', 'asian.sex', 'кормлениигрудь', 'outlet cheap', '1.ro', 'hardyhuppari', 'preference 6', 'essay empire', 'y http', 'сµ', 'jordan 4', 'webcam site', 'rayban cheap', 'article', 'shoes uk', 'sachermes', 'marketinghero', 'wypoczynku', 'watchesquartz', 'legitimate drug', 'furlaout', 'vuitton usa', 'loan fast', '+uptime', 'dumps cheap', 'raahe3', 'porn girl', 'bruiseviolet', 'favorite justif', 'organic seo', 'seo wise', 'monsterearphone', 'quality content', 'gowns love', 'managed dedicated', 'chaussuresadidas', 'forexpro', 'bangbros', 'excellentweblog', 'beatscustom', 'images/chanel', 'ghd gold', 'lv france', 'obuv ženy', 'pornopop', 'replica ray', 'your sijte', 'dameskopen', 'moncler man', 'adulttoy', 'seaddons', 'free pron', 'nakedsex', 'reviews.htm', 'kvartiry', 'videoporn', 'server 2012', 'health stock', 'tamoxifen', 'rayban schwar', 'financing.', 'hello sex', 'sitoufficial', 'ass lick', '%d1%86', 'jordanretro', 'scarpeprada', 'ต่างหู', '4u.', 'prettyworth', 'francemaillot', 'стилист проспект', 'skque', 'mbt clear', 'insurances', 'java', 'cig promo', 'alarmingly grow', 'lv bras', 'mastery discover', 'hermesstore', 'urlsfx', 'blog loading', 'heuer sale', 'tittiepic', 'bioactive', '00.php', 'diet.jsp', 'insaneworkout', 'ampedpage', 'enter', 'tо', '20mg cheap', 'viral storie', 'noreferer', 'document/chanel', 'lawoffice.net', 'рµ', 'bielizna', 'greatwrit', 'sex movie', 'sex vid', 'datingservice', 'www.adults', 'cheats.co', 'cashgenerat', 'ufficiale moncler', '.ru/,', 'rusex', 'extensive internet', 'tthe other', 'giubbottoout', 'wholesalejersey', '.com/3.', 'günstig kaufen', 'reborn post', '2bj.ru', 'page4.', 'cabins lithu', '?p', 'documents/new', 'fastidious site', 'moncler men', 'your account', 'how to win', 'truthabout', 'your ugg', 'gratuite roulette', 'property sales', 'greatpublish', '.sensual.', 'parrainage banque', '1.5 mg', 'getridof', 'known blog', 'businessboss', 'japanese dr', 'clothes bag', 'woolrichesite', 'site link', 'kors sale', 'japanmarcjacobs', 'runningsneak', 'engine', 'internetsavvy', 'captain rank', 'car requirement', 'jerseys 3', 'hoverboardking', 'sale template', 'stivale timber', 'jewelrycollect', 'delivers result', 'loved onein', 'скачать', 'king hack', 'rayban aviat', 'blahnik out', '2017 longchamp', 'messages erotic', 'mlb.asp', 'queenout', 'for adult', 'article content', 'discounted wheel', 'del pene.', '4ever', 'chaussures paris', 'quality quartz', 'good respond', 'pokemoncheat', 'iin many', 'supplements energy', 'preparation wise', 'component', 'needbitcoin', '.za/serv', 'neat blog', '.ashematme.', 'domain123', 'uggfor', '! .', 'wikka.jsp', 'email.jsp', 'dre phone', 'promotional shop', 'uniquebroker', 'bengalsmerch', 'a1.htm', 'rayban groben', 'wholesale chinese', 'addiction xxx', 'doctermarten', 'full movie', 'paulsmith201', 'pennyauction', 'coach out', 'condotel tai', 'trà đạo quán', 'costume  aged', 'about marijuana', 'bagsout', 'cheapmbt', '.armpit.', 'wallet out', 'pantys.', 'kobe', 'commerce money', 'progs.ru', '.kartridj.', 'manicshop', 'asp?folder', 'mit rezept', 'your publish', 'beat gainer', 'bulgari replica', '.mrwhite.', 'secret', '|det', 'hoverboard shop', 'b2.php', 'биткоин', 'cpa ppv', 'outletfrance', 'funds.in', 'pin dump', 'goose calgary', 'カルティエは', 'barataperson', 'ban price', 'dre cheap', 'allso visit', '3you', 'instant week', 'buddie', 'a1.cfm', 'shemael movie', 'wholesale led', 'movementpart', 'london genuine', 'fuck your', 'critically article', 'prokey', 'awesome page', 'de mbt', 'parkiety', 'babycanread', 'arise balls', 'articles,i', 'cheatss', 'mastermindteam', \"j'ai tjrs\", 'a?a', 'androidclan', 'clrb', 'site ufficial', 'ropa interior', 'display', 'eyeglass sale', 'createown', 'dentalimplant', '荷物', 'uuse', 'пенное вечери', 'ad_flagship', 'online longbow', 'приобрести недви', 'ferragamo.', '40折', 'galaxy s7 s7', 'skool shoe', 'airmax', 'suplemento natural', '%d0%b1', 'handbags store', 'boy pantie', 'zfymail', '%d1%83', 'монтаж ауп', 'uefa fifa', 'successfularticle', 'blog world', '=brace', 'lancelpas', 'hollister swim', 'bots3.', 'партнёрские програм', 'investment tips', 'barbour cloth', 'самое свежее', 'buy soma', 'of internet', 'quelques autre', 'cheap hotel', '0,5 mg', '攜心山', 'sac lancel', 'morre about', 'nfl shop', 'nude.cfm', 'blackjackpit', 'acne', 'lɑ', 'link issue', 'hollisterpolo', 'iphone suppl', 'suhagra 100', 'lancel sac', 'outlet moncler', 'sale factor', 'evvery', 'registry repair', 'online espa', 'fortnite free', 'hoststo.ru', 'cheap gold', '.za/page', 'phishcasino', 'trailer sale', 'pokemon amulet', 'kanadakommen', 'images old', 'cosmetics eye', 'b1.asp', 'coachtokyo', '[a..z]', 'kledingwinkel', 'hollister milan', 'kpkfprbrq', 'genemy', 'sildenafil', 'site ', 'shop for', '404.htm', 'jerseys for', '金引換', 'zapatasmbt', 'bitcoincas', 'zapatasdc', 'free private', 'plastic caserole', 'uggs neder', 'ugg botte', 'celinebag', 'superiorcompan', 'b6.htm', 'fixing credit', 'detail/www', 'bow ugg', 'enceinte rapid', 'pg/blog', 'cabas vanessa', 'バンズ', 'fruitful design', 'bagjap', 'filvce.in', 'campaigns.', 'gemmes illimit', 'casino only', 'weredouble', 'links exchange', 'fit website', 'jackets 2014', 'bio skin', 'оt', '04.asp', 'website particular', 'chaussure mbt', 'eь', 'chung hightop', 'cultureparis', 'costume versace', 'ashematme.', 'registryclean', 'tiendasde', 'thinking playing', 'mister design', 'seo comp', 'last looker', '.webgarden.cz', 'learnhelp', 'a04.php', 'sawfwaf', 'isto blog', 'merely wanna', 'image/game', 'wrinkle lotion', 'speedproxy', 'heook', '.noreferer.', 'throughout simply', 'newarticle', 'gratis coin', 'costume medi', 'dietsolution', 'instructcar', 'jacobs purse', 'cryptocurreny', 'perfect diet', 'posts manually', 'goose', 'chrr llc', 'list erotic', 'incredibleblog', 'curcumin capsule', 'meinem profil', 'coins', 'огромный выи', '.nikeshox', 'ismavailable', '口コミ', 'hd erotic', '2.za', 'timberland cheap', 'compiltaions of', 'coins xbox', 'downjack', 'fatburn', 'seo god', 'подешевле', 'best direct', 'howtobuy', 'virus', 'payday.za', 'ageless women', 'myanmartour', 'businessknow', 'getting knowledge', 'pharmacy 24', 'drugs buy', 'site proprietor', 'catalog.cfm', 'moncler femme', 'promo bag', '+hollis', 'article.ru', 'sac sold', 'versace uk', 'air max', 'shoes dior', 'のブックマーク', 'women', 'gucciyu', 'nike sb', 'optimization enhance', 'schuhgünstig', 'signout widget', 'board sale', 'gamejers', 'shoejp', 'flirt fever', 'autentica', 'shoxturbo', 'sneakersretail', 'datarecoveryhospital', 'false passport', 'knigki', 'internetview', 'luck jers', 'buy canadagoose', 'origami origami', 'order forte', 'in’ tremendous', 'hollister jap', 'girls escort', 'japan mont', 'vti', 'legal cash', 'affliction offer', 'справкой', 'propertypro', 'much important', '¥°', 'nouvelles sneaker', 'cannabis seed', 'eе', 'subscriptionlink', 'add.', 'ativan', 'guru1', 'zapatillas mbt', 'ppc program', 'bulk computer', 'penis forstor', 'soldesloubou', 'gown love', 'clothing online', 'votre site', 'hotsale', 'cartier replica', 'marinegay', 'boutiques chaus', 'ugg ad', 'ukonline', 'baykalozero', 'watchreplica', 'cheap salv', 'fund market', 'giants jers', 'глубокое глот', 'cyprus payment', 'porn search', 'ブランド', 'paydayon', 'drmartens', 'vuitton bag', 'china whole', 'jotasyde', 'jackets2013', 'phonefree', 'spam respon', 'every wearer', 'cosmeticwhole', 'temp', 'bitcoin casino', 'riot points', 'celebritynude', 'forums.su', 'tattoo', 'forexvalu', 'payroll calc', 'jacket', 'valueble', 'plugin', 'ru proshop', 'more smartly', 'oρ', 'dress shop', 'уa', 'assortiment', 'coachbest', 'salvatore outlet', 'ipad tablet', 'casino bonus', 'helpoful', 'online', 'низкие цен', 'melhor preco', '365.ro', 'public fuck', 'похудение живота', 'gold and silver', 'vuittonfashion', 'online pokie', 'botasmujere', 'puzzle band', 'shoe jp', 'better serv', 'studio beat', 'equipement manufact', 'cartier bracelet', 'istnieje oficjalna', 'autenticahermes', 'thkuafnl', 'bonus casino', 'readnews.', 'open.', 'kors grayson', 'burberry thai', 'b5.php', 'shoes on sale', '.filosov.', 'very hypo', 'автотерморегулятор', 'просыпаются филосо', 'bluecaps turbo', 'paulsmith shop', '.rekla', 'ðµ', 'video erotic', 'announce suggest', 'nhl home', 'dewelop', 'sledge bat', 'a05.htm', 'coach mise', 'moneygenerat', ', annd', 'курс евро', 'wholesale jord', 'plugins', 'howtospeed', 'catalogue.asp', 'finance', 'handbagout', 'жизненные проблем', 'hats', 'zolpidem', '&#12524;&#12503;&#12522;&#12459;', 'sale lulu', 'bague bvlgari', 'tods jp', '..a', 'vip девочк', 'witgh', 'cigars.', 'genuinely fruit', 'hermesbag', 'vuittonpas', 'guaranteed uptime', 'b8.php', 'moncler 2016', 'to truly', 'citalopramm', 'louis vuittone', 'reeview', 'officialsteeler', 'psychedelicparent', 'nike jers', 'linkissue', 'agen bola', 'kors now!', 'party boat', 'eâcute', 'frozen cloth', 'puma schuh', '0.ro', 'usually relative', 'shoppingsite', '\\\\u003e\\\\u003e', 'china scarf', 'tumi tumi', 'ugg wom', 'hack zip', '线上销售', 'googoozuza', '20mgcheap', 'clansipad', 'news.in', 'value page', 'merely extremely', 'гадание online', 'features christs', 'реальные деньги', 'diamanticartier', 'feacutedeacute', 'get face', 'vetement femme', 'pozyczki', 'zippozubehor', 'thrity', '６０＄', 'lovelyto', 'toms cheap', 'instappraisal', 'solde ralph', '札入れ', 'buy forskolin', 'g.o.a.dk', 'vuitton sold', 'spotify.htm', 'ɑl', 'sd3546a', 'footballshirt', 'across thiss', 'moncler parka', 'generate money', 'gambling', 'according your', 'web.best', 'aereas barata', 'gucci online', 'thumbs.asp', 'info base', 'nudeshare', 'shoes man', '.za/katalog', 'ugg sold', 'lv bag', 'cars insur', 'outlet coach', 'ugg femme', 'software.ro', 'them inaccurate', 'hello that', 'oakley', 'cool share', 'fakeoakley', 'men barbour', 'wallinside', 'zapatas', 'sportsbet', '.pl/shop', 'snowboot.', 'cozy ugg', 'download apk', 'free full', 'articlebuzz', 'join.', 'abercrombieital', 'pedigree: proficien', 'signin_password', 'grosen ray', 'warehouse.net', 'very oone', 'calvicieblog', 'madeknown', 'outlet.click', 'thm.htm', 'coach bay', 'canadagoose.', '?mulberry', 'pflegezusatzversicherung', 'one nike', 'copy scrape', '로얄카지노', 'pc access', 'malware', 'writing a dissertation', 'straplesstiered', 'this landline', 'tiendas hollis', 'alviero martini', 'weteihal', 'bvlgaribijoux', 'forsaleuk', 'tr.im', 'our blogroll', 'suchavailable', 'gmail mirror', '99online', 'nfl jers', 'n1 takeaway', 'hollister shop', 'formaldress', 'lauren sale', 'promo+', 'financial emerg', 'финансового мира', 'twerk fitness', 'mbt schuh', 'przeprowadzki', 'uomo cartier', 'bottes paris', 'offerwatch', 'ugg bamsestøvler', 'sex club', 'joma jewelry', 'publicsex', '2012 prospect', 'outletcoach', '+bene', 'ženysrun', 'deltasone', 'chaussure boutique', 'url status', 'lipi=urn', 'extract pill', 'hollister prix', 'carinsur.', 'northfacecanad', 'celine boutique', 'boot online', 'bvlgarisshop', 'dial face', 'ejaculationhelp', 'clenbuterol', 'standing website', 'calvin klein', 'site owner', 'tra dao quan', 'iphone repair', 'online poker', 'sell dump', 'zx’s', 'wholesale north', 'gay porn', 'nikerun', 'hydroxatone', 'プレゼント', 'uggs.', 'free online', 'discount afl', 'librarys.php', 'fanciful belly', 'wille sold', 'jovial salon', 'nicolasbit', 'poker 0', 'investor.ro', '.lineshake', 'ugg sale', 'gaga uk', 'zanottishoe', 'ghd uk', 'slipper khaki', 'internet post', 'handbag whole', 'premiumdignity', 'rouge moncler', 'button ugg', 'cosmetics review', 'win7.co', 'baratoropa', 'borse prada', 'animated porn', 'py.t.ho.n', 'biz.za', 'create 100%', 'social marketing', 'a09.asp', 'buyout keep', '+furla', 'temp test', 'fanatics shop', 'weblog}', 'futuristicmarket', 'www.fot.', 'fortegeneric', 'charge essay', 'men cheap', 'donne donn', 'making provider', 'imptortant', 'insanity.cfm', 'mygiftcard', 'juice detox', 'payments nigeria', 'ãª', 'nhl replica', 'point|', 'sito official', 'jewelry collect', 'upseseglype', 'value site', 'trainingpro', 'nhl.jsp', 'boot get', 'submissive porn', 'mcqueen online', 'aree good', '+outlet', 'likewise pick', 'storiesfunny', '2,5 mg', 'insurancehome', 'partypoker', 'picload.', 'rayban polar', 'turbo vac', 'cigars', 'hngnep', 'подарочной коробк', '.secret', 'loan canad', 'rebeccajap', 'moscowmodel', 'ugg gunstig', 'мужские пальто', 'gucci', 'women jers', 'teamshirt', 'spotify download', 'vuittonsac', 'birkenstock store', 'weight work', 'oakleycan', '1c.in', 'moncler', 'для взлома', 'reviews.net', 'test', 'auto.brand', 'obuvnike', 'com/online', 'chinese low', 'sportsfanstore', 'amateur homemade', 'blog thus', 'myy page', 'взломать одно', 'furworld.ru', 'spyderski', 'dump seller', 'free xxx', 'brand baseball', '7.@', 'lovely thong', 'g string', 'ɍa', 'laminina lpgn', 'moustache play', 'youre so', 'rayban out', 'pink large', 'shed pound', 'wallpapererotic', 'penis.su', 'athletica lululemon', 'porn.vergin', 'wihesd', 'mastery since', 'mulberry uk', 'borse chanel', 'executive', 'bot3.', 'produk kerajinan', 'abcmart.', 'ух ты', 'fitness gym', 'туалетных кабин', '殺手的秘', 'nice designed', '[/move', 'kit that'}\n"
     ]
    }
   ],
   "source": [
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "####### Discrete ##########\n",
    "\n",
    "\n",
    "# Helper function to get last name\n",
    "\n",
    "import re\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def l1words(c):\n",
    "#     return (1,1) if re.search(ltp(l1), c['text'], flags=re.I) else (0,0)\n",
    "\n",
    "# def l2words(c):\n",
    "#      return (1,1) if re.search(ltp(l2), c['text'], flags=re.I) else (0,0)\n",
    "\n",
    "# def l3words(c):\n",
    "#      return (1,1) if re.search(ltp(l3), c['text'], flags=re.I) else (0,0)\n",
    "\n",
    "# notFree = ['you','toll','your','call','meet','talk','freez']\n",
    "\n",
    "# def notFreeSpam(c):\n",
    "#     return (-1,1) if re.search('(free.*'+ltp(notFree)+')|('+ltp(notFree)+'.*free)',\\\n",
    "#                                flags=re.I) else (0,0)\n",
    "\n",
    "def l1words(c):\n",
    "    return (1,1) if len(l1.intersection(c['text'].split())) > 0 else (0,0)\n",
    "\n",
    "def l2words(c):\n",
    "    return (1,1) if len(l2.intersection(c['text'].split())) > 0 else (0,0)\n",
    "\n",
    "def l3words(c):\n",
    "    return (1,1) if len(l3.intersection(c['text'].split())) > 0 else (0,0)\n",
    "\n",
    "\n",
    "\n",
    "notFree1 = {'toll','Toll','freely','call','meet','talk','feedback'}\n",
    "\n",
    "def notFreeSpam(c):\n",
    "    return (-1,1) if 'free' in c['text'].split() and len(notFree.intersection(c['text'].split()))>0 else (0,0)        \n",
    "\n",
    "notFree2 = {'not free','you are','when','wen'}\n",
    "\n",
    "def notFreeSpam2(c):\n",
    "    return (-1,1) if 'free' in c['text'].split() and re.search(ltp(notFree2),c['text'], flags= re.I) else (0,0)        \n",
    "\n",
    "person1 = {'I','i','u','you','ur','your','our','we','us','you\\'re,'}\n",
    "\n",
    "person2 = {'He','he','She','she','they','They','Them','them','their','Their'}\n",
    "\n",
    "def personWords(c):\n",
    "    return (-1,1) if 'free' in c['text'].split() and len(person1.intersection(c['text'].split()))>0 else (0,0)        \n",
    "\n",
    "def secondPersonWords(c):\n",
    "    return (-1,1) if 'free' in c['text'].split() and len(person2.intersection(c['text'].split()))>0 else (0,0)        \n",
    "\n",
    "def noOfCapChars(c):\n",
    "    return (1,1) if (sum(1 for ch in c['text'] if ch.isupper()) > 6) else (0,0)\n",
    "\n",
    "LFs = [\n",
    "   l1words,l2words,l3words,noOfCapChars,notFreeSpam,notFreeSpam2,personWords,secondPersonWords\n",
    "]\n",
    "\n",
    "LF_l = [1,1,1,1,-1,-1,-1,-1]\n",
    "\n",
    "\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "\n",
    "\n",
    "def l1words(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    l1 = ['free','credit','cheap','apply','buy','attention','shop','sex','soon','now','spam']\n",
    "    for w in l1:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "def l2words(c):\n",
    "    sc = 0\n",
    "    l2 = ['gift','click','new','online','discount','earn','miss','hesitate','exclusive','urgent']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in l2:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "def l3words(c):\n",
    "    sc = 0\n",
    "    l3 = ['cash','refund','insurance','money','guaranteed','save','win','teen','weight','hair']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in l3:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "   \n",
    "def notFreeSpam(c):\n",
    "    sc = 0\n",
    "    notFree = ['not','when','call','meet','talk','feedback','toll']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in notFree:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "   \n",
    "\n",
    "def notFreeSpam2(c):\n",
    "    sc = 0\n",
    "    notFree2 =  ['not free','you are','when']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in notFree2:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "def personWords(c):\n",
    "    sc = 0\n",
    "    notFree2 = ['I','you','your','we','us']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in person1:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (-1,sc)\n",
    "    return (-1,1) if 'free' in c['text'].split() and re.search(ltp(notFree2),c['text'], flags= re.I) else (0,0)        \n",
    "\n",
    "def secondPersonWords(c):\n",
    "    sc = 0\n",
    "    notFree2 = ['he','she','they','them','their']\n",
    "    word_vectors = get_word_vectors(c['text'].split())\n",
    "    for w in person1:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (-1,sc)\n",
    "   \n",
    "    \n",
    "def noOfCapChars(c):\n",
    "    l = sum(1 for ch in c['text'] if ch.isupper()) \n",
    "    return (1,l/150)\n",
    "\n",
    "LFs = [\n",
    "   l1words,l2words,l3words,noOfCapChars,notFreeSpam,notFreeSpam2,personWords,secondPersonWords\n",
    "]\n",
    "\n",
    "LF_l = [1,1,1,1,-1,-1,-1,-1]\n",
    "\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def get_L_S_Tensor(df,msg):     \n",
    "    L_S = []\n",
    "    print('labelling ',msg,' data')\n",
    "    for i in range(len(df.index)):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "#             print(i,LF.__name__)    \n",
    "#             print(df.iloc[i]['text'])\n",
    "            l,s = LF(df.iloc[i])\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%250==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "        \n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 9-6-2018, 20:24:46\n",
      "labelling  regex test  data\n",
      "250data points labelled in 0.004436842600504557 mins\n",
      "500data points labelled in 0.008656620979309082 mins\n",
      "750data points labelled in 0.01289513111114502 mins\n",
      "1000data points labelled in 0.017128411928812662 mins\n",
      "1250data points labelled in 0.021420172850290933 mins\n",
      "1500data points labelled in 0.02580949068069458 mins\n",
      "1750data points labelled in 0.0302552858988444 mins\n",
      "labelling  regex train  data\n",
      "250data points labelled in 0.03731296062469482 mins\n",
      "500data points labelled in 0.041551840305328366 mins\n",
      "750data points labelled in 0.045717823505401614 mins\n",
      "1000data points labelled in 0.04994643131891886 mins\n",
      "1250data points labelled in 0.054119853178660075 mins\n",
      "1500data points labelled in 0.05829683542251587 mins\n",
      "1750data points labelled in 0.06246569554011027 mins\n",
      "2000data points labelled in 0.06664372682571411 mins\n",
      "2250data points labelled in 0.07073365052541097 mins\n",
      "2500data points labelled in 0.0749161958694458 mins\n",
      "2750data points labelled in 0.07917277812957764 mins\n",
      "3000data points labelled in 0.08347198565800985 mins\n",
      "3250data points labelled in 0.08775001366933187 mins\n",
      "3500data points labelled in 0.09205952485402426 mins\n",
      "--- 5.732852935791016 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "\n",
    "test_L_S = get_L_S_Tensor(test_df,'regex test')\n",
    "np.save(\"test_L_S_discrete\",np.array(test_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_df,'regex train')\n",
    "np.save(\"train_L_S_discrete\",np.array(train_L_S))\n",
    "\n",
    "\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_df,'regex test')\n",
    "# np.save(\"test_L_S_smooth\",np.array(test_L_S))\n",
    "\n",
    "# train_L_S = get_L_S_Tensor(train_df,'regex train')\n",
    "# np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "(1872, 2, 16) (3700, 2, 16)\n"
     ]
    }
   ],
   "source": [
    "LF_l = [1,1,1,1,-1,-1,-1,-1]\n",
    "\n",
    "def merge(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        ci = []\n",
    "        ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "        ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "        ci.append(ci_l)\n",
    "        ci.append(ci_s)\n",
    "        c.append(ci)\n",
    "    return c\n",
    "import numpy as np\n",
    "test_L_S_s = np.load(\"test_L_S_smooth.npy\")\n",
    "train_L_S_s = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "test_L_S_d = np.load(\"test_L_S_discrete.npy\")\n",
    "train_L_S_d = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "test_L_S = np.array(merge(test_L_S_d,test_L_S_s))\n",
    "train_L_S = np.array(merge(train_L_S_d,train_L_S_s))\n",
    "\n",
    "LF_l = LF_l + LF_l\n",
    "print(len(LF_l))\n",
    "\n",
    "LF_names = [lf.__name__ for lf in LFs] + ['s'+lf.__name__ for lf in LFs]\n",
    "print(len(LF_names))\n",
    "print(test_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(true_labels,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(true_labels,pl))\n",
    "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(true_labels, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1872, 2, 8) (3700, 2, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_L_S = np.load(\"test_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "print(test_L_S.shape,train_L_S.shape)\n",
    "LF_names= [lf.__name__ for lf in LFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1872, 2, 8) (3700, 2, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_L_S = np.load(\"test_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "print(test_L_S.shape,train_L_S.shape)\n",
    "\n",
    "LF_names= ['s'+lf.__name__ for lf in LFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "LF_l = [1,1,1,1,-1,-1,-1,-1]\n",
    "NoOfLFs= len(LF_l)\n",
    "NoOfClasses = 2\n",
    "print(len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def majority():\n",
    "    \n",
    "    # test_L_S_df.mode(axis=1)[0].tolist()\n",
    "    test_L_S_df = pd.DataFrame((np.array(test_L_S)[:,0,:]))\n",
    "    # test_L_S_df = test_L_S_df.astype(int)\n",
    "    test_L_S_df = test_L_S_df.replace(0, np.NaN)\n",
    "\n",
    "    predicted_labels=test_L_S_df.mode(axis=1)[0].tolist()\n",
    "    # print(predicted_labels)\n",
    "    predicted_labels = [ int(x) if not math.isnan(x) else -1 for x in predicted_labels ]\n",
    "    predicted_labels = [1 if x==1 else 0 for x in predicted_labels ]\n",
    "    unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"acc\",accuracy_score(true_labels,predicted_labels))\n",
    "    print(precision_recall_fscore_support(np.array(true_labels),np.array(predicted_labels),average=\"binary\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n"
     ]
    }
   ],
   "source": [
    "#8 discrete\n",
    "majority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n"
     ]
    }
   ],
   "source": [
    "#8 smooth\n",
    "majority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n"
     ]
    }
   ],
   "source": [
    "#16 smooth + discrete\n",
    "majority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(lr,ep,th,af,pcl=np.array([-1,1],dtype=np.float64),norm=True,smooth=True,penalty=0,p3k=3,alp=0.99):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (don't let alpha to be greater than this value)\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        \n",
    "        s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "        print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "\n",
    "    \n",
    "# \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "        else:\n",
    "            print(\"normalized\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            \n",
    "        print(\"loss\",loss)\n",
    "        tf.summary.scalar('un-normloss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(true_labels,pl))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            predictAndPrint(pl)\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 5), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 5), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 5), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 5), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(5,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(5,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 13726.844830684313\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.1169368  0.81415761 1.00906606 1.00020806 1.10542386]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n",
      "\n",
      "1 loss 13725.56064226124\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.11632971 0.8141008  1.00857303 0.99980796 1.10453901]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n",
      "\n",
      "2 loss 13724.282107377767\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.11572269 0.81404392 1.00808007 0.99940882 1.10365554]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n",
      "\n",
      "3 loss 13723.005057427483\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.11511574 0.81398696 1.00758716 0.99901046 1.10277276]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n",
      "\n",
      "4 loss 13721.729424203011\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.11450888 0.81392992 1.00709431 0.99861286 1.10189066]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n",
      "\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378]\n",
      "[[1.11450888 0.81392992 1.00709431 0.99861286 1.10189066]]\n",
      "{0: 856, 1: 1016}\n",
      "acc 0.5331196581196581\n",
      "acc 0.5331196581196581\n",
      "[[807 825]\n",
      " [ 49 191]]\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 LFs\n",
    "train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                            af = tf.truncated_normal_initializer(0.9,0.001,seed),\n",
    "                          pcl=np.array([-1,1],dtype=np.float64),smooth=False,penalty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 22236.173218776625\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11758662 0.81475495 1.00975357 1.00072731 1.10540322 1.04034068\n",
      "  1.16822805 1.03020839]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n",
      "\n",
      "1 loss 22230.42149502927\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11762725 0.81529943 1.00994504 1.00082886 1.10442707 1.03934601\n",
      "  1.16725516 1.02921237]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n",
      "\n",
      "2 loss 22224.70118448782\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11766474 0.81584225 1.01013367 1.00092719 1.10345113 1.03835144\n",
      "  1.16628259 1.02821645]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n",
      "\n",
      "3 loss 22219.007971459185\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11769911 0.81638339 1.01031946 1.00102231 1.10247539 1.03735691\n",
      "  1.16531024 1.02722056]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n",
      "\n",
      "4 loss 22213.34162359708\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11773036 0.81692284 1.01050243 1.00111422 1.10149986 1.03636242\n",
      "  1.16433812 1.02622469]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n",
      "\n",
      "[0.90117545 0.89814215 0.9000956  0.90000624 0.90106378 0.90041339\n",
      " 0.90169206 0.90031208]\n",
      "[[1.11773036 0.81692284 1.01050243 1.00111422 1.10149986 1.03636242\n",
      "  1.16433812 1.02622469]]\n",
      "{0: 760, 1: 1112}\n",
      "acc 0.5235042735042735\n",
      "acc 0.5235042735042735\n",
      "[[750 882]\n",
      " [ 10 230]]\n",
      "(0.2068345323741007, 0.9583333333333334, 0.34023668639053256, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 LFs\n",
    "train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                            af = tf.truncated_normal_initializer(0.9,0.001,seed),\n",
    "                          pcl=np.array([-1,1],dtype=np.float64),smooth=False,penalty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 18360.82658841929\n",
      "[ 1.76799135e-04 -2.85665373e-03 -9.03101723e-04 -9.92460379e-04\n",
      "  6.45089183e-05  1.40033299e-03  2.69095251e-03  1.31093455e-03]\n",
      "[[1.11853327 0.81520651 1.01054947 1.00161913 1.10736541 1.04036003\n",
      "  1.1682123  1.03021431]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 18317.080739006153\n",
      "[-0.00082236 -0.00385599 -0.00190232 -0.00199169 -0.000936    0.00237687\n",
      "  0.00369006  0.00231   ]\n",
      "[[1.11952355 0.81619908 1.01154208 1.00261378 1.10835584 1.0393967\n",
      "  1.16721868 1.02922083]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 18273.334829808686\n",
      "[-0.00182155 -0.00485535 -0.00290156 -0.00299093 -0.0019366   0.00335225\n",
      "  0.00468916  0.00330906]\n",
      "[[1.12051388 0.8171917  1.01253475 1.00360846 1.10934644 1.0384349\n",
      "  1.16622512 1.02822742]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 18229.592625424426\n",
      "[-0.00282076 -0.00585472 -0.00390082 -0.0039902  -0.00293723  0.00432661\n",
      "  0.00568827  0.00430813]\n",
      "[[1.12150423 0.81818434 1.01352744 1.00460318 1.11033715 1.03747448\n",
      "  1.16523161 1.02723406]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 18185.855912969546\n",
      "[-0.00381997 -0.00685411 -0.0049001  -0.00498948 -0.0039379   0.00529991\n",
      "  0.00668739  0.00530721]\n",
      "[[1.12249461 0.81917701 1.01452015 1.00559792 1.11132797 1.03651546\n",
      "  1.16423816 1.02624075]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[-0.00381997 -0.00685411 -0.0049001  -0.00498948 -0.0039379   0.00529991\n",
      "  0.00668739  0.00530721]\n",
      "[[1.12249461 0.81917701 1.01452015 1.00559792 1.11132797 1.03651546\n",
      "  1.16423816 1.02624075]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 18936.46658980817\n",
      "[0.10017841 0.09714484 0.09909847 0.09900911 0.10006657 0.10139972\n",
      " 0.10268999 0.10130995]\n",
      "[[1.11852808 0.81520207 1.01054455 1.00161446 1.10736016 1.04036184\n",
      "  1.16821422 1.03021626]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 18897.673256851478\n",
      "[0.09918023 0.09614639 0.09810019 0.09801082 0.09906752 0.10237525\n",
      " 0.1036886  0.1023085 ]\n",
      "[[1.11951404 0.81619089 1.01153342 1.00260478 1.10834631 1.03940143\n",
      "  1.16722211 1.02922432]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 18858.81664842444\n",
      "[0.09818195 0.09514786 0.09710183 0.09701245 0.09806831 0.10334948\n",
      " 0.10468725 0.10330709]\n",
      "[[1.12050014 0.81717983 1.01252243 1.00359523 1.10933272 1.03844286\n",
      "  1.16623005 1.02823244]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 18819.90564977269\n",
      "[0.09718361 0.09414927 0.09610341 0.09601402 0.097069   0.10432253\n",
      " 0.10568593 0.10430572]\n",
      "[[1.12148633 0.81816886 1.01351152 1.00458579 1.11031933 1.03748599\n",
      "  1.16523806 1.02724062]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 18780.942744425854\n",
      "[0.09618522 0.09315062 0.09510493 0.09501553 0.0960696  0.10529435\n",
      " 0.10668465 0.10530438]\n",
      "[[1.12247262 0.81915796 1.0145007  1.00557646 1.11130613 1.03653085\n",
      "  1.16424613 1.02624886]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.09618522 0.09315062 0.09510493 0.09501553 0.0960696  0.10529435\n",
      " 0.10668465 0.10530438]\n",
      "[[1.12247262 0.81915796 1.0145007  1.00557646 1.11130613 1.03653085\n",
      "  1.16424613 1.02624886]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 19488.61017684833\n",
      "[0.20018295 0.19714906 0.19910289 0.19901351 0.20007181 0.20139982\n",
      " 0.20268724 0.20130713]\n",
      "[[1.11851804 0.81519297 1.01053458 1.00160221 1.10735007 1.04036237\n",
      "  1.16821767 1.03021978]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "1 loss 19455.76217108926\n",
      "[0.19918811 0.19615369 0.19810786 0.19801847 0.1990768  0.20237546\n",
      " 0.20368396 0.20230373]\n",
      "[[1.1194954  0.81617391 1.01151546 1.00258077 1.10832776 1.03940298\n",
      "  1.16722817 1.0292305 ]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "2 loss 19422.77853029775\n",
      "[0.19819302 0.1951581  0.19711259 0.19702319 0.19808142 0.20334963\n",
      " 0.20468082 0.20330047]\n",
      "[[1.12047309 0.81715515 1.01249667 1.00355975 1.10930596 1.03844577\n",
      "  1.1662387  1.02824125]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "3 loss 19389.67393310329\n",
      "[0.19719774 0.19416232 0.19611714 0.19602773 0.19708577 0.20432247\n",
      " 0.20567778 0.20429732]\n",
      "[[1.12145105 0.81813663 1.01347814 1.00453909 1.11028454 1.03749062\n",
      "  1.16524928 1.02725205]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "4 loss 19356.451496980942\n",
      "[0.19620228 0.19316638 0.19512151 0.19503208 0.19608987 0.20529391\n",
      " 0.20667484 0.20529428]\n",
      "[[1.12242926 0.81911835 1.01445985 1.00551878 1.11126351 1.03653758\n",
      "  1.16425991 1.02626291]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "[0.19620228 0.19316638 0.19512151 0.19503208 0.19608987 0.20529391\n",
      " 0.20667484 0.20529428]\n",
      "[[1.12242926 0.81911835 1.01445985 1.00551878 1.11126351 1.03653758\n",
      "  1.16425991 1.02626291]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "acc 0.12927350427350429\n",
      "[[   2 1630]\n",
      " [   0  240]]\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 19982.189070065207\n",
      "[0.30019604 0.29716124 0.29911565 0.29902625 0.30008628 0.30140113\n",
      " 0.30267928 0.30129895]\n",
      "[[1.11849626 0.81517219 1.01051173 1.00156253 1.10732819 1.04036057\n",
      "  1.16822518 1.03022745]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "1 loss 19956.63672050369\n",
      "[0.29921208 0.29617595 0.2981312  0.29804177 0.29910341 0.30237876\n",
      " 0.30366965 0.30228903]\n",
      "[[1.11945429 0.81613448 1.01147326 1.00250153 1.10828684 1.03939876\n",
      "  1.1672415  1.02924414]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "2 loss 19930.874436296574\n",
      "[0.29822742 0.29519    0.29714607 0.29705661 0.2981196  0.30335484\n",
      " 0.30466041 0.3032795 ]\n",
      "[[1.12041315 0.81709757 1.01243567 1.00344198 1.10924659 1.0384394\n",
      "  1.16625771 1.0282607 ]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "3 loss 19904.921933029327\n",
      "[0.29724216 0.2942035  0.29616037 0.29607087 0.297135   0.30432947\n",
      " 0.30565149 0.30427031]\n",
      "[[1.12137275 0.81806134 1.01339882 1.00438375 1.11020729 1.0374824\n",
      "  1.16527386 1.02727719]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "4 loss 19878.782531169847\n",
      "[0.29625633 0.29321648 0.29517411 0.29508458 0.29614966 0.30530257\n",
      " 0.30664289 0.30526144]\n",
      "[[1.12233304 0.81902578 1.01436267 1.00532681 1.11116889 1.03652784\n",
      "  1.16428996 1.02629364]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "[0.29625633 0.29321648 0.29517411 0.29508458 0.29614966 0.30530257\n",
      " 0.30664289 0.30526144]\n",
      "[[1.12233304 0.81902578 1.01436267 1.00532681 1.11116889 1.03652784\n",
      "  1.16428996 1.02629364]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "acc 0.13034188034188035\n",
      "[[   4 1628]\n",
      " [   0  240]]\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20366.773019683744\n",
      "[0.40023972 0.39720168 0.39915814 0.39906866 0.40013367 0.40140369\n",
      " 0.40265366 0.40127261]\n",
      "[[1.11843997 0.81511603 1.01044888 1.00135797 1.10727105 1.0403561\n",
      "  1.16824673 1.03024957]]\n",
      "{0: 8, 1: 1864}\n",
      "acc 0.13247863247863248\n",
      "(0.12875536480686695, 1.0, 0.22813688212927757, None)\n",
      "\n",
      "1 loss 20350.046107880175\n",
      "[0.39929502 0.3962526  0.39821183 0.39812226 0.39919338 0.40238546\n",
      " 0.40362157 0.40223955]\n",
      "[[1.11934625 0.81602613 1.01135429 1.00208784 1.10817797 1.03938768\n",
      "  1.16728112 1.02928484]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "2 loss 20333.05449813594\n",
      "[0.39834796 0.39530136 0.39726324 0.39717357 0.3982501  0.40336582\n",
      " 0.4045907  0.40320776]\n",
      "[[1.120255   0.81693864 1.01226241 1.00282517 1.10908793 1.03842163\n",
      "  1.16631472 1.02831927]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "3 loss 20315.818639301655\n",
      "[0.39739877 0.39434818 0.39631259 0.39622284 0.39730414 0.40434483\n",
      " 0.40556093 0.40417711]\n",
      "[[1.12116601 0.81785333 1.01317295 1.00356971 1.11000063 1.03745789\n",
      "  1.16534765 1.02735302]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "4 loss 20298.340970965535\n",
      "[0.39644755 0.39339314 0.39535998 0.39527014 0.39635563 0.4053224\n",
      " 0.40653219 0.40514754]\n",
      "[[1.12207919 0.81877011 1.01408582 1.00432124 1.11091594 1.03649662\n",
      "  1.16437998 1.02638613]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "[0.39644755 0.39339314 0.39535998 0.39527014 0.39635563 0.4053224\n",
      " 0.40653219 0.40514754]\n",
      "[[1.12207919 0.81877011 1.01408582 1.00432124 1.11091594 1.03649662\n",
      "  1.16437998 1.02638613]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "acc 0.13194444444444445\n",
      "[[   7 1625]\n",
      " [   0  240]]\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20583.92165970571\n",
      "[0.50045124 0.49738986 0.49935774 0.49926707 0.50036045 0.50140421\n",
      " 0.50253896 0.50115409]\n",
      "[[1.11823991 0.8149103  1.01021416 1.00014502 1.10706126 1.04035505\n",
      "  1.16835285 1.03035933]]\n",
      "{0: 117, 1: 1755}\n",
      "acc 0.1842948717948718\n",
      "(0.13333333333333333, 0.975, 0.23458646616541354, None)\n",
      "\n",
      "1 loss 20577.235033002056\n",
      "[0.49970622 0.49661987 0.49860093 0.49850972 0.49963278 0.50238814\n",
      " 0.50339857 0.50200908]\n",
      "[[1.11895534 0.81562229 1.01089697 0.9997101  1.10776935 1.0393838\n",
      "  1.16748495 1.02949577]]\n",
      "{0: 110, 1: 1762}\n",
      "acc 0.18055555555555555\n",
      "(0.13280363223609534, 0.975, 0.23376623376623373, None)\n",
      "\n",
      "2 loss 20570.30070905498\n",
      "[0.49894681 0.49584067 0.49783434 0.49774254 0.49888939 0.50337135\n",
      " 0.5042628  0.50286888]\n",
      "[[1.11967984 0.8163432  1.01158999 0.99928469 1.10848846 1.03841372\n",
      "  1.16661175 1.02862666]]\n",
      "{0: 94, 1: 1778}\n",
      "acc 0.172008547008547\n",
      "(0.13160854893138357, 0.975, 0.2319127849355798, None)\n",
      "\n",
      "3 loss 20563.14033615703\n",
      "[0.49817784 0.49505272 0.49705841 0.49696599 0.49813437 0.5043539\n",
      " 0.50513137 0.50373323]\n",
      "[[1.12041298 0.8170726  1.0122927  0.9988691  1.10921796 1.03744477\n",
      "  1.16573362 1.0277524 ]]\n",
      "{0: 79, 1: 1793}\n",
      "acc 0.1639957264957265\n",
      "(0.1305075292805354, 0.975, 0.23020167240531234, None)\n",
      "\n",
      "4 loss 20555.755966247747\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "acc 0.16346153846153846\n",
      "[[  72 1560]\n",
      " [   6  234]]\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20580.630500302086\n",
      "[0.60097827 0.59790639 0.5998904  0.60053532 0.60089577 0.60139461\n",
      " 0.60218203 0.60078749]\n",
      "[[1.11775041 0.81439293 1.00966378 0.99989442 1.1065355  1.04037393\n",
      "  1.16885854 1.03087742]]\n",
      "{0: 751, 1: 1121}\n",
      "acc 0.43643162393162394\n",
      "(0.13648528099910795, 0.6375, 0.22483468038207202, None)\n",
      "\n",
      "1 loss 20579.77009147968\n",
      "[0.6007707  0.59765879 0.59967356 0.60101271 0.60071643 0.60236449\n",
      " 0.60268451 0.60127542]\n",
      "[[1.11796968 0.81458048 1.00978291 0.99920048 1.10670749 1.0394319\n",
      "  1.16849901 1.03053491]]\n",
      "{0: 741, 1: 1131}\n",
      "acc 0.4310897435897436\n",
      "(0.13527851458885942, 0.6375, 0.22319474835886216, None)\n",
      "\n",
      "2 loss 20578.88072004359\n",
      "[0.60055311 0.59740151 0.59944682 0.60148889 0.60052589 0.60333415\n",
      " 0.60319375 0.60177033]\n",
      "[[1.11819803 0.81477668 1.0099108  0.99850732 1.10689022 1.03849019\n",
      "  1.16812976 1.03018245]]\n",
      "{0: 730, 1: 1142}\n",
      "acc 0.42628205128205127\n",
      "(0.13485113835376533, 0.6416666666666667, 0.22286541244573085, None)\n",
      "\n",
      "3 loss 20577.960021739724\n",
      "[0.60032525 0.59713433 0.59920998 0.60196405 0.60032389 0.60430373\n",
      " 0.60370979 0.60227229]\n",
      "[[1.11843561 0.81498172 1.01004765 0.99781478 1.10708388 1.03754853\n",
      "  1.16775058 1.02981983]]\n",
      "{0: 724, 1: 1148}\n",
      "acc 0.4252136752136752\n",
      "(0.13588850174216027, 0.65, 0.22478386167146971, None)\n",
      "\n",
      "4 loss 20577.00603319121\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "acc 0.41933760683760685\n",
      "[[ 629 1003]\n",
      " [  84  156]]\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20491.89484005952\n",
      "[0.70171324 0.69864685 0.70062745 0.70081443 0.70161249 0.7013397\n",
      " 0.70130827 0.69991784]\n",
      "[[1.11707412 0.81377001 1.00911163 0.99976108 1.10584689 1.04046883\n",
      "  1.16974557 1.03175364]]\n",
      "{0: 1319, 1: 553}\n",
      "acc 0.6746794871794872\n",
      "(0.16636528028933092, 0.38333333333333336, 0.23203026481715006, None)\n",
      "\n",
      "1 loss 20490.3533559554\n",
      "[0.7022599  0.69916111 0.70116915 0.7015996  0.70216931 0.70222736\n",
      " 0.70091162 0.69951094]\n",
      "[[1.11658712 0.81331962 1.00864149 0.99891966 1.10529742 1.03967097\n",
      "  1.17028754 1.03230168]]\n",
      "{0: 1330, 1: 542}\n",
      "acc 0.6794871794871795\n",
      "(0.16789667896678967, 0.37916666666666665, 0.23273657289002558, None)\n",
      "\n",
      "2 loss 20488.751850435427\n",
      "[0.70281562 0.69968474 0.70171976 0.70238411 0.70273541 0.70311257\n",
      " 0.70050276 0.699092  ]\n",
      "[[1.11608988 0.81286174 1.00816411 0.9980787  1.10473822 1.03887721\n",
      "  1.17083634 1.03285645]]\n",
      "{0: 1338, 1: 534}\n",
      "acc 0.6837606837606838\n",
      "(0.1704119850187266, 0.37916666666666665, 0.2351421188630491, None)\n",
      "\n",
      "3 loss 20487.085580849205\n",
      "[0.70337933 0.70021824 0.70227811 0.70316809 0.7033107  0.70399565\n",
      " 0.7000817  0.69866102]\n",
      "[[1.11558246 0.81239647 1.00767963 0.99723801 1.10416943 1.038087\n",
      "  1.17139194 1.03341792]]\n",
      "{0: 1358, 1: 514}\n",
      "acc 0.6944444444444444\n",
      "(0.17704280155642024, 0.37916666666666665, 0.24137931034482757, None)\n",
      "\n",
      "4 loss 20485.35878728632\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n",
      "\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "acc 0.6971153846153846\n",
      "[[1217  415]\n",
      " [ 152   88]]\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20416.11477267166\n",
      "[0.80177876 0.79841384 0.80034635 0.80086779 0.80166794 0.80119559\n",
      " 0.80097821 0.79959615]\n",
      "[[1.11708161 0.81399095 1.00936536 0.99972003 1.10587144 1.04065528\n",
      "  1.16991253 1.03191626]]\n",
      "{0: 1596, 1: 276}\n",
      "acc 0.7767094017094017\n",
      "(0.17753623188405798, 0.20416666666666666, 0.18992248062015504, None)\n",
      "\n",
      "1 loss 20414.244143426273\n",
      "[0.80239279 0.79868839 0.80060122 0.80170315 0.80226859 0.80186248\n",
      " 0.80026426 0.79888019]\n",
      "[[1.11660934 0.81376312 1.00916489 0.99883219 1.10535989 1.04013851\n",
      "  1.17061714 1.03262275]]\n",
      "{0: 1603, 1: 269}\n",
      "acc 0.780448717948718\n",
      "(0.1821561338289963, 0.20416666666666666, 0.19253438113948923, None)\n",
      "\n",
      "2 loss 20412.362813009768\n",
      "[0.80301028 0.79896271 0.8008549  0.80253786 0.80286907 0.80252403\n",
      " 0.79954715 0.79816412]\n",
      "[[1.11613539 0.81353481 1.0089634  0.99794452 1.10484758 1.03962777\n",
      "  1.17132341 1.03333089]]\n",
      "{0: 1609, 1: 263}\n",
      "acc 0.7836538461538461\n",
      "(0.18631178707224336, 0.20416666666666666, 0.194831013916501, None)\n",
      "\n",
      "3 loss 20410.456859340014\n",
      "[0.80362912 0.79923582 0.80110915 0.80337217 0.80347255 0.80318104\n",
      " 0.79882679 0.79744528]\n",
      "[[1.11565994 0.81330614 1.00876096 0.99705684 1.10433462 1.03912213\n",
      "  1.17203133 1.03404068]]\n",
      "{0: 1614, 1: 258}\n",
      "acc 0.7852564102564102\n",
      "(0.18604651162790697, 0.2, 0.19277108433734938, None)\n",
      "\n",
      "4 loss 20408.5262260676\n",
      "[0.80425134 0.79950939 0.80136351 0.80420606 0.80407753 0.8038335\n",
      " 0.7981063  0.79672311]\n",
      "[[1.11518315 0.81307713 1.00855762 0.99616913 1.10382115 1.03862156\n",
      "  1.17274088 1.03475209]]\n",
      "{0: 1621, 1: 251}\n",
      "acc 0.7889957264957265\n",
      "(0.19123505976095617, 0.2, 0.19551934826883913, None)\n",
      "\n",
      "[0.80425134 0.79950939 0.80136351 0.80420606 0.80407753 0.8038335\n",
      " 0.7981063  0.79672311]\n",
      "[[1.11518315 0.81307713 1.00855762 0.99616913 1.10382115 1.03862156\n",
      "  1.17274088 1.03475209]]\n",
      "{0: 1621, 1: 251}\n",
      "acc 0.7889957264957265\n",
      "acc 0.7889957264957265\n",
      "[[1429  203]\n",
      " [ 192   48]]\n",
      "(0.19123505976095617, 0.2, 0.19551934826883913, None)\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20481.7534573971\n",
      "[0.90133331 0.89832628 0.90021453 0.90094761 0.9012611  0.90101194\n",
      " 0.90101186 0.89963158]\n",
      "[[1.11746069 0.81405708 1.00947687 0.99963875 1.10625051 1.04082387\n",
      "  1.16988746 1.03189007]]\n",
      "{0: 1743, 1: 129}\n",
      "acc 0.8317307692307693\n",
      "(0.20930232558139536, 0.1125, 0.14634146341463414, None)\n",
      "\n",
      "1 loss 20481.16357573364\n",
      "[0.90149226 0.89851361 0.90033706 0.9018821  0.90145401 0.9013798\n",
      " 0.90033358 0.89895234]\n",
      "[[1.11737236 0.81389664 1.00939196 0.99865592 1.10612373 1.04057069\n",
      "  1.17056664 1.03257002]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "2 loss 20480.567918255358\n",
      "[0.90165278 0.8987009  0.90045996 0.90281836 0.90164796 0.90174282\n",
      " 0.89965397 0.89827319]\n",
      "[[1.11728195 0.81373573 1.00930624 0.99767268 1.10599554 1.04032152\n",
      "  1.17124635 1.0332505 ]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "3 loss 20479.96679819928\n",
      "[0.90181488 0.89888808 0.90058323 0.90375661 0.90184298 0.90210194\n",
      " 0.89897299 0.89759503]\n",
      "[[1.11718946 0.81357443 1.0092197  0.99668882 1.10586598 1.0400756\n",
      "  1.17192655 1.03393148]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "4 loss 20479.359065684752\n",
      "[0.90197856 0.89907516 0.90070687 0.90469686 0.90203905 0.90245721\n",
      " 0.8982917  0.89691552]\n",
      "[[1.11709492 0.81341274 1.00913233 0.99570438 1.10573508 1.0398329\n",
      "  1.17260724 1.03461294]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "[0.90197856 0.89907516 0.90070687 0.90469686 0.90203905 0.90245721\n",
      " 0.8982917  0.89691552]\n",
      "[[1.11709492 0.81341274 1.00913233 0.99570438 1.10573508 1.0398329\n",
      "  1.17260724 1.03461294]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "acc 0.8327991452991453\n",
      "[[1532  100]\n",
      " [ 213   27]]\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20516.768699937562\n",
      "[1.00236167 0.99893892 1.00099994 1.0000077  1.00226032 1.00167774\n",
      " 1.00283782 1.00156836]\n",
      "[[1.11753383 0.81411864 1.00955677 1.00062431 1.10634892 1.04135482\n",
      "  1.16989014 1.03189198]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "1 loss 20516.76817400778\n",
      "[1.00357064 0.99949902 1.00240177 1.00001264 1.0034769  1.00295744\n",
      " 1.00401409 1.0028639 ]\n",
      "[[1.11752122 0.81401947 1.00955653 1.00062431 1.10632061 1.04136795\n",
      "  1.17057034 1.03257215]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "2 loss 20516.767601123014\n",
      "[1.00471708 0.99981021 1.00361256 1.00003389 1.00462641 1.00413127\n",
      " 1.00514486 1.00404328]\n",
      "[[1.11750872 0.81392017 1.00955639 1.00062431 1.10629254 1.04138195\n",
      "  1.17125056 1.03325233]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "3 loss 20516.766965825198\n",
      "[1.00582915 0.99991273 1.00475688 1.00048536 1.00573998 1.00525779\n",
      " 1.00624771 1.00517275]\n",
      "[[1.1174965  0.81382077 1.00955654 1.00062449 1.10626496 1.04139759\n",
      "  1.17193088 1.03393257]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "4 loss 20516.766252634578\n",
      "[1.00692028 0.99993241 1.00586764 1.00200698 1.00683198 1.00635791\n",
      " 1.00733278 1.00627479]\n",
      "[[1.11748479 0.81372128 1.00955723 1.00067032 1.10623817 1.04141579\n",
      "  1.17261133 1.03461289]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "[1.00692028 0.99993241 1.00586764 1.00200698 1.00683198 1.00635791\n",
      " 1.00733278 1.00627479]\n",
      "[[1.11748479 0.81372128 1.00955723 1.00067032 1.10623817 1.04141579\n",
      "  1.17261133 1.03461289]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "acc 0.844017094017094\n",
      "[[1553   79]\n",
      " [ 213   27]]\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "for i in np.linspace(0,1,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20583.92165970571\n",
      "[0.50045124 0.49738986 0.49935774 0.49926707 0.50036045 0.50140421\n",
      " 0.50253896 0.50115409]\n",
      "[[1.11823991 0.8149103  1.01021416 1.00014502 1.10706126 1.04035505\n",
      "  1.16835285 1.03035933]]\n",
      "{0: 117, 1: 1755}\n",
      "acc 0.1842948717948718\n",
      "(0.13333333333333333, 0.975, 0.23458646616541354, None)\n",
      "\n",
      "1 loss 20577.235033002056\n",
      "[0.49970622 0.49661987 0.49860093 0.49850972 0.49963278 0.50238814\n",
      " 0.50339857 0.50200908]\n",
      "[[1.11895534 0.81562229 1.01089697 0.9997101  1.10776935 1.0393838\n",
      "  1.16748495 1.02949577]]\n",
      "{0: 110, 1: 1762}\n",
      "acc 0.18055555555555555\n",
      "(0.13280363223609534, 0.975, 0.23376623376623373, None)\n",
      "\n",
      "2 loss 20570.30070905498\n",
      "[0.49894681 0.49584067 0.49783434 0.49774254 0.49888939 0.50337135\n",
      " 0.5042628  0.50286888]\n",
      "[[1.11967984 0.8163432  1.01158999 0.99928469 1.10848846 1.03841372\n",
      "  1.16661175 1.02862666]]\n",
      "{0: 94, 1: 1778}\n",
      "acc 0.172008547008547\n",
      "(0.13160854893138357, 0.975, 0.2319127849355798, None)\n",
      "\n",
      "3 loss 20563.14033615703\n",
      "[0.49817784 0.49505272 0.49705841 0.49696599 0.49813437 0.5043539\n",
      " 0.50513137 0.50373323]\n",
      "[[1.12041298 0.8170726  1.0122927  0.9988691  1.10921796 1.03744477\n",
      "  1.16573362 1.0277524 ]]\n",
      "{0: 79, 1: 1793}\n",
      "acc 0.1639957264957265\n",
      "(0.1305075292805354, 0.975, 0.23020167240531234, None)\n",
      "\n",
      "4 loss 20555.755966247747\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "acc 0.16346153846153846\n",
      "[[  72 1560]\n",
      " [   6  234]]\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "alpha-mean 0.51\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20591.94686971871\n",
      "[0.51049119 0.50743261 0.50940091 0.50965662 0.51040234 0.51140393\n",
      " 0.51251739 0.51113178]\n",
      "[[1.11820278 0.81487188 1.01017062 1.00006912 1.10702145 1.04035565\n",
      "  1.16837773 1.03038511]]\n",
      "{0: 199, 1: 1673}\n",
      "acc 0.22382478632478633\n",
      "(0.13747758517632994, 0.9583333333333334, 0.24046001045478305, None)\n",
      "\n",
      "1 loss 20586.577948592258\n",
      "[0.5097888  0.50670563 0.50868896 0.50929653 0.50972069 0.51238772\n",
      " 0.51335568 0.51196468]\n",
      "[[1.11888104 0.8155451  1.01080972 0.99955247 1.10768962 1.03938497\n",
      "  1.16753429 1.0295469 ]]\n",
      "{0: 189, 1: 1683}\n",
      "acc 0.21955128205128205\n",
      "(0.13725490196078433, 0.9625, 0.24024960998439937, None)\n",
      "\n",
      "2 loss 20581.032981565204\n",
      "[0.50907627 0.50596942 0.5079677  0.50893089 0.50902676 0.51337091\n",
      " 0.51419873 0.51280257]\n",
      "[[1.11956855 0.81622742 1.01145917 0.99903846 1.1083691  1.03841527\n",
      "  1.16668509 1.02870268]]\n",
      "{0: 182, 1: 1690}\n",
      "acc 0.21581196581196582\n",
      "(0.1366863905325444, 0.9625, 0.23937823834196892, None)\n",
      "\n",
      "3 loss 20575.31665374323\n",
      "[0.50835402 0.50522438 0.50723704 0.50855988 0.50832111 0.51435355\n",
      " 0.51504627 0.5136452 ]\n",
      "[[1.12026492 0.81691844 1.01211853 0.99852702 1.10905931 1.03744648\n",
      "  1.16583049 1.0278528 ]]\n",
      "{0: 167, 1: 1705}\n",
      "acc 0.20993589743589744\n",
      "(0.13665689149560117, 0.9708333333333333, 0.23958868894601543, None)\n",
      "\n",
      "4 loss 20569.427504224226\n",
      "[0.50762234 0.50447078 0.50649728 0.50818367 0.50760416 0.51533559\n",
      " 0.51589809 0.51449239]\n",
      "[[1.12096988 0.81761792 1.01278752 0.99801821 1.10975987 1.03647874\n",
      "  1.1649707  1.0269975 ]]\n",
      "{0: 152, 1: 1720}\n",
      "acc 0.202991452991453\n",
      "(0.13604651162790699, 0.975, 0.23877551020408164, None)\n",
      "\n",
      "[0.50762234 0.50447078 0.50649728 0.50818367 0.50760416 0.51533559\n",
      " 0.51589809 0.51449239]\n",
      "[[1.12096988 0.81761792 1.01278752 0.99801821 1.10975987 1.03647874\n",
      "  1.1649707  1.0269975 ]]\n",
      "{0: 152, 1: 1720}\n",
      "acc 0.202991452991453\n",
      "acc 0.202991452991453\n",
      "[[ 146 1486]\n",
      " [   6  234]]\n",
      "(0.13604651162790699, 0.975, 0.23877551020408164, None)\n",
      "alpha-mean 0.52\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20595.02147404541\n",
      "[0.52052685 0.51746648 0.51943632 0.51990092 0.52043477 0.52140363\n",
      " 0.52249668 0.52111038]\n",
      "[[1.11816869 0.81483626 1.01013043 1.00003181 1.10698487 1.04035629\n",
      "  1.16840263 1.03041088]]\n",
      "{0: 261, 1: 1611}\n",
      "acc 0.2537393162393162\n",
      "(0.1409062693978895, 0.9458333333333333, 0.24527282549972984, None)\n",
      "\n",
      "1 loss 20590.360122374983\n",
      "[0.51985988 0.51677291 0.5187582  0.51978308 0.51978486 0.52238703\n",
      " 0.52331476 0.52192239]\n",
      "[[1.11881332 0.81547406 1.01072974 0.99947785 1.10761682 1.0393866\n",
      "  1.16758329 1.02959764]]\n",
      "{0: 247, 1: 1625}\n",
      "acc 0.24626068376068377\n",
      "(0.1396923076923077, 0.9458333333333333, 0.24343163538873994, None)\n",
      "\n",
      "2 loss 20585.538069193608\n",
      "[0.51918203 0.51606935 0.51806944 0.51966146 0.51912191 0.5233699\n",
      " 0.52413807 0.52273987]\n",
      "[[1.11946786 0.81612163 1.01134013 0.99892536 1.10826085 1.03841773\n",
      "  1.16675737 1.02877752]]\n",
      "{0: 232, 1: 1640}\n",
      "acc 0.23824786324786323\n",
      "(0.13841463414634148, 0.9458333333333333, 0.24148936170212768, None)\n",
      "\n",
      "3 loss 20580.55967535081\n",
      "[0.51849391 0.51535617 0.51737047 0.51953611 0.51844653 0.52435233\n",
      " 0.52496635 0.52356256]\n",
      "[[1.12013195 0.81677861 1.01196116 0.99837421 1.10891643 1.03744961\n",
      "  1.16592522 1.02795089]]\n",
      "{0: 205, 1: 1667}\n",
      "acc 0.22596153846153846\n",
      "(0.13737252549490103, 0.9541666666666667, 0.2401678028316728, None)\n",
      "\n",
      "4 loss 20575.42329295364\n",
      "[0.51779581 0.51463365 0.5166616  0.51940707 0.51775906 0.52533426\n",
      " 0.5257995  0.52439033]\n",
      "[[1.12080535 0.81744477 1.0125926  0.99782442 1.1095832  1.03648235\n",
      "  1.16508707 1.02711799]]\n",
      "{0: 195, 1: 1677}\n",
      "acc 0.2206196581196581\n",
      "(0.13655336911150864, 0.9541666666666667, 0.23891497130933748, None)\n",
      "\n",
      "[0.51779581 0.51463365 0.5166616  0.51940707 0.51775906 0.52533426\n",
      " 0.5257995  0.52439033]\n",
      "[[1.12080535 0.81744477 1.0125926  0.99782442 1.1095832  1.03648235\n",
      "  1.16508707 1.02711799]]\n",
      "{0: 195, 1: 1677}\n",
      "acc 0.2206196581196581\n",
      "acc 0.2206196581196581\n",
      "[[ 184 1448]\n",
      " [  11  229]]\n",
      "(0.13655336911150864, 0.9541666666666667, 0.23891497130933748, None)\n",
      "alpha-mean 0.53\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20596.569187213732\n",
      "[0.53056713 0.52750172 0.5294748  0.53000599 0.53047614 0.53140315\n",
      " 0.53247358 0.53108655]\n",
      "[[1.11813161 0.81479709 1.01008689 1.00000316 1.10694509 1.04035727\n",
      "  1.16843204 1.03044129]]\n",
      "{0: 318, 1: 1554}\n",
      "acc 0.2745726495726496\n",
      "(0.1402831402831403, 0.9083333333333333, 0.24303232998885174, None)\n",
      "\n",
      "1 loss 20592.512709864957\n",
      "[0.52993998 0.52684464 0.52883482 0.52998922 0.5298669  0.53238579\n",
      " 0.53326919 0.53187535]\n",
      "[[1.11873964 0.81539604 1.01064296 0.99942115 1.10753763 1.03938922\n",
      "  1.16764122 1.02965756]]\n",
      "{0: 300, 1: 1572}\n",
      "acc 0.2670940170940171\n",
      "(0.13994910941475827, 0.9166666666666666, 0.24282560706401765, None)\n",
      "\n",
      "2 loss 20588.308656833797\n",
      "[0.52930104 0.52617689 0.52818343 0.529965   0.52924377 0.53336796\n",
      " 0.53407054 0.53267015]\n",
      "[[1.11935829 0.81600546 1.01121082 0.99884031 1.10814309 1.0384219\n",
      "  1.16684288 1.02886598]]\n",
      "{0: 283, 1: 1589}\n",
      "acc 0.26014957264957267\n",
      "(0.13971050975456262, 0.925, 0.2427556041552761, None)\n",
      "\n",
      "3 loss 20583.959728711245\n",
      "[0.52865062 0.52549876 0.52752096 0.52993748 0.5286072  0.53434974\n",
      " 0.5348774  0.53347071]\n",
      "[[1.11998723 0.81662505 1.01179012 0.9982605  1.10876098 1.03745522\n",
      "  1.16603736 1.02806689]]\n",
      "{0: 266, 1: 1606}\n",
      "acc 0.25427350427350426\n",
      "(0.14009962640099627, 0.9375, 0.24377031419284945, None)\n",
      "\n",
      "4 loss 20579.463361370716\n",
      "[0.52798899 0.52481045 0.52684769 0.52990667 0.52795758 0.53533107\n",
      " 0.53568965 0.5342769 ]\n",
      "[[1.12062624 0.81725458 1.01238062 0.99768173 1.10939096 1.03648929\n",
      "  1.16522489 1.02726056]]\n",
      "{0: 255, 1: 1617}\n",
      "acc 0.2483974358974359\n",
      "(0.1391465677179963, 0.9375, 0.24232633279483035, None)\n",
      "\n",
      "[0.52798899 0.52481045 0.52684769 0.52990667 0.52795758 0.53533107\n",
      " 0.53568965 0.5342769 ]\n",
      "[[1.12062624 0.81725458 1.01238062 0.99768173 1.10939096 1.03648929\n",
      "  1.16522489 1.02726056]]\n",
      "{0: 255, 1: 1617}\n",
      "acc 0.2483974358974359\n",
      "acc 0.2483974358974359\n",
      "[[ 240 1392]\n",
      " [  15  225]]\n",
      "(0.1391465677179963, 0.9375, 0.24232633279483035, None)\n",
      "alpha-mean 0.54\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20597.014980907403\n",
      "[0.54061148 0.53754019 0.53951829 0.54008357 0.54052225 0.54140255\n",
      " 0.54244713 0.54105924]\n",
      "[[1.11809061 0.81475351 1.01003899 0.99997941 1.10690107 1.04035845\n",
      "  1.16846718 1.03047758]]\n",
      "{0: 367, 1: 1505}\n",
      "acc 0.2932692307692308\n",
      "(0.14019933554817277, 0.8791666666666667, 0.24183381088825215, None)\n",
      "\n",
      "1 loss 20593.52645823294\n",
      "[0.54002814 0.53691945 0.53892153 0.54015403 0.5399589  0.54238427\n",
      " 0.54321683 0.54182132]\n",
      "[[1.11865789 0.81530899 1.01054708 0.99937402 1.10744968 1.03939238\n",
      "  1.16771072 1.02972937]]\n",
      "{0: 352, 1: 1520}\n",
      "acc 0.28739316239316237\n",
      "(0.1401315789473684, 0.8875, 0.2420454545454545, None)\n",
      "\n",
      "2 loss 20589.904127117134\n",
      "[0.53943233 0.53628717 0.53831255 0.54022151 0.53938087 0.54336557\n",
      " 0.54399279 0.54258995]\n",
      "[[1.11923644 0.81587559 1.01106759 0.99876966 1.10801196 1.03842693\n",
      "  1.16694574 1.02897226]]\n",
      "{0: 340, 1: 1532}\n",
      "acc 0.28205128205128205\n",
      "(0.13968668407310705, 0.8916666666666667, 0.24153498871331827, None)\n",
      "\n",
      "3 loss 20586.14966530774\n",
      "[0.53882454 0.53564366 0.53769167 0.54028603 0.53878856 0.54434654\n",
      " 0.5447748  0.54336486]\n",
      "[[1.11982597 0.81645304 1.01160021 0.99816615 1.10858749 1.03746201\n",
      "  1.16617254 1.02820659]]\n",
      "{0: 327, 1: 1545}\n",
      "acc 0.2761752136752137\n",
      "(0.13915857605177995, 0.8958333333333334, 0.24089635854341743, None)\n",
      "\n",
      "4 loss 20582.260105126254\n",
      "[0.53820515 0.53498914 0.53705913 0.54034758 0.53818227 0.54532713\n",
      " 0.54556275 0.54414597]\n",
      "[[1.12042627 0.81704117 1.01214476 0.99756352 1.10917595 1.03649771\n",
      "  1.16539137 1.0274326 ]]\n",
      "{0: 310, 1: 1562}\n",
      "acc 0.2702991452991453\n",
      "(0.13956466069142126, 0.9083333333333333, 0.24195338512763598, None)\n",
      "\n",
      "[0.53820515 0.53498914 0.53705913 0.54034758 0.53818227 0.54532713\n",
      " 0.54556275 0.54414597]\n",
      "[[1.12042627 0.81704117 1.01214476 0.99756352 1.10917595 1.03649771\n",
      "  1.16539137 1.0274326 ]]\n",
      "{0: 310, 1: 1562}\n",
      "acc 0.2702991452991453\n",
      "acc 0.2702991452991453\n",
      "[[ 288 1344]\n",
      " [  22  218]]\n",
      "(0.13956466069142126, 0.9083333333333333, 0.24195338512763598, None)\n",
      "alpha-mean 0.55\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20596.458542167147\n",
      "[0.55066128 0.54758706 0.54956498 0.55014657 0.55056567 0.55140182\n",
      " 0.55241683 0.55102802]\n",
      "[[1.11804542 0.81470528 1.00998653 0.99995977 1.1068524  1.04035989\n",
      "  1.1685091  1.03052082]]\n",
      "{0: 419, 1: 1453}\n",
      "acc 0.3125\n",
      "(0.1397109428768066, 0.8458333333333333, 0.23981098641464854, None)\n",
      "\n",
      "1 loss 20593.506286957927\n",
      "[0.55012779 0.54701279 0.54901483 0.5502812  0.55004826 0.55238242\n",
      " 0.55315672 0.55175932]\n",
      "[[1.11856742 0.81521234 1.01044161 0.99933462 1.10735206 1.03939617\n",
      "  1.16779407 1.02981536]]\n",
      "{0: 404, 1: 1468}\n",
      "acc 0.30662393162393164\n",
      "(0.1396457765667575, 0.8541666666666666, 0.24004683840749416, None)\n",
      "\n",
      "2 loss 20590.43511424412\n",
      "[0.54958098 0.54642631 0.54845184 0.55041305 0.54951562 0.55336265\n",
      " 0.55390338 0.55249769]\n",
      "[[1.1191012  0.81573101 1.01090951 0.99871042 1.10786598 1.03843298\n",
      "  1.16706949 1.02909995]]\n",
      "{0: 391, 1: 1481}\n",
      "acc 0.30288461538461536\n",
      "(0.14044564483457123, 0.8666666666666667, 0.24171993027309702, None)\n",
      "\n",
      "3 loss 20587.246069431007\n",
      "[0.54902123 0.54582796 0.54787799 0.55054211 0.54896804 0.55434261\n",
      " 0.55465648 0.55324293]\n",
      "[[1.11964651 0.81626109 1.01139002 0.998087   1.10839382 1.0374702\n",
      "  1.16633564 1.02837489]]\n",
      "{0: 379, 1: 1493}\n",
      "acc 0.29754273504273504\n",
      "(0.13998660415271266, 0.8708333333333333, 0.24120023081361802, None)\n",
      "\n",
      "4 loss 20583.936311783655\n",
      "[0.5484487  0.54521784 0.54729177 0.55066835 0.54840576 0.55532226\n",
      " 0.55541604 0.55399494]\n",
      "[[1.12020321 0.81680243 1.01188301 0.99746439 1.10893532 1.03650792\n",
      "  1.16559274 1.0276404 ]]\n",
      "{0: 366, 1: 1506}\n",
      "acc 0.2938034188034188\n",
      "(0.1407702523240372, 0.8833333333333333, 0.24284077892325315, None)\n",
      "\n",
      "[0.5484487  0.54521784 0.54729177 0.55066835 0.54840576 0.55532226\n",
      " 0.55541604 0.55399494]\n",
      "[[1.12020321 0.81680243 1.01188301 0.99746439 1.10893532 1.03650792\n",
      "  1.16559274 1.0276404 ]]\n",
      "{0: 366, 1: 1506}\n",
      "acc 0.2938034188034188\n",
      "acc 0.2938034188034188\n",
      "[[ 338 1294]\n",
      " [  28  212]]\n",
      "(0.1407702523240372, 0.8833333333333333, 0.24284077892325315, None)\n",
      "alpha-mean 0.5599999999999999\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20595.000832338654\n",
      "[0.56071099 0.55763933 0.55961904 0.56025386 0.56061905 0.56140095\n",
      " 0.56238189 0.56099208]\n",
      "[[1.11799567 0.81465218 1.00992947 0.99994365 1.10679867 1.04036162\n",
      "  1.16855896 1.03057212]]\n",
      "{0: 487, 1: 1385}\n",
      "acc 0.33814102564102566\n",
      "(0.13935018050541517, 0.8041666666666667, 0.23753846153846156, None)\n",
      "\n",
      "1 loss 20592.549418794068\n",
      "[0.56022731 0.55711779 0.55912339 0.56048359 0.56015563 0.5623802\n",
      " 0.56308703 0.56168763]\n",
      "[[1.11846744 0.81510556 1.01032626 0.9993016  1.10724381 1.03940069\n",
      "  1.16789368 1.0299179 ]]\n",
      "{0: 479, 1: 1393}\n",
      "acc 0.33600427350427353\n",
      "(0.1399856424982053, 0.8125, 0.2388242498469075, None)\n",
      "\n",
      "2 loss 20589.996015987737\n",
      "[0.55973013 0.55658365 0.55861467 0.5607107  0.5596767  0.56335915\n",
      " 0.5637994  0.56239067]\n",
      "[[1.11895126 0.8155708  1.01073601 0.99866048 1.10770351 1.0384402\n",
      "  1.16721792 1.02925279]]\n",
      "{0: 460, 1: 1412}\n",
      "acc 0.327991452991453\n",
      "(0.1395184135977337, 0.8208333333333333, 0.23849878934624696, None)\n",
      "\n",
      "3 loss 20587.339804873027\n",
      "[0.55921959 0.55603706 0.55809294 0.5609352  0.55918241 0.56433789\n",
      " 0.56451882 0.56310107]\n",
      "[[1.119447   0.8160478  1.01115858 0.99802011 1.10817754 1.03748\n",
      "  1.16653189 1.028577  ]]\n",
      "{0: 438, 1: 1434}\n",
      "acc 0.3173076923076923\n",
      "(0.13807531380753138, 0.825, 0.23655913978494625, None)\n",
      "\n",
      "4 loss 20584.577287147782\n",
      "[0.55869579 0.55547811 0.55755824 0.56115706 0.55867292 0.56531639\n",
      " 0.56524523 0.56381876]\n",
      "[[1.11995456 0.81653647 1.01159394 0.9973805  1.10866574 1.03652017\n",
      "  1.16583575 1.02789071]]\n",
      "{0: 421, 1: 1451}\n",
      "acc 0.31143162393162394\n",
      "(0.13852515506547208, 0.8375, 0.2377291543465405, None)\n",
      "\n",
      "[0.55869579 0.55547811 0.55755824 0.56115706 0.55867292 0.56531639\n",
      " 0.56524523 0.56381876]\n",
      "[[1.11995456 0.81653647 1.01159394 0.9973805  1.10866574 1.03652017\n",
      "  1.16583575 1.02789071]]\n",
      "{0: 421, 1: 1451}\n",
      "acc 0.31143162393162394\n",
      "acc 0.31143162393162394\n",
      "[[ 382 1250]\n",
      " [  39  201]]\n",
      "(0.13852515506547208, 0.8375, 0.2377291543465405, None)\n",
      "alpha-mean 0.57\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20592.62965308428\n",
      "[0.57077204 0.56769767 0.5696771  0.57033248 0.57068057 0.57139988\n",
      " 0.57234164 0.5709507 ]\n",
      "[[1.11794111 0.81459406 1.00986798 0.99992984 1.1067399  1.04036371\n",
      "  1.16861812 1.03063284]]\n",
      "{0: 551, 1: 1321}\n",
      "acc 0.3541666666666667\n",
      "(0.1332323996971991, 0.7333333333333333, 0.22549647661755284, None)\n",
      "\n",
      "1 loss 20590.647987083503\n",
      "[0.57035044 0.5672351  0.56924049 0.5706362  0.57027979 0.5723775\n",
      " 0.5730065  0.57160479]\n",
      "[[1.11835738 0.81498834 1.01020138 0.99927338 1.10712485 1.03940615\n",
      "  1.16801243 1.03003981]]\n",
      "{0: 542, 1: 1330}\n",
      "acc 0.3525641025641026\n",
      "(0.13458646616541353, 0.7458333333333333, 0.22802547770700637, None)\n",
      "\n",
      "2 loss 20588.58243244335\n",
      "[0.56991538 0.56675986 0.5687907  0.57093764 0.56986367 0.57335486\n",
      " 0.57367889 0.57226671]\n",
      "[[1.11878565 0.81539439 1.01054743 0.99861785 1.10752426 1.03844895\n",
      "  1.16739558 1.02943522]]\n",
      "{0: 529, 1: 1343}\n",
      "acc 0.3488247863247863\n",
      "(0.1355174981384959, 0.7583333333333333, 0.22994314592545798, None)\n",
      "\n",
      "3 loss 20586.431374762335\n",
      "[0.56946689 0.56627199 0.56832775 0.57123681 0.56943239 0.57433206\n",
      " 0.5743587  0.57293636]\n",
      "[[1.11922587 0.81581218 1.01090611 0.99796307 1.10793799 1.03749194\n",
      "  1.16676768 1.02881917]]\n",
      "{0: 510, 1: 1362}\n",
      "acc 0.3472222222222222\n",
      "(0.1395007342143906, 0.7916666666666666, 0.23720349563046192, None)\n",
      "\n",
      "4 loss 20584.19160418265\n",
      "[0.56900495 0.56577152 0.56785214 0.57153367 0.56898582 0.57530907\n",
      " 0.5750459  0.5736137 ]\n",
      "[[1.11967803 0.81624171 1.01127746 0.99730905 1.10836602 1.0365352\n",
      "  1.16612881 1.02819176]]\n",
      "{0: 493, 1: 1379}\n",
      "acc 0.3402777777777778\n",
      "(0.13923132704858593, 0.8, 0.237183446571958, None)\n",
      "\n",
      "[0.56900495 0.56577152 0.56785214 0.57153367 0.56898582 0.57530907\n",
      " 0.5750459  0.5736137 ]\n",
      "[[1.11967803 0.81624171 1.01127746 0.99730905 1.10836602 1.0365352\n",
      "  1.16612881 1.02819176]]\n",
      "{0: 493, 1: 1379}\n",
      "acc 0.3402777777777778\n",
      "acc 0.3402777777777778\n",
      "[[ 445 1187]\n",
      " [  48  192]]\n",
      "(0.13923132704858593, 0.8, 0.237183446571958, None)\n",
      "alpha-mean 0.58\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20589.416749840362\n",
      "[0.58083876 0.57776197 0.579743   0.58041963 0.58074687 0.5813984\n",
      " 0.58229544 0.58090332]\n",
      "[[1.11788193 0.81453118 1.00980267 0.99991784 1.10667619 1.04036661\n",
      "  1.16868763 1.03070392]]\n",
      "{0: 605, 1: 1267}\n",
      "acc 0.3766025641025641\n",
      "(0.13417521704814522, 0.7083333333333334, 0.225613802256138, None)\n",
      "\n",
      "1 loss 20587.86212808639\n",
      "[0.58048551 0.57736515 0.57937384 0.58078968 0.58041355 0.58237386\n",
      " 0.58291371 0.58150959]\n",
      "[[1.11823751 0.81486114 1.0100681  0.99924858 1.10699532 1.03941343\n",
      "  1.16815254 1.03018312]]\n",
      "{0: 594, 1: 1278}\n",
      "acc 0.3717948717948718\n",
      "(0.13380281690140844, 0.7125, 0.22529644268774704, None)\n",
      "\n",
      "2 loss 20586.242784242284\n",
      "[0.58011935 0.57695606 0.57899199 0.58115804 0.58006559 0.58334908\n",
      " 0.58353941 0.58212378]\n",
      "[[1.11860463 0.81520234 1.01034541 0.99858022 1.10732827 1.03846058\n",
      "  1.16760605 1.02965056]]\n",
      "{0: 588, 1: 1284}\n",
      "acc 0.3696581196581197\n",
      "(0.13395638629283488, 0.7166666666666667, 0.22572178477690288, None)\n",
      "\n",
      "3 loss 20584.55653051028\n",
      "[0.57974018 0.57653464 0.57859746 0.5815248  0.57970391 0.58432419\n",
      " 0.58417272 0.58274585]\n",
      "[[1.11898333 0.81555485 1.01063466 0.99791257 1.10767506 1.03750785\n",
      "  1.16704815 1.02910621]]\n",
      "{0: 579, 1: 1293}\n",
      "acc 0.36485042735042733\n",
      "(0.13302397525135345, 0.7166666666666667, 0.2243966079582518, None)\n",
      "\n",
      "4 loss 20582.800579106344\n",
      "[0.57934791 0.5761008  0.57819028 0.58188992 0.57932794 0.58529915\n",
      " 0.58481362 0.58337563]\n",
      "[[1.11937367 0.81591875 1.01093598 0.99724565 1.10803575 1.03655529\n",
      "  1.16647882 1.02855006]]\n",
      "{0: 573, 1: 1299}\n",
      "acc 0.36164529914529914\n",
      "(0.13240954580446498, 0.7166666666666667, 0.22352176738141655, None)\n",
      "\n",
      "[0.57934791 0.5761008  0.57819028 0.58188992 0.57932794 0.58529915\n",
      " 0.58481362 0.58337563]\n",
      "[[1.11937367 0.81591875 1.01093598 0.99724565 1.10803575 1.03655529\n",
      "  1.16647882 1.02855006]]\n",
      "{0: 573, 1: 1299}\n",
      "acc 0.36164529914529914\n",
      "acc 0.36164529914529914\n",
      "[[ 505 1127]\n",
      " [  68  172]]\n",
      "(0.13240954580446498, 0.7166666666666667, 0.22352176738141655, None)\n",
      "alpha-mean 0.59\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20585.400222932578\n",
      "[0.59090288 0.58783178 0.58981419 0.59047376 0.59081926 0.59139665\n",
      " 0.59224234 0.590849  ]\n",
      "[[1.11781833 0.814464   1.00973435 0.999906   1.10660793 1.04036999\n",
      "  1.16876783 1.03078556]]\n",
      "{0: 683, 1: 1189}\n",
      "acc 0.4097222222222222\n",
      "(0.13624894869638352, 0.675, 0.22673198040587822, None)\n",
      "\n",
      "1 loss 20584.222557781814\n",
      "[0.59061721 0.58750681 0.58951836 0.59088675 0.59056055 0.59236955\n",
      " 0.59280663 0.59140001]\n",
      "[[1.11810824 0.81472492 1.00992808 0.99922423 1.10685597 1.03942196\n",
      "  1.16831482 1.03034836]]\n",
      "{0: 672, 1: 1200}\n",
      "acc 0.4049145299145299\n",
      "(0.13583333333333333, 0.6791666666666667, 0.2263888888888889, None)\n",
      "\n",
      "2 loss 20582.998595400688\n",
      "[0.59031986 0.58717054 0.58921091 0.59129832 0.59028856 0.59334222\n",
      " 0.59337834 0.5919587 ]\n",
      "[[1.11840874 0.81499606 1.01013241 0.9985433  1.10711662 1.03847423\n",
      "  1.16785083 1.02989987]]\n",
      "{0: 659, 1: 1213}\n",
      "acc 0.39903846153846156\n",
      "(0.13520197856553998, 0.6833333333333333, 0.22573984858912594, None)\n",
      "\n",
      "3 loss 20581.725745234653\n",
      "[0.59001053 0.5868228  0.58889167 0.59170866 0.5900031  0.59431481\n",
      " 0.59395747 0.5925251 ]\n",
      "[[1.11871994 0.81527757 1.01034746 0.99786304 1.10739001 1.03752658\n",
      "  1.16737571 1.02943994]]\n",
      "{0: 646, 1: 1226}\n",
      "acc 0.39316239316239315\n",
      "(0.13458401305057097, 0.6875, 0.22510231923601637, None)\n",
      "\n",
      "4 loss 20580.401311011778\n",
      "[0.58968901 0.58646344 0.58856046 0.59211773 0.58970397 0.59528729\n",
      " 0.5945441  0.59309928]\n",
      "[[1.11904199 0.81556959 1.01057344 0.99718347 1.10767631 1.03657903\n",
      "  1.16688934 1.02896844]]\n",
      "{0: 637, 1: 1235}\n",
      "acc 0.38835470085470086\n",
      "(0.13360323886639677, 0.6875, 0.22372881355932206, None)\n",
      "\n",
      "[0.58968901 0.58646344 0.58856046 0.59211773 0.58970397 0.59528729\n",
      " 0.5945441  0.59309928]\n",
      "[[1.11904199 0.81556959 1.01057344 0.99718347 1.10767631 1.03657903\n",
      "  1.16688934 1.02896844]]\n",
      "{0: 637, 1: 1235}\n",
      "acc 0.38835470085470086\n",
      "acc 0.38835470085470086\n",
      "[[ 562 1070]\n",
      " [  75  165]]\n",
      "(0.13360323886639677, 0.6875, 0.22372881355932206, None)\n",
      "alpha-mean 0.6\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20580.630500302086\n",
      "[0.60097827 0.59790639 0.5998904  0.60053532 0.60089577 0.60139461\n",
      " 0.60218203 0.60078749]\n",
      "[[1.11775041 0.81439293 1.00966378 0.99989442 1.1065355  1.04037393\n",
      "  1.16885854 1.03087742]]\n",
      "{0: 751, 1: 1121}\n",
      "acc 0.43643162393162394\n",
      "(0.13648528099910795, 0.6375, 0.22483468038207202, None)\n",
      "\n",
      "1 loss 20579.77009147968\n",
      "[0.6007707  0.59765879 0.59967356 0.60101271 0.60071643 0.60236449\n",
      " 0.60268451 0.60127542]\n",
      "[[1.11796968 0.81458048 1.00978291 0.99920048 1.10670749 1.0394319\n",
      "  1.16849901 1.03053491]]\n",
      "{0: 741, 1: 1131}\n",
      "acc 0.4310897435897436\n",
      "(0.13527851458885942, 0.6375, 0.22319474835886216, None)\n",
      "\n",
      "2 loss 20578.88072004359\n",
      "[0.60055311 0.59740151 0.59944682 0.60148889 0.60052589 0.60333415\n",
      " 0.60319375 0.60177033]\n",
      "[[1.11819803 0.81477668 1.0099108  0.99850732 1.10689022 1.03849019\n",
      "  1.16812976 1.03018245]]\n",
      "{0: 730, 1: 1142}\n",
      "acc 0.42628205128205127\n",
      "(0.13485113835376533, 0.6416666666666667, 0.22286541244573085, None)\n",
      "\n",
      "3 loss 20577.960021739724\n",
      "[0.60032525 0.59713433 0.59920998 0.60196405 0.60032389 0.60430373\n",
      " 0.60370979 0.60227229]\n",
      "[[1.11843561 0.81498172 1.01004765 0.99781478 1.10708388 1.03754853\n",
      "  1.16775058 1.02981983]]\n",
      "{0: 724, 1: 1148}\n",
      "acc 0.4252136752136752\n",
      "(0.13588850174216027, 0.65, 0.22478386167146971, None)\n",
      "\n",
      "4 loss 20577.006033191214\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "acc 0.41933760683760685\n",
      "[[ 629 1003]\n",
      " [  84  156]]\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "import numpy as np\n",
    "for i in np.linspace(0.5,0.6,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.6\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20580.630500302086\n",
      "[0.60097827 0.59790639 0.5998904  0.60053532 0.60089577 0.60139461\n",
      " 0.60218203 0.60078749]\n",
      "[[1.11775041 0.81439293 1.00966378 0.99989442 1.1065355  1.04037393\n",
      "  1.16885854 1.03087742]]\n",
      "{0: 751, 1: 1121}\n",
      "acc 0.43643162393162394\n",
      "(0.13648528099910795, 0.6375, 0.22483468038207202, None)\n",
      "\n",
      "1 loss 20579.77009147968\n",
      "[0.6007707  0.59765879 0.59967356 0.60101271 0.60071643 0.60236449\n",
      " 0.60268451 0.60127542]\n",
      "[[1.11796968 0.81458048 1.00978291 0.99920048 1.10670749 1.0394319\n",
      "  1.16849901 1.03053491]]\n",
      "{0: 741, 1: 1131}\n",
      "acc 0.4310897435897436\n",
      "(0.13527851458885942, 0.6375, 0.22319474835886216, None)\n",
      "\n",
      "2 loss 20578.88072004359\n",
      "[0.60055311 0.59740151 0.59944682 0.60148889 0.60052589 0.60333415\n",
      " 0.60319375 0.60177033]\n",
      "[[1.11819803 0.81477668 1.0099108  0.99850732 1.10689022 1.03849019\n",
      "  1.16812976 1.03018245]]\n",
      "{0: 730, 1: 1142}\n",
      "acc 0.42628205128205127\n",
      "(0.13485113835376533, 0.6416666666666667, 0.22286541244573085, None)\n",
      "\n",
      "3 loss 20577.960021739724\n",
      "[0.60032525 0.59713433 0.59920998 0.60196405 0.60032389 0.60430373\n",
      " 0.60370979 0.60227229]\n",
      "[[1.11843561 0.81498172 1.01004765 0.99781478 1.10708388 1.03754853\n",
      "  1.16775058 1.02981983]]\n",
      "{0: 724, 1: 1148}\n",
      "acc 0.4252136752136752\n",
      "(0.13588850174216027, 0.65, 0.22478386167146971, None)\n",
      "\n",
      "4 loss 20577.006033191214\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "acc 0.41933760683760685\n",
      "[[ 629 1003]\n",
      " [  84  156]]\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "alpha-mean 0.61\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20575.080939278756\n",
      "[0.61105863 0.60797888 0.60997059 0.61055416 0.61097432 0.61139219\n",
      " 0.61211373 0.61071808]\n",
      "[[1.11767892 0.81431887 1.0095921  0.999883   1.10645968 1.04037853\n",
      "  1.16895855 1.03097809]]\n",
      "{0: 805, 1: 1067}\n",
      "acc 0.45886752136752135\n",
      "(0.13776944704779756, 0.6125, 0.2249426166794185, None)\n",
      "\n",
      "1 loss 20574.475507727086\n",
      "[0.61093451 0.60781124 0.60983717 0.61106056 0.61087689 0.6123585\n",
      " 0.61254573 0.61113435]\n",
      "[[1.11782341 0.8144297  1.00963498 0.99917695 1.10655163 1.0394435\n",
      "  1.16870247 1.03073972]]\n",
      "{0: 802, 1: 1070}\n",
      "acc 0.45726495726495725\n",
      "(0.13738317757009347, 0.6125, 0.22442748091603054, None)\n",
      "\n",
      "2 loss 20573.85635432842\n",
      "[0.61080278 0.60763928 0.60969622 0.61156589 0.61077094 0.61332457\n",
      " 0.61298326 0.61155631]\n",
      "[[1.11797489 0.81454709 1.00968435 0.99847158 1.10665182 1.03850887\n",
      "  1.16843885 1.03049369]]\n",
      "{0: 798, 1: 1074}\n",
      "acc 0.4561965811965812\n",
      "(0.1378026070763501, 0.6166666666666667, 0.2252663622526636, None)\n",
      "\n",
      "3 loss 20573.22114037534\n",
      "[0.6106632  0.60745973 0.60954753 0.61207031 0.61065624 0.61429054\n",
      " 0.61342639 0.61198407]\n",
      "[[1.11813353 0.81467122 1.0097404  0.9977667  1.10676044 1.03757432\n",
      "  1.16816747 1.03023974]]\n",
      "{0: 793, 1: 1079}\n",
      "acc 0.4545940170940171\n",
      "(0.1380908248378128, 0.6208333333333333, 0.2259287338893101, None)\n",
      "\n",
      "4 loss 20572.56859078964\n",
      "[0.61051557 0.60727233 0.60939061 0.61257379 0.61053254 0.61525643\n",
      " 0.61387528 0.61241775]\n",
      "[[1.11829951 0.81480228 1.00980333 0.99706232 1.10687773 1.03663987\n",
      "  1.1678881  1.02997766]]\n",
      "{0: 790, 1: 1082}\n",
      "acc 0.452991452991453\n",
      "(0.1377079482439926, 0.6208333333333333, 0.22541603630862328, None)\n",
      "\n",
      "[0.61051557 0.60727233 0.60939061 0.61257379 0.61053254 0.61525643\n",
      " 0.61387528 0.61241775]\n",
      "[[1.11829951 0.81480228 1.00980333 0.99706232 1.10687773 1.03663987\n",
      "  1.1678881  1.02997766]]\n",
      "{0: 790, 1: 1082}\n",
      "acc 0.452991452991453\n",
      "acc 0.452991452991453\n",
      "[[699 933]\n",
      " [ 91 149]]\n",
      "(0.1377079482439926, 0.6208333333333333, 0.22541603630862328, None)\n",
      "alpha-mean 0.62\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20568.838028315382\n",
      "[0.62113996 0.61806306 0.62004547 0.62058457 0.62105609 0.62138935\n",
      " 0.62203734 0.62064077]\n",
      "[[1.11760472 0.81424266 1.00952029 0.99987111 1.10638161 1.04038388\n",
      "  1.16906496 1.03108454]]\n",
      "{0: 855, 1: 1017}\n",
      "acc 0.4813034188034188\n",
      "(0.14060963618485742, 0.5958333333333333, 0.2275258552108194, None)\n",
      "\n",
      "1 loss 20568.41265908879\n",
      "[0.62110046 0.61797965 0.61999135 0.62112619 0.62104391 0.62235146\n",
      " 0.62239007 0.62097677]\n",
      "[[1.11767126 0.81427447 1.00948648 0.99915256 1.1063908  1.03945701\n",
      "  1.16891907 1.03095641]]\n",
      "{0: 850, 1: 1022}\n",
      "acc 0.47863247863247865\n",
      "(0.13992172211350293, 0.5958333333333333, 0.22662440570522976, None)\n",
      "\n",
      "2 loss 20567.98324482279\n",
      "[0.62105616 0.61789149 0.61993245 0.621667   0.6210263  0.62331325\n",
      " 0.62274653 0.62131661]\n",
      "[[1.11774229 0.8143104  1.00945666 0.99843454 1.10640531 1.03853066\n",
      "  1.16876852 1.03082353]]\n",
      "{0: 849, 1: 1023}\n",
      "acc 0.4780982905982906\n",
      "(0.13978494623655913, 0.5958333333333333, 0.22644497228820265, None)\n",
      "\n",
      "3 loss 20567.548600148864\n",
      "[0.62100689 0.61779843 0.61986859 0.62220713 0.62100311 0.62427492\n",
      " 0.62310679 0.62166037]\n",
      "[[1.11781794 0.81435058 1.00943097 0.99771685 1.1064253  1.03760449\n",
      "  1.16861311 1.03068573]]\n",
      "{0: 844, 1: 1028}\n",
      "acc 0.4754273504273504\n",
      "(0.13910505836575876, 0.5958333333333333, 0.2255520504731861, None)\n",
      "\n",
      "4 loss 20567.108246207845\n",
      "[0.62095249 0.6177006  0.61979963 0.62274658 0.62097415 0.62523647\n",
      " 0.62347098 0.62200818]\n",
      "[[1.11789835 0.81439515 1.00940953 0.99699951 1.10645091 1.0366785\n",
      "  1.1684527  1.03054284]]\n",
      "{0: 844, 1: 1028}\n",
      "acc 0.4754273504273504\n",
      "(0.13910505836575876, 0.5958333333333333, 0.2255520504731861, None)\n",
      "\n",
      "[0.62095249 0.6177006  0.61979963 0.62274658 0.62097415 0.62523647\n",
      " 0.62347098 0.62200818]\n",
      "[[1.11789835 0.81439515 1.00940953 0.99699951 1.10645091 1.0366785\n",
      "  1.1684527  1.03054284]]\n",
      "{0: 844, 1: 1028}\n",
      "acc 0.4754273504273504\n",
      "acc 0.4754273504273504\n",
      "[[747 885]\n",
      " [ 97 143]]\n",
      "(0.13910505836575876, 0.5958333333333333, 0.2255520504731861, None)\n",
      "alpha-mean 0.63\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20561.84355157744\n",
      "[0.63122439 0.62814879 0.63012988 0.63059203 0.63113796 0.63138601\n",
      " 0.63195069 0.63055585]\n",
      "[[1.11752899 0.81416609 1.00944992 0.99985896 1.10630264 1.04039009\n",
      "  1.16917421 1.03119318]]\n",
      "{0: 906, 1: 966}\n",
      "acc 0.5032051282051282\n",
      "(0.14285714285714285, 0.575, 0.2288557213930348, None)\n",
      "\n",
      "1 loss 20561.524869074357\n",
      "[0.63127271 0.62815466 0.63016349 0.63114329 0.63121118 0.63234314\n",
      " 0.63221351 0.63080339]\n",
      "[[1.1175157  0.81411845 1.00934073 0.99912735 1.10622795 1.03947272\n",
      "  1.16914125 1.03117732]]\n",
      "{0: 905, 1: 967}\n",
      "acc 0.5026709401709402\n",
      "(0.14270941054808686, 0.575, 0.22866611433305717, None)\n",
      "\n",
      "2 loss 20561.207794809292\n",
      "[0.63131938 0.62815892 0.63019546 0.631694   0.63128236 0.63329987\n",
      " 0.63247761 0.63105224]\n",
      "[[1.11750402 0.81407218 1.00923284 0.99839599 1.1061553  1.03855606\n",
      "  1.16910693 1.03116005]]\n",
      "{0: 904, 1: 968}\n",
      "acc 0.5021367521367521\n",
      "(0.14256198347107438, 0.575, 0.22847682119205298, None)\n",
      "\n",
      "3 loss 20560.89190788444\n",
      "[0.63136437 0.62816151 0.63022573 0.63224432 0.63135144 0.63425638\n",
      " 0.63274302 0.63129998]\n",
      "[[1.11749399 0.81402732 1.00912631 0.99766464 1.10608474 1.03763975\n",
      "  1.16907118 1.03114132]]\n",
      "{0: 904, 1: 968}\n",
      "acc 0.5021367521367521\n",
      "(0.14256198347107438, 0.575, 0.22847682119205298, None)\n",
      "\n",
      "4 loss 20560.577178631967\n",
      "[0.6314076  0.62816238 0.63025425 0.63279428 0.63141839 0.6352127\n",
      " 0.63300977 0.63154941]\n",
      "[[1.11748565 0.81398392 1.00902118 0.99693328 1.10601634 1.03672379\n",
      "  1.16903396 1.03112106]]\n",
      "{0: 904, 1: 968}\n",
      "acc 0.5021367521367521\n",
      "(0.14256198347107438, 0.575, 0.22847682119205298, None)\n",
      "\n",
      "[0.6314076  0.62816238 0.63025425 0.63279428 0.63141839 0.6352127\n",
      " 0.63300977 0.63154941]\n",
      "[[1.11748565 0.81398392 1.00902118 0.99693328 1.10601634 1.03672379\n",
      "  1.16903396 1.03112106]]\n",
      "{0: 904, 1: 968}\n",
      "acc 0.5021367521367521\n",
      "acc 0.5021367521367521\n",
      "[[802 830]\n",
      " [102 138]]\n",
      "(0.14256198347107438, 0.575, 0.22847682119205298, None)\n",
      "alpha-mean 0.64\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20554.12271188835\n",
      "[0.64130798 0.63823484 0.64021444 0.64062866 0.64121967 0.6413821\n",
      " 0.64185967 0.6404622 ]\n",
      "[[1.11745318 0.81409086 1.00938251 0.99984293 1.10622432 1.04039728\n",
      "  1.16928186 1.03129962]]\n",
      "{0: 936, 1: 936}\n",
      "acc 0.5138888888888888\n",
      "(0.1420940170940171, 0.5541666666666667, 0.22619047619047622, None)\n",
      "\n",
      "1 loss 20553.82613887485\n",
      "[0.64144303 0.63833012 0.64033581 0.64120767 0.64137784 0.64233337\n",
      " 0.64202747 0.6406123 ]\n",
      "[[1.11735994 0.81396531 1.00920105 0.99909324 1.10606654 1.03949091\n",
      "  1.16935959 1.03139316]]\n",
      "{0: 936, 1: 936}\n",
      "acc 0.5138888888888888\n",
      "(0.1420940170940171, 0.5541666666666667, 0.22619047619047622, None)\n",
      "\n",
      "2 loss 20553.5313967521\n",
      "[0.64157954 0.63842692 0.64045866 0.64178635 0.64153724 0.6432841\n",
      " 0.64219365 0.64076079]\n",
      "[[1.11726534 0.81383838 1.00901833 0.99834351 1.10590752 1.03858551\n",
      "  1.16943913 1.03148846]]\n",
      "{0: 936, 1: 936}\n",
      "acc 0.5138888888888888\n",
      "(0.1420940170940171, 0.5541666666666667, 0.22619047619047622, None)\n",
      "\n",
      "3 loss 20553.237393059084\n",
      "[0.64171756 0.6385253  0.64058305 0.64236493 0.64169793 0.6442345\n",
      " 0.64235814 0.64090761]\n",
      "[[1.11716932 0.81371003 1.00883433 0.99759351 1.10574722 1.03768072\n",
      "  1.16952053 1.03158556]]\n",
      "{0: 937, 1: 935}\n",
      "acc 0.5144230769230769\n",
      "(0.14224598930481283, 0.5541666666666667, 0.22638297872340427, None)\n",
      "\n",
      "4 loss 20552.944088945682\n",
      "[0.64185716 0.6386253  0.64070901 0.64294342 0.64185996 0.64518458\n",
      " 0.64252072 0.64105272]\n",
      "[[1.11707185 0.81358021 1.00864901 0.99684324 1.1055856  1.03677652\n",
      "  1.16960383 1.03168452]]\n",
      "{0: 938, 1: 934}\n",
      "acc 0.5149572649572649\n",
      "(0.1423982869379015, 0.5541666666666667, 0.22657580919931855, None)\n",
      "\n",
      "[0.64185716 0.6386253  0.64070901 0.64294342 0.64185996 0.64518458\n",
      " 0.64252072 0.64105272]\n",
      "[[1.11707185 0.81358021 1.00864901 0.99684324 1.1055856  1.03677652\n",
      "  1.16960383 1.03168452]]\n",
      "{0: 938, 1: 934}\n",
      "acc 0.5149572649572649\n",
      "acc 0.5149572649572649\n",
      "[[831 801]\n",
      " [107 133]]\n",
      "(0.1423982869379015, 0.5541666666666667, 0.22657580919931855, None)\n",
      "alpha-mean 0.6499999999999999\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20545.67943162276\n",
      "[0.65139148 0.64831925 0.65029707 0.65069098 0.65129732 0.65137751\n",
      " 0.65175885 0.65036155]\n",
      "[[1.11737876 0.81401875 1.00931932 0.99982103 1.10614822 1.04040559\n",
      "  1.16938385 1.03140001]]\n",
      "{0: 994, 1: 878}\n",
      "acc 0.5288461538461539\n",
      "(0.13439635535307518, 0.49166666666666664, 0.2110912343470483, None)\n",
      "\n",
      "1 loss 20545.320006367234\n",
      "[0.65161286 0.64850167 0.65050378 0.65133658 0.65153579 0.6523219\n",
      " 0.65182176 0.65040697]\n",
      "[[1.11720714 0.8138188  1.00907014 0.99904663 1.10590994 1.03951189\n",
      "  1.16956567 1.03159597]]\n",
      "{0: 997, 1: 875}\n",
      "acc 0.530448717948718\n",
      "(0.13485714285714287, 0.49166666666666664, 0.21165919282511214, None)\n",
      "\n",
      "2 loss 20544.957096871505\n",
      "[0.65183852 0.64868852 0.65071486 0.65198217 0.65177844 0.65326558\n",
      " 0.65187996 0.65044769]\n",
      "[[1.11703132 0.81361494 1.00881741 0.99827192 1.10566739 1.03861953\n",
      "  1.16975194 1.03179631]]\n",
      "{0: 998, 1: 874}\n",
      "acc 0.530982905982906\n",
      "(0.13501144164759726, 0.49166666666666664, 0.21184919210053862, None)\n",
      "\n",
      "3 loss 20544.58912540647\n",
      "[0.65206859 0.64887993 0.6509304  0.65262799 0.65202542 0.65420875\n",
      " 0.65193328 0.65048354]\n",
      "[[1.11685118 0.81340705 1.00856106 0.99749669 1.10542047 1.0377281\n",
      "  1.16994278 1.03200113]]\n",
      "{0: 1001, 1: 871}\n",
      "acc 0.5325854700854701\n",
      "(0.1354764638346728, 0.49166666666666664, 0.21242124212421243, None)\n",
      "\n",
      "4 loss 20544.215526697655\n",
      "[0.6523038  0.64907605 0.65115053 0.65327404 0.65227737 0.65515141\n",
      " 0.65198157 0.65051437]\n",
      "[[1.11666657 0.81319503 1.00830099 0.99672091 1.10516905 1.03683761\n",
      "  1.17013832 1.03221057]]\n",
      "{0: 1004, 1: 868}\n",
      "acc 0.5331196581196581\n",
      "(0.1347926267281106, 0.4875, 0.2111913357400722, None)\n",
      "\n",
      "[0.6523038  0.64907605 0.65115053 0.65327404 0.65227737 0.65515141\n",
      " 0.65198157 0.65051437]\n",
      "[[1.11666657 0.81319503 1.00830099 0.99672091 1.10516905 1.03683761\n",
      "  1.17013832 1.03221057]]\n",
      "{0: 1004, 1: 868}\n",
      "acc 0.5331196581196581\n",
      "acc 0.5331196581196581\n",
      "[[881 751]\n",
      " [123 117]]\n",
      "(0.1347926267281106, 0.4875, 0.2111913357400722, None)\n",
      "alpha-mean 0.6599999999999999\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20536.41964999217\n",
      "[0.66147382 0.65840373 0.66037405 0.66075036 0.66137391 0.66137211\n",
      " 0.66166114 0.66026498]\n",
      "[[1.11730747 0.8139518  1.00926188 0.99980133 1.10607597 1.04041515\n",
      "  1.16947726 1.03149159]]\n",
      "{0: 1051, 1: 821}\n",
      "acc 0.5496794871794872\n",
      "(0.13276492082825822, 0.45416666666666666, 0.20546654099905748, None)\n",
      "\n",
      "1 loss 20535.91719703751\n",
      "[0.66177963 0.65867294 0.66065969 0.66146337 0.661691   0.66230842\n",
      " 0.66162268 0.66021024]\n",
      "[[1.11706109 0.8136832  1.00895128 0.99900477 1.10576175 1.03953604\n",
      "  1.16975352 1.03178013]]\n",
      "{0: 1053, 1: 819}\n",
      "acc 0.5507478632478633\n",
      "(0.1330891330891331, 0.45416666666666666, 0.20585457979225683, None)\n",
      "\n",
      "2 loss 20535.401561243834\n",
      "[0.66209189 0.65894888 0.66095194 0.66217654 0.66201477 0.66324377\n",
      " 0.66157664 0.66014794]\n",
      "[[1.1168081  0.81340867 1.0086353  0.99820795 1.10544082 1.03865869\n",
      "  1.17003606 1.03207484]]\n",
      "{0: 1057, 1: 815}\n",
      "acc 0.5528846153846154\n",
      "(0.13374233128834356, 0.45416666666666666, 0.2066350710900474, None)\n",
      "\n",
      "3 loss 20534.870515757593\n",
      "[0.66241074 0.65923171 0.66125095 0.66289008 0.66234521 0.66417839\n",
      " 0.66152279 0.66007786]\n",
      "[[1.11654836 0.81312806 1.00831388 0.99741063 1.10511304 1.03778269\n",
      "  1.17032502 1.03237586]]\n",
      "{0: 1059, 1: 813}\n",
      "acc 0.5539529914529915\n",
      "(0.13407134071340712, 0.45416666666666666, 0.20702754036087367, None)\n",
      "\n",
      "4 loss 20534.323130404315\n",
      "[0.66273635 0.65952162 0.66155688 0.66360304 0.66268247 0.66511228\n",
      " 0.66146092 0.65999979]\n",
      "[[1.11628167 0.81284121 1.00798687 0.99661281 1.10477825 1.03690806\n",
      "  1.17062051 1.0326833 ]]\n",
      "{0: 1061, 1: 811}\n",
      "acc 0.5550213675213675\n",
      "(0.1344019728729963, 0.45416666666666666, 0.20742150333016174, None)\n",
      "\n",
      "[0.66273635 0.65952162 0.66155688 0.66360304 0.66268247 0.66511228\n",
      " 0.66146092 0.65999979]\n",
      "[[1.11628167 0.81284121 1.00798687 0.99661281 1.10477825 1.03690806\n",
      "  1.17062051 1.0326833 ]]\n",
      "{0: 1061, 1: 811}\n",
      "acc 0.5550213675213675\n",
      "acc 0.5550213675213675\n",
      "[[930 702]\n",
      " [131 109]]\n",
      "(0.1344019728729963, 0.45416666666666666, 0.20742150333016174, None)\n",
      "alpha-mean 0.6699999999999999\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20526.328851641123\n",
      "[0.67154416 0.66847941 0.67045463 0.67077348 0.67144319 0.67136579\n",
      " 0.67156536 0.67017035]\n",
      "[[1.11724065 0.81389216 1.00921141 0.99978512 1.10600915 1.04042611\n",
      "  1.16956026 1.03157276]]\n",
      "{0: 1103, 1: 769}\n",
      "acc 0.5763888888888888\n",
      "(0.14044213263979194, 0.45, 0.21407333994053518, None)\n",
      "\n",
      "1 loss 20525.62161047148\n",
      "[0.67192148 0.66882609 0.67082186 0.67151572 0.67183063 0.67229259\n",
      " 0.67142826 0.67001821]\n",
      "[[1.11692459 0.8135629  1.00884701 0.99897002 1.10562532 1.0395637\n",
      "  1.16991963 1.03194256]]\n",
      "{0: 1109, 1: 763}\n",
      "acc 0.5785256410256411\n",
      "(0.1402359108781127, 0.44583333333333336, 0.21335992023928213, None)\n",
      "\n",
      "2 loss 20524.889204142186\n",
      "[0.67230678 0.66918073 0.67119717 0.67225829 0.67222615 0.67321815\n",
      " 0.67128127 0.66985624]\n",
      "[[1.1166001  0.81322629 1.00847598 0.99815464 1.10523303 1.03870356\n",
      "  1.17028626 1.0323195 ]]\n",
      "{0: 1114, 1: 758}\n",
      "acc 0.5801282051282052\n",
      "(0.13984168865435356, 0.44166666666666665, 0.2124248496993988, None)\n",
      "\n",
      "3 loss 20524.128748399846\n",
      "[0.67269971 0.66954389 0.67158062 0.67300138 0.67262991 0.6741427\n",
      " 0.67112417 0.66968425]\n",
      "[[1.11626703 0.81288221 1.00809828 0.99733876 1.10483219 1.03784525\n",
      "  1.17066026 1.03270368]]\n",
      "{0: 1123, 1: 749}\n",
      "acc 0.5838675213675214\n",
      "(0.14018691588785046, 0.4375, 0.21233569261880683, None)\n",
      "\n",
      "4 loss 20523.33853801939\n",
      "[0.67310072 0.66991569 0.67197244 0.67374521 0.67304202 0.67506621\n",
      " 0.67095675 0.66950201]\n",
      "[[1.11592522 0.81253052 1.00771378 0.99652236 1.10442266 1.03698882\n",
      "  1.17104171 1.03309518]]\n",
      "{0: 1133, 1: 739}\n",
      "acc 0.5881410256410257\n",
      "(0.14073071718538566, 0.43333333333333335, 0.21246169560776304, None)\n",
      "\n",
      "[0.67310072 0.66991569 0.67197244 0.67374521 0.67304202 0.67506621\n",
      " 0.67095675 0.66950201]\n",
      "[[1.11592522 0.81253052 1.00771378 0.99652236 1.10442266 1.03698882\n",
      "  1.17104171 1.03309518]]\n",
      "{0: 1133, 1: 739}\n",
      "acc 0.5881410256410257\n",
      "acc 0.5881410256410257\n",
      "[[997 635]\n",
      " [136 104]]\n",
      "(0.14073071718538566, 0.43333333333333335, 0.21246169560776304, None)\n",
      "alpha-mean 0.6799999999999999\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20515.472996084773\n",
      "[0.68160694 0.67854288 0.68052199 0.68079493 0.68150736 0.68135838\n",
      " 0.68147298 0.68007946]\n",
      "[[1.11717898 0.81384114 1.00916894 0.99977356 1.10594845 1.04043862\n",
      "  1.16963233 1.03164314]]\n",
      "{0: 1174, 1: 698}\n",
      "acc 0.6068376068376068\n",
      "(0.14469914040114612, 0.42083333333333334, 0.21535181236673773, None)\n",
      "\n",
      "1 loss 20514.51532114381\n",
      "[0.68204755 0.6789533  0.68095716 0.68155987 0.68195951 0.68227407\n",
      " 0.68124166 0.67983471]\n",
      "[[1.11679909 0.81346052 1.00875941 0.9989457  1.10550202 1.0395952\n",
      "  1.17006318 1.03208274]]\n",
      "{0: 1188, 1: 684}\n",
      "acc 0.6132478632478633\n",
      "(0.14619883040935672, 0.4166666666666667, 0.21645021645021642, None)\n",
      "\n",
      "2 loss 20513.51974618026\n",
      "[0.68249704 0.67937306 0.68140093 0.68232504 0.68242061 0.68318816\n",
      " 0.68099894 0.67957866]\n",
      "[[1.1164096  0.81307182 1.00834255 0.99811769 1.10504613 1.03875461\n",
      "  1.17050157 1.03252975]]\n",
      "{0: 1204, 1: 668}\n",
      "acc 0.6217948717948718\n",
      "(0.1497005988023952, 0.4166666666666667, 0.2202643171806167, None)\n",
      "\n",
      "3 loss 20512.48248927772\n",
      "[0.6829555  0.6798016  0.68185361 0.68309066 0.68289076 0.68410092\n",
      " 0.68074466 0.67931134]\n",
      "[[1.11601039 0.81267496 1.00791836 0.99728931 1.10458072 1.03791641\n",
      "  1.17094756 1.03298423]]\n",
      "{0: 1208, 1: 664}\n",
      "acc 0.6239316239316239\n",
      "(0.15060240963855423, 0.4166666666666667, 0.22123893805309733, None)\n",
      "\n",
      "4 loss 20511.402233889316\n",
      "[0.68342299 0.68023937 0.68231538 0.68385671 0.68336958 0.6850123\n",
      " 0.68047882 0.67903241]\n",
      "[[1.11560135 0.81226987 1.00748675 0.99646055 1.10410575 1.03708064\n",
      "  1.17140117 1.03344622]]\n",
      "{0: 1221, 1: 651}\n",
      "acc 0.6287393162393162\n",
      "(0.15053763440860216, 0.4083333333333333, 0.21997755331088667, None)\n",
      "\n",
      "[0.68342299 0.68023937 0.68231538 0.68385671 0.68336958 0.6850123\n",
      " 0.68047882 0.67903241]\n",
      "[[1.11560135 0.81226987 1.00748675 0.99646055 1.10410575 1.03708064\n",
      "  1.17140117 1.03344622]]\n",
      "{0: 1221, 1: 651}\n",
      "acc 0.6287393162393162\n",
      "acc 0.6287393162393162\n",
      "[[1079  553]\n",
      " [ 142   98]]\n",
      "(0.15053763440860216, 0.4083333333333333, 0.21997755331088667, None)\n",
      "alpha-mean 0.69\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20503.96435088804\n",
      "[0.69166433 0.68859535 0.69058171 0.6908075  0.69156313 0.69134975\n",
      " 0.69138706 0.68999502]\n",
      "[[1.11712324 0.81379975 1.00913533 0.99976534 1.10589424 1.04045282\n",
      "  1.16969382 1.03170314]]\n",
      "{0: 1255, 1: 617}\n",
      "acc 0.6458333333333334\n",
      "(0.15721231766612642, 0.4041666666666667, 0.2263710618436406, None)\n",
      "\n",
      "1 loss 20502.722307473967\n",
      "[0.69216236 0.68905899 0.69107667 0.69158611 0.69207172 0.69225247\n",
      " 0.69106898 0.68966505]\n",
      "[[1.11668617 0.81337805 1.00869021 0.99892818 1.1053925  1.03963086\n",
      "  1.17018519 1.03220179]]\n",
      "{0: 1263, 1: 609}\n",
      "acc 0.6501068376068376\n",
      "(0.1592775041050903, 0.4041666666666667, 0.22850412249705535, None)\n",
      "\n",
      "2 loss 20501.43067241286\n",
      "[0.69266963 0.68953155 0.69158013 0.69236435 0.69258945 0.6931532\n",
      " 0.69073875 0.68932315]\n",
      "[[1.11623891 0.81294817 1.0082376  0.99809133 1.10488088 1.03881235\n",
      "  1.17068388 1.03270766]]\n",
      "{0: 1275, 1: 597}\n",
      "acc 0.655448717948718\n",
      "(0.16080402010050251, 0.4, 0.22939068100358423, None)\n",
      "\n",
      "3 loss 20500.084750199883\n",
      "[0.69318616 0.69001412 0.69209281 0.69314237 0.69311654 0.69405221\n",
      " 0.69039632 0.68896915]\n",
      "[[1.11578142 0.81251012 1.00777756 0.9972546  1.10435945 1.03799681\n",
      "  1.17118992 1.03322077]]\n",
      "{0: 1286, 1: 586}\n",
      "acc 0.6613247863247863\n",
      "(0.16382252559726962, 0.4, 0.23244552058111378, None)\n",
      "\n",
      "4 loss 20498.682591985114\n",
      "[0.69371197 0.69050611 0.69261409 0.69392016 0.69365297 0.69494946\n",
      " 0.69004157 0.68860297]\n",
      "[[1.11531368 0.81206391 1.00731007 0.996418   1.1038282  1.03718429\n",
      "  1.17170329 1.03374109]]\n",
      "{0: 1296, 1: 576}\n",
      "acc 0.6655982905982906\n",
      "(0.16493055555555555, 0.3958333333333333, 0.23284313725490197, None)\n",
      "\n",
      "[0.69371197 0.69050611 0.69261409 0.69392016 0.69365297 0.69494946\n",
      " 0.69004157 0.68860297]\n",
      "[[1.11531368 0.81206391 1.00731007 0.996418   1.1038282  1.03718429\n",
      "  1.17170329 1.03374109]]\n",
      "{0: 1296, 1: 576}\n",
      "acc 0.6655982905982906\n",
      "acc 0.6655982905982906\n",
      "[[1151  481]\n",
      " [ 145   95]]\n",
      "(0.16493055555555555, 0.3958333333333333, 0.23284313725490197, None)\n",
      "alpha-mean 0.7\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20491.89484005952\n",
      "[0.70171324 0.69864685 0.70062745 0.70081443 0.70161249 0.7013397\n",
      " 0.70130827 0.69991784]\n",
      "[[1.11707412 0.81377001 1.00911163 0.99976108 1.10584689 1.04046883\n",
      "  1.16974557 1.03175364]]\n",
      "{0: 1319, 1: 553}\n",
      "acc 0.6746794871794872\n",
      "(0.16636528028933092, 0.38333333333333336, 0.23203026481715006, None)\n",
      "\n",
      "1 loss 20490.353355955398\n",
      "[0.7022599  0.69916111 0.70116915 0.7015996  0.70216931 0.70222736\n",
      " 0.70091162 0.69951094]\n",
      "[[1.11658712 0.81331962 1.00864149 0.99891966 1.10529742 1.03967097\n",
      "  1.17028754 1.03230168]]\n",
      "{0: 1330, 1: 542}\n",
      "acc 0.6794871794871795\n",
      "(0.16789667896678967, 0.37916666666666665, 0.23273657289002558, None)\n",
      "\n",
      "2 loss 20488.751850435427\n",
      "[0.70281562 0.69968474 0.70171976 0.70238411 0.70273541 0.70311257\n",
      " 0.70050276 0.699092  ]\n",
      "[[1.11608988 0.81286174 1.00816411 0.9980787  1.10473822 1.03887721\n",
      "  1.17083634 1.03285645]]\n",
      "{0: 1338, 1: 534}\n",
      "acc 0.6837606837606838\n",
      "(0.1704119850187266, 0.37916666666666665, 0.2351421188630491, None)\n",
      "\n",
      "3 loss 20487.085580849212\n",
      "[0.70337933 0.70021824 0.70227811 0.70316809 0.7033107  0.70399565\n",
      " 0.7000817  0.69866102]\n",
      "[[1.11558246 0.81239647 1.00767963 0.99723801 1.10416943 1.038087\n",
      "  1.17139194 1.03341792]]\n",
      "{0: 1358, 1: 514}\n",
      "acc 0.6944444444444444\n",
      "(0.17704280155642024, 0.37916666666666665, 0.24137931034482757, None)\n",
      "\n",
      "4 loss 20485.358787286325\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n",
      "\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "acc 0.6971153846153846\n",
      "[[1217  415]\n",
      " [ 152   88]]\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "import numpy as np\n",
    "for i in np.linspace(0.6,0.7,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 18360.82658841929\n",
      "[ 1.76799135e-04 -2.85665373e-03 -9.03101723e-04 -9.92460379e-04\n",
      "  6.45089183e-05  1.40033299e-03  2.69095251e-03  1.31093455e-03]\n",
      "[[1.11853327 0.81520651 1.01054947 1.00161913 1.10736541 1.04036003\n",
      "  1.1682123  1.03021431]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 18317.080739006153\n",
      "[-0.00082236 -0.00385599 -0.00190232 -0.00199169 -0.000936    0.00237687\n",
      "  0.00369006  0.00231   ]\n",
      "[[1.11952355 0.81619908 1.01154208 1.00261378 1.10835584 1.0393967\n",
      "  1.16721868 1.02922083]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 18273.334829808686\n",
      "[-0.00182155 -0.00485535 -0.00290156 -0.00299093 -0.0019366   0.00335225\n",
      "  0.00468916  0.00330906]\n",
      "[[1.12051388 0.8171917  1.01253475 1.00360846 1.10934644 1.0384349\n",
      "  1.16622512 1.02822742]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 18229.592625424426\n",
      "[-0.00282076 -0.00585472 -0.00390082 -0.0039902  -0.00293723  0.00432661\n",
      "  0.00568827  0.00430813]\n",
      "[[1.12150423 0.81818434 1.01352744 1.00460318 1.11033715 1.03747448\n",
      "  1.16523161 1.02723406]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 18185.855912969546\n",
      "[-0.00381997 -0.00685411 -0.0049001  -0.00498948 -0.0039379   0.00529991\n",
      "  0.00668739  0.00530721]\n",
      "[[1.12249461 0.81917701 1.01452015 1.00559792 1.11132797 1.03651546\n",
      "  1.16423816 1.02624075]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[-0.00381997 -0.00685411 -0.0049001  -0.00498948 -0.0039379   0.00529991\n",
      "  0.00668739  0.00530721]\n",
      "[[1.12249461 0.81917701 1.01452015 1.00559792 1.11132797 1.03651546\n",
      "  1.16423816 1.02624075]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 18936.46658980817\n",
      "[0.10017841 0.09714484 0.09909847 0.09900911 0.10006657 0.10139972\n",
      " 0.10268999 0.10130995]\n",
      "[[1.11852808 0.81520207 1.01054455 1.00161446 1.10736016 1.04036184\n",
      "  1.16821422 1.03021626]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 18897.673256851478\n",
      "[0.09918023 0.09614639 0.09810019 0.09801082 0.09906752 0.10237525\n",
      " 0.1036886  0.1023085 ]\n",
      "[[1.11951404 0.81619089 1.01153342 1.00260478 1.10834631 1.03940143\n",
      "  1.16722211 1.02922432]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 18858.81664842444\n",
      "[0.09818195 0.09514786 0.09710183 0.09701245 0.09806831 0.10334948\n",
      " 0.10468725 0.10330709]\n",
      "[[1.12050014 0.81717983 1.01252243 1.00359523 1.10933272 1.03844286\n",
      "  1.16623005 1.02823244]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 18819.90564977269\n",
      "[0.09718361 0.09414927 0.09610341 0.09601402 0.097069   0.10432253\n",
      " 0.10568593 0.10430572]\n",
      "[[1.12148633 0.81816886 1.01351152 1.00458579 1.11031933 1.03748599\n",
      "  1.16523806 1.02724062]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 18780.942744425854\n",
      "[0.09618522 0.09315062 0.09510493 0.09501553 0.0960696  0.10529435\n",
      " 0.10668465 0.10530438]\n",
      "[[1.12247262 0.81915796 1.0145007  1.00557646 1.11130613 1.03653085\n",
      "  1.16424613 1.02624886]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.09618522 0.09315062 0.09510493 0.09501553 0.0960696  0.10529435\n",
      " 0.10668465 0.10530438]\n",
      "[[1.12247262 0.81915796 1.0145007  1.00557646 1.11130613 1.03653085\n",
      "  1.16424613 1.02624886]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 19488.61017684833\n",
      "[0.20018295 0.19714906 0.19910289 0.19901351 0.20007181 0.20139982\n",
      " 0.20268724 0.20130713]\n",
      "[[1.11851804 0.81519297 1.01053458 1.00160221 1.10735007 1.04036237\n",
      "  1.16821767 1.03021978]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "1 loss 19455.76217108926\n",
      "[0.19918811 0.19615369 0.19810786 0.19801847 0.1990768  0.20237546\n",
      " 0.20368396 0.20230373]\n",
      "[[1.1194954  0.81617391 1.01151546 1.00258077 1.10832776 1.03940298\n",
      "  1.16722817 1.0292305 ]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "2 loss 19422.77853029775\n",
      "[0.19819302 0.1951581  0.19711259 0.19702319 0.19808142 0.20334963\n",
      " 0.20468082 0.20330047]\n",
      "[[1.12047309 0.81715515 1.01249667 1.00355975 1.10930596 1.03844577\n",
      "  1.1662387  1.02824125]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "3 loss 19389.67393310329\n",
      "[0.19719774 0.19416232 0.19611714 0.19602773 0.19708577 0.20432247\n",
      " 0.20567778 0.20429732]\n",
      "[[1.12145105 0.81813663 1.01347814 1.00453909 1.11028454 1.03749062\n",
      "  1.16524928 1.02725205]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "4 loss 19356.451496980942\n",
      "[0.19620228 0.19316638 0.19512151 0.19503208 0.19608987 0.20529391\n",
      " 0.20667484 0.20529428]\n",
      "[[1.12242926 0.81911835 1.01445985 1.00551878 1.11126351 1.03653758\n",
      "  1.16425991 1.02626291]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "\n",
      "[0.19620228 0.19316638 0.19512151 0.19503208 0.19608987 0.20529391\n",
      " 0.20667484 0.20529428]\n",
      "[[1.12242926 0.81911835 1.01445985 1.00551878 1.11126351 1.03653758\n",
      "  1.16425991 1.02626291]]\n",
      "{0: 2, 1: 1870}\n",
      "acc 0.12927350427350429\n",
      "acc 0.12927350427350429\n",
      "[[   2 1630]\n",
      " [   0  240]]\n",
      "(0.12834224598930483, 1.0, 0.2274881516587678, None)\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 19982.189070065207\n",
      "[0.30019604 0.29716124 0.29911565 0.29902625 0.30008628 0.30140113\n",
      " 0.30267928 0.30129895]\n",
      "[[1.11849626 0.81517219 1.01051173 1.00156253 1.10732819 1.04036057\n",
      "  1.16822518 1.03022745]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "1 loss 19956.63672050369\n",
      "[0.29921208 0.29617595 0.2981312  0.29804177 0.29910341 0.30237876\n",
      " 0.30366965 0.30228903]\n",
      "[[1.11945429 0.81613448 1.01147326 1.00250153 1.10828684 1.03939876\n",
      "  1.1672415  1.02924414]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "2 loss 19930.874436296574\n",
      "[0.29822742 0.29519    0.29714607 0.29705661 0.2981196  0.30335484\n",
      " 0.30466041 0.3032795 ]\n",
      "[[1.12041315 0.81709757 1.01243567 1.00344198 1.10924659 1.0384394\n",
      "  1.16625771 1.0282607 ]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "3 loss 19904.921933029327\n",
      "[0.29724216 0.2942035  0.29616037 0.29607087 0.297135   0.30432947\n",
      " 0.30565149 0.30427031]\n",
      "[[1.12137275 0.81806134 1.01339882 1.00438375 1.11020729 1.0374824\n",
      "  1.16527386 1.02727719]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "4 loss 19878.782531169847\n",
      "[0.29625633 0.29321648 0.29517411 0.29508458 0.29614966 0.30530257\n",
      " 0.30664289 0.30526144]\n",
      "[[1.12233304 0.81902578 1.01436267 1.00532681 1.11116889 1.03652784\n",
      "  1.16428996 1.02629364]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "\n",
      "[0.29625633 0.29321648 0.29517411 0.29508458 0.29614966 0.30530257\n",
      " 0.30664289 0.30526144]\n",
      "[[1.12233304 0.81902578 1.01436267 1.00532681 1.11116889 1.03652784\n",
      "  1.16428996 1.02629364]]\n",
      "{0: 4, 1: 1868}\n",
      "acc 0.13034188034188035\n",
      "acc 0.13034188034188035\n",
      "[[   4 1628]\n",
      " [   0  240]]\n",
      "(0.1284796573875803, 1.0, 0.22770398481973433, None)\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20366.773019683744\n",
      "[0.40023972 0.39720168 0.39915814 0.39906866 0.40013367 0.40140369\n",
      " 0.40265366 0.40127261]\n",
      "[[1.11843997 0.81511603 1.01044888 1.00135797 1.10727105 1.0403561\n",
      "  1.16824673 1.03024957]]\n",
      "{0: 8, 1: 1864}\n",
      "acc 0.13247863247863248\n",
      "(0.12875536480686695, 1.0, 0.22813688212927757, None)\n",
      "\n",
      "1 loss 20350.046107880175\n",
      "[0.39929502 0.3962526  0.39821183 0.39812226 0.39919338 0.40238546\n",
      " 0.40362157 0.40223955]\n",
      "[[1.11934625 0.81602613 1.01135429 1.00208784 1.10817797 1.03938768\n",
      "  1.16728112 1.02928484]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "2 loss 20333.05449813594\n",
      "[0.39834796 0.39530136 0.39726324 0.39717357 0.3982501  0.40336582\n",
      " 0.4045907  0.40320776]\n",
      "[[1.120255   0.81693864 1.01226241 1.00282517 1.10908793 1.03842163\n",
      "  1.16631472 1.02831927]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "3 loss 20315.818639301655\n",
      "[0.39739877 0.39434818 0.39631259 0.39622284 0.39730414 0.40434483\n",
      " 0.40556093 0.40417711]\n",
      "[[1.12116601 0.81785333 1.01317295 1.00356971 1.11000063 1.03745789\n",
      "  1.16534765 1.02735302]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "4 loss 20298.340970965535\n",
      "[0.39644755 0.39339314 0.39535998 0.39527014 0.39635563 0.4053224\n",
      " 0.40653219 0.40514754]\n",
      "[[1.12207919 0.81877011 1.01408582 1.00432124 1.11091594 1.03649662\n",
      "  1.16437998 1.02638613]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "[0.39644755 0.39339314 0.39535998 0.39527014 0.39635563 0.4053224\n",
      " 0.40653219 0.40514754]\n",
      "[[1.12207919 0.81877011 1.01408582 1.00432124 1.11091594 1.03649662\n",
      "  1.16437998 1.02638613]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "acc 0.13194444444444445\n",
      "[[   7 1625]\n",
      " [   0  240]]\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20583.92165970571\n",
      "[0.50045124 0.49738986 0.49935774 0.49926707 0.50036045 0.50140421\n",
      " 0.50253896 0.50115409]\n",
      "[[1.11823991 0.8149103  1.01021416 1.00014502 1.10706126 1.04035505\n",
      "  1.16835285 1.03035933]]\n",
      "{0: 117, 1: 1755}\n",
      "acc 0.1842948717948718\n",
      "(0.13333333333333333, 0.975, 0.23458646616541354, None)\n",
      "\n",
      "1 loss 20577.235033002056\n",
      "[0.49970622 0.49661987 0.49860093 0.49850972 0.49963278 0.50238814\n",
      " 0.50339857 0.50200908]\n",
      "[[1.11895534 0.81562229 1.01089697 0.9997101  1.10776935 1.0393838\n",
      "  1.16748495 1.02949577]]\n",
      "{0: 110, 1: 1762}\n",
      "acc 0.18055555555555555\n",
      "(0.13280363223609534, 0.975, 0.23376623376623373, None)\n",
      "\n",
      "2 loss 20570.30070905498\n",
      "[0.49894681 0.49584067 0.49783434 0.49774254 0.49888939 0.50337135\n",
      " 0.5042628  0.50286888]\n",
      "[[1.11967984 0.8163432  1.01158999 0.99928469 1.10848846 1.03841372\n",
      "  1.16661175 1.02862666]]\n",
      "{0: 94, 1: 1778}\n",
      "acc 0.172008547008547\n",
      "(0.13160854893138357, 0.975, 0.2319127849355798, None)\n",
      "\n",
      "3 loss 20563.14033615703\n",
      "[0.49817784 0.49505272 0.49705841 0.49696599 0.49813437 0.5043539\n",
      " 0.50513137 0.50373323]\n",
      "[[1.12041298 0.8170726  1.0122927  0.9988691  1.10921796 1.03744477\n",
      "  1.16573362 1.0277524 ]]\n",
      "{0: 79, 1: 1793}\n",
      "acc 0.1639957264957265\n",
      "(0.1305075292805354, 0.975, 0.23020167240531234, None)\n",
      "\n",
      "4 loss 20555.755966247747\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "\n",
      "[0.49739957 0.49425633 0.4962735  0.49618042 0.49736819 0.50533571\n",
      " 0.50600414 0.50460196]\n",
      "[[1.12115443 0.81781021 1.01300478 0.99846374 1.10995741 1.0364771\n",
      "  1.16485079 1.02687324]]\n",
      "{0: 78, 1: 1794}\n",
      "acc 0.16346153846153846\n",
      "acc 0.16346153846153846\n",
      "[[  72 1560]\n",
      " [   6  234]]\n",
      "(0.13043478260869565, 0.975, 0.2300884955752212, None)\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20580.630500302086\n",
      "[0.60097827 0.59790639 0.5998904  0.60053532 0.60089577 0.60139461\n",
      " 0.60218203 0.60078749]\n",
      "[[1.11775041 0.81439293 1.00966378 0.99989442 1.1065355  1.04037393\n",
      "  1.16885854 1.03087742]]\n",
      "{0: 751, 1: 1121}\n",
      "acc 0.43643162393162394\n",
      "(0.13648528099910795, 0.6375, 0.22483468038207202, None)\n",
      "\n",
      "1 loss 20579.77009147968\n",
      "[0.6007707  0.59765879 0.59967356 0.60101271 0.60071643 0.60236449\n",
      " 0.60268451 0.60127542]\n",
      "[[1.11796968 0.81458048 1.00978291 0.99920048 1.10670749 1.0394319\n",
      "  1.16849901 1.03053491]]\n",
      "{0: 741, 1: 1131}\n",
      "acc 0.4310897435897436\n",
      "(0.13527851458885942, 0.6375, 0.22319474835886216, None)\n",
      "\n",
      "2 loss 20578.88072004359\n",
      "[0.60055311 0.59740151 0.59944682 0.60148889 0.60052589 0.60333415\n",
      " 0.60319375 0.60177033]\n",
      "[[1.11819803 0.81477668 1.0099108  0.99850732 1.10689022 1.03849019\n",
      "  1.16812976 1.03018245]]\n",
      "{0: 730, 1: 1142}\n",
      "acc 0.42628205128205127\n",
      "(0.13485113835376533, 0.6416666666666667, 0.22286541244573085, None)\n",
      "\n",
      "3 loss 20577.960021739724\n",
      "[0.60032525 0.59713433 0.59920998 0.60196405 0.60032389 0.60430373\n",
      " 0.60370979 0.60227229]\n",
      "[[1.11843561 0.81498172 1.01004765 0.99781478 1.10708388 1.03754853\n",
      "  1.16775058 1.02981983]]\n",
      "{0: 724, 1: 1148}\n",
      "acc 0.4252136752136752\n",
      "(0.13588850174216027, 0.65, 0.22478386167146971, None)\n",
      "\n",
      "4 loss 20577.00603319121\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "\n",
      "[0.6000869  0.59685704 0.59896279 0.60243819 0.60011019 0.60527323\n",
      " 0.60423278 0.60278142]\n",
      "[[1.11868261 0.8151958  1.01019367 0.99712286 1.10728869 1.03660695\n",
      "  1.16736125 1.02944683]]\n",
      "{0: 713, 1: 1159}\n",
      "acc 0.41933760683760685\n",
      "acc 0.41933760683760685\n",
      "[[ 629 1003]\n",
      " [  84  156]]\n",
      "(0.13459879206212252, 0.65, 0.22301644031451037, None)\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20491.89484005952\n",
      "[0.70171324 0.69864685 0.70062745 0.70081443 0.70161249 0.7013397\n",
      " 0.70130827 0.69991784]\n",
      "[[1.11707412 0.81377001 1.00911163 0.99976108 1.10584689 1.04046883\n",
      "  1.16974557 1.03175364]]\n",
      "{0: 1319, 1: 553}\n",
      "acc 0.6746794871794872\n",
      "(0.16636528028933092, 0.38333333333333336, 0.23203026481715006, None)\n",
      "\n",
      "1 loss 20490.3533559554\n",
      "[0.7022599  0.69916111 0.70116915 0.7015996  0.70216931 0.70222736\n",
      " 0.70091162 0.69951094]\n",
      "[[1.11658712 0.81331962 1.00864149 0.99891966 1.10529742 1.03967097\n",
      "  1.17028754 1.03230168]]\n",
      "{0: 1330, 1: 542}\n",
      "acc 0.6794871794871795\n",
      "(0.16789667896678967, 0.37916666666666665, 0.23273657289002558, None)\n",
      "\n",
      "2 loss 20488.751850435427\n",
      "[0.70281562 0.69968474 0.70171976 0.70238411 0.70273541 0.70311257\n",
      " 0.70050276 0.699092  ]\n",
      "[[1.11608988 0.81286174 1.00816411 0.9980787  1.10473822 1.03887721\n",
      "  1.17083634 1.03285645]]\n",
      "{0: 1338, 1: 534}\n",
      "acc 0.6837606837606838\n",
      "(0.1704119850187266, 0.37916666666666665, 0.2351421188630491, None)\n",
      "\n",
      "3 loss 20487.085580849205\n",
      "[0.70337933 0.70021824 0.70227811 0.70316809 0.7033107  0.70399565\n",
      " 0.7000817  0.69866102]\n",
      "[[1.11558246 0.81239647 1.00767963 0.99723801 1.10416943 1.038087\n",
      "  1.17139194 1.03341792]]\n",
      "{0: 1358, 1: 514}\n",
      "acc 0.6944444444444444\n",
      "(0.17704280155642024, 0.37916666666666665, 0.24137931034482757, None)\n",
      "\n",
      "4 loss 20485.35878728632\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n",
      "\n",
      "[0.70395006 0.70076047 0.70284371 0.70395153 0.7038951  0.70487653\n",
      " 0.69964848 0.69821808]\n",
      "[[1.1150649  0.81192395 1.00718807 0.99639759 1.10359117 1.03730042\n",
      "  1.17195427 1.03398604]]\n",
      "{0: 1369, 1: 503}\n",
      "acc 0.6971153846153846\n",
      "acc 0.6971153846153846\n",
      "[[1217  415]\n",
      " [ 152   88]]\n",
      "(0.1749502982107356, 0.36666666666666664, 0.23687752355316283, None)\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20416.11477267166\n",
      "[0.80177876 0.79841384 0.80034635 0.80086779 0.80166794 0.80119559\n",
      " 0.80097821 0.79959615]\n",
      "[[1.11708161 0.81399095 1.00936536 0.99972003 1.10587144 1.04065528\n",
      "  1.16991253 1.03191626]]\n",
      "{0: 1596, 1: 276}\n",
      "acc 0.7767094017094017\n",
      "(0.17753623188405798, 0.20416666666666666, 0.18992248062015504, None)\n",
      "\n",
      "1 loss 20414.244143426273\n",
      "[0.80239279 0.79868839 0.80060122 0.80170315 0.80226859 0.80186248\n",
      " 0.80026426 0.79888019]\n",
      "[[1.11660934 0.81376312 1.00916489 0.99883219 1.10535989 1.04013851\n",
      "  1.17061714 1.03262275]]\n",
      "{0: 1603, 1: 269}\n",
      "acc 0.780448717948718\n",
      "(0.1821561338289963, 0.20416666666666666, 0.19253438113948923, None)\n",
      "\n",
      "2 loss 20412.362813009768\n",
      "[0.80301028 0.79896271 0.8008549  0.80253786 0.80286907 0.80252403\n",
      " 0.79954715 0.79816412]\n",
      "[[1.11613539 0.81353481 1.0089634  0.99794452 1.10484758 1.03962777\n",
      "  1.17132341 1.03333089]]\n",
      "{0: 1609, 1: 263}\n",
      "acc 0.7836538461538461\n",
      "(0.18631178707224336, 0.20416666666666666, 0.194831013916501, None)\n",
      "\n",
      "3 loss 20410.456859340014\n",
      "[0.80362912 0.79923582 0.80110915 0.80337217 0.80347255 0.80318104\n",
      " 0.79882679 0.79744528]\n",
      "[[1.11565994 0.81330614 1.00876096 0.99705684 1.10433462 1.03912213\n",
      "  1.17203133 1.03404068]]\n",
      "{0: 1614, 1: 258}\n",
      "acc 0.7852564102564102\n",
      "(0.18604651162790697, 0.2, 0.19277108433734938, None)\n",
      "\n",
      "4 loss 20408.5262260676\n",
      "[0.80425134 0.79950939 0.80136351 0.80420606 0.80407753 0.8038335\n",
      " 0.7981063  0.79672311]\n",
      "[[1.11518315 0.81307713 1.00855762 0.99616913 1.10382115 1.03862156\n",
      "  1.17274088 1.03475209]]\n",
      "{0: 1621, 1: 251}\n",
      "acc 0.7889957264957265\n",
      "(0.19123505976095617, 0.2, 0.19551934826883913, None)\n",
      "\n",
      "[0.80425134 0.79950939 0.80136351 0.80420606 0.80407753 0.8038335\n",
      " 0.7981063  0.79672311]\n",
      "[[1.11518315 0.81307713 1.00855762 0.99616913 1.10382115 1.03862156\n",
      "  1.17274088 1.03475209]]\n",
      "{0: 1621, 1: 251}\n",
      "acc 0.7889957264957265\n",
      "acc 0.7889957264957265\n",
      "[[1429  203]\n",
      " [ 192   48]]\n",
      "(0.19123505976095617, 0.2, 0.19551934826883913, None)\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20481.7534573971\n",
      "[0.90133331 0.89832628 0.90021453 0.90094761 0.9012611  0.90101194\n",
      " 0.90101186 0.89963158]\n",
      "[[1.11746069 0.81405708 1.00947687 0.99963875 1.10625051 1.04082387\n",
      "  1.16988746 1.03189007]]\n",
      "{0: 1743, 1: 129}\n",
      "acc 0.8317307692307693\n",
      "(0.20930232558139536, 0.1125, 0.14634146341463414, None)\n",
      "\n",
      "1 loss 20481.16357573364\n",
      "[0.90149226 0.89851361 0.90033706 0.9018821  0.90145401 0.9013798\n",
      " 0.90033358 0.89895234]\n",
      "[[1.11737236 0.81389664 1.00939196 0.99865592 1.10612373 1.04057069\n",
      "  1.17056664 1.03257002]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "2 loss 20480.567918255358\n",
      "[0.90165278 0.8987009  0.90045996 0.90281836 0.90164796 0.90174282\n",
      " 0.89965397 0.89827319]\n",
      "[[1.11728195 0.81373573 1.00930624 0.99767268 1.10599554 1.04032152\n",
      "  1.17124635 1.0332505 ]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "3 loss 20479.96679819928\n",
      "[0.90181488 0.89888808 0.90058323 0.90375661 0.90184298 0.90210194\n",
      " 0.89897299 0.89759503]\n",
      "[[1.11718946 0.81357443 1.0092197  0.99668882 1.10586598 1.0400756\n",
      "  1.17192655 1.03393148]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "4 loss 20479.359065684752\n",
      "[0.90197856 0.89907516 0.90070687 0.90469686 0.90203905 0.90245721\n",
      " 0.8982917  0.89691552]\n",
      "[[1.11709492 0.81341274 1.00913233 0.99570438 1.10573508 1.0398329\n",
      "  1.17260724 1.03461294]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "\n",
      "[0.90197856 0.89907516 0.90070687 0.90469686 0.90203905 0.90245721\n",
      " 0.8982917  0.89691552]\n",
      "[[1.11709492 0.81341274 1.00913233 0.99570438 1.10573508 1.0398329\n",
      "  1.17260724 1.03461294]]\n",
      "{0: 1745, 1: 127}\n",
      "acc 0.8327991452991453\n",
      "acc 0.8327991452991453\n",
      "[[1532  100]\n",
      " [ 213   27]]\n",
      "(0.2125984251968504, 0.1125, 0.14713896457765668, None)\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 8), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 8), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 8), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 8), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(8,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(8,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(8,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 20516.768699937562\n",
      "[1.00236167 0.99893892 1.00099994 1.0000077  1.00226032 1.00167774\n",
      " 1.00283782 1.00156836]\n",
      "[[1.11753383 0.81411864 1.00955677 1.00062431 1.10634892 1.04135482\n",
      "  1.16989014 1.03189198]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "1 loss 20516.76817400778\n",
      "[1.00357064 0.99949902 1.00240177 1.00001264 1.0034769  1.00295744\n",
      " 1.00401409 1.0028639 ]\n",
      "[[1.11752122 0.81401947 1.00955653 1.00062431 1.10632061 1.04136795\n",
      "  1.17057034 1.03257215]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "2 loss 20516.767601123014\n",
      "[1.00471708 0.99981021 1.00361256 1.00003389 1.00462641 1.00413127\n",
      " 1.00514486 1.00404328]\n",
      "[[1.11750872 0.81392017 1.00955639 1.00062431 1.10629254 1.04138195\n",
      "  1.17125056 1.03325233]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "3 loss 20516.766965825198\n",
      "[1.00582915 0.99991273 1.00475688 1.00048536 1.00573998 1.00525779\n",
      " 1.00624771 1.00517275]\n",
      "[[1.1174965  0.81382077 1.00955654 1.00062449 1.10626496 1.04139759\n",
      "  1.17193088 1.03393257]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "4 loss 20516.766252634578\n",
      "[1.00692028 0.99993241 1.00586764 1.00200698 1.00683198 1.00635791\n",
      " 1.00733278 1.00627479]\n",
      "[[1.11748479 0.81372128 1.00955723 1.00067032 1.10623817 1.04141579\n",
      "  1.17261133 1.03461289]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n",
      "\n",
      "[1.00692028 0.99993241 1.00586764 1.00200698 1.00683198 1.00635791\n",
      " 1.00733278 1.00627479]\n",
      "[[1.11748479 0.81372128 1.00955723 1.00067032 1.10623817 1.04141579\n",
      "  1.17261133 1.03461289]]\n",
      "{0: 1766, 1: 106}\n",
      "acc 0.844017094017094\n",
      "acc 0.844017094017094\n",
      "[[1553   79]\n",
      " [ 213   27]]\n",
      "(0.25471698113207547, 0.1125, 0.15606936416184972, None)\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "for i in np.linspace(0,1,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 36272.053384859966\n",
      "[ 0.00096819 -0.00246081 -0.00022373 -0.00023594  0.00193544  0.00139114\n",
      "  0.00257035  0.00130007  0.00046464 -0.00035151 -0.00116307 -0.0003326\n",
      " -0.00117648  0.00176462  0.00140698  0.00142122]\n",
      "[[1.1177623  0.81481647 1.00988389 1.00087171 1.1055609  1.04037169\n",
      "  1.16837987 1.03022449 1.14732371 1.06571468 0.98456076 1.06760853\n",
      "  0.98337146 1.07761299 1.03982016 1.0412438 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 36220.222009824654\n",
      "[ 0.00077152 -0.00306429 -0.00053328 -0.00046991  0.00280593  0.00235019\n",
      "  0.00342943  0.00226625 -0.00053377 -0.00134997 -0.00216158 -0.00133106\n",
      " -0.0021776   0.00272547  0.00240545  0.00241968]\n",
      "[[1.11797456 0.81542123 1.01020277 1.00111604 1.10474283 1.03943018\n",
      "  1.16757476 1.02927183 1.14831355 1.0667059  0.9855528  1.06860199\n",
      "  0.98436336 1.07666858 1.03882848 1.04025212]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 36168.73391918599\n",
      "[ 0.00058591 -0.00366311 -0.00083354 -0.00069318  0.00367051  0.00330647\n",
      "  0.00428227  0.00323015 -0.00153218 -0.00234842 -0.00316008 -0.00232951\n",
      " -0.0031788   0.00368373  0.00340389  0.00341812]\n",
      "[[1.11818103 0.81602357 1.0105168  1.0013548  1.10392999 1.03849163\n",
      "  1.16677518 1.02832158 1.14930338 1.06769711 0.98654484 1.06959542\n",
      "  0.98535542 1.07572696 1.03783687 1.03926051]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 36117.56060794781\n",
      "[ 0.00041137 -0.00425722 -0.00112447 -0.00090571  0.00452896  0.00426011\n",
      "  0.00512886  0.00419204 -0.00253059 -0.00334688 -0.00415859 -0.00332797\n",
      " -0.00418002  0.00463952  0.00440234  0.00441658]\n",
      "[[1.11838173 0.81662346 1.010826   1.001588   1.10312251 1.03755584\n",
      "  1.16598095 1.02737338 1.15029319 1.06868832 0.98753688 1.07058884\n",
      "  0.98634759 1.07478791 1.0368453  1.03826894]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 36066.69894879077\n",
      "[ 0.0002479  -0.00484658 -0.00140603 -0.00110747  0.00538094  0.00521093\n",
      "  0.00596889  0.00515177 -0.00352902 -0.00434536 -0.00515712 -0.00432645\n",
      " -0.00518125  0.00559269  0.0054008   0.00541504]\n",
      "[[1.11857667 0.81722089 1.01113035 1.00181562 1.10232054 1.03662293\n",
      "  1.16519222 1.02642734 1.151283   1.06967952 0.98852892 1.07158226\n",
      "  0.98733987 1.07385155 1.03585377 1.03727741]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[ 0.0002479  -0.00484658 -0.00140603 -0.00110747  0.00538094  0.00521093\n",
      "  0.00596889  0.00515177 -0.00352902 -0.00434536 -0.00515712 -0.00432645\n",
      " -0.00518125  0.00559269  0.0054008   0.00541504]\n",
      "[[1.11857667 0.81722089 1.01113035 1.00181562 1.10232054 1.03662293\n",
      "  1.16519222 1.02642734 1.151283   1.06967952 0.98852892 1.07158226\n",
      "  0.98733987 1.07385155 1.03585377 1.03727741]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 37001.80337293428\n",
      "[0.1009594  0.09753896 0.09977131 0.09975826 0.10190758 0.10138807\n",
      " 0.10254439 0.10129883 0.10046577 0.0996496  0.09883802 0.09966851\n",
      " 0.09882508 0.10176176 0.1014063  0.10142053]\n",
      "[[1.11778627 0.81482251 1.00990119 1.00089162 1.10560216 1.04037828\n",
      "  1.16841878 1.03022762 1.14731924 1.06571036 0.98455647 1.06760411\n",
      "  0.98336709 1.07761919 1.03982311 1.04124675]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 36954.36754660866\n",
      "[0.10075163 0.09693414 0.09945476 0.09951616 0.10275094 0.1023414\n",
      " 0.10337612 0.10225939 0.09946814 0.0986519  0.09784025 0.09867081\n",
      " 0.09782537 0.10271729 0.10240432 0.10241855]\n",
      "[[1.11802361 0.81543403 1.01023844 1.00115724 1.10482484 1.03944844\n",
      "  1.16765413 1.02928613 1.14830518 1.06669767 0.98554495 1.06859322\n",
      "  0.98435514 1.0766858  1.03883422 1.04025786]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 36907.17321961758\n",
      "[0.10055253 0.09633298 0.0991455  0.09928245 0.10358802 0.10329167\n",
      " 0.10420127 0.10321739 0.09847046 0.09765416 0.09684245 0.09767307\n",
      " 0.09682553 0.10366994 0.10340235 0.10341659]\n",
      "[[1.11825647 0.81604369 1.01057194 1.00141855 1.1040531  1.03852209\n",
      "  1.16689531 1.02834755 1.14929117 1.06768503 0.9865335  1.06958237\n",
      "  0.98534344 1.07575569 1.03784538 1.03926902]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 36860.19687474014\n",
      "[0.10036209 0.09573553 0.09884354 0.09905717 0.10441861 0.10423902\n",
      " 0.10501989 0.10417311 0.09747274 0.09665638 0.09584459 0.09667529\n",
      " 0.09582561 0.10461986 0.10440042 0.10441465]\n",
      "[[1.11848488 0.81665145 1.01090171 1.00167556 1.10328706 1.03759902\n",
      "  1.16614213 1.02741147 1.15027721 1.06867245 0.9875221  1.07057157\n",
      "  0.98633192 1.07482864 1.03685657 1.0382802 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 36813.43679756721\n",
      "[0.10018032 0.09514181 0.0985489  0.09884032 0.1052424  0.10518325\n",
      " 0.1058317  0.1051264  0.09647497 0.09565854 0.09484669 0.09567745\n",
      " 0.09482562 0.10556689 0.10539852 0.10541276]\n",
      "[[1.11870884 0.81725732 1.01122776 1.00192825 1.10252685 1.03667936\n",
      "  1.16539471 1.02647798 1.1512633  1.06965992 0.98851076 1.07156083\n",
      "  0.98732057 1.07390478 1.03586778 1.03729141]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.10018032 0.09514181 0.0985489  0.09884032 0.1052424  0.10518325\n",
      " 0.1058317  0.1051264  0.09647497 0.09565854 0.09484669 0.09567745\n",
      " 0.09482562 0.10556689 0.10539852 0.10541276]\n",
      "[[1.11870884 0.81725732 1.01122776 1.00192825 1.10252685 1.03667936\n",
      "  1.16539471 1.02647798 1.1512633  1.06965992 0.98851076 1.07156083\n",
      "  0.98732057 1.07390478 1.03586778 1.03729141]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 37786.20914276529\n",
      "[0.20094112 0.19753584 0.19975919 0.19974404 0.20187006 0.20138418\n",
      " 0.20250962 0.20129747 0.20046947 0.19965324 0.1988416  0.19967215\n",
      " 0.19882922 0.20175813 0.20140387 0.2014181 ]\n",
      "[[1.11781275 0.81482912 1.00992018 1.00091373 1.1056542  1.04038641\n",
      "  1.16846767 1.03023099 1.14731056 1.06570142 0.98454774 1.06759253\n",
      "  0.98335879 1.07762686 1.03982885 1.04125249]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 37743.65974699688\n",
      "[0.20071311 0.19692677 0.19942867 0.1994856  0.20267654 0.20233016\n",
      " 0.20330495 0.20225025 0.19947486 0.19865851 0.19784675 0.19867743\n",
      " 0.1978331  0.20270677 0.20239995 0.20241419]\n",
      "[[1.11807748 0.815448   1.0102774  1.0012027  1.1049286  1.03947117\n",
      "  1.16775328 1.0293044  1.14828878 1.06668056 0.98552869 1.06857023\n",
      "  0.98433954 1.07670729 1.03884527 1.04026891]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 37701.250863488196\n",
      "[0.20049165 0.19632046 0.19910367 0.19923352 0.20347632 0.20327267\n",
      " 0.20409332 0.20320004 0.1984801  0.19766363 0.19685175 0.19768255\n",
      " 0.19683672 0.20365215 0.20339613 0.20341037]\n",
      "[[1.11833885 0.8160655  1.01063183 1.00148845 1.10420885 1.03856011\n",
      "  1.16704497 1.02838138 1.14926719 1.06765992 0.98650987 1.06954815\n",
      "  0.9853207  1.07579164 1.03786167 1.03928531]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 37658.96444479593\n",
      "[0.20027673 0.19571697 0.19878417 0.1989878  0.20426919 0.20421186\n",
      " 0.20487481 0.20414718 0.19748518 0.1966686  0.1958566  0.19668752\n",
      " 0.19584011 0.20459444 0.20439242 0.20440666]\n",
      "[[1.11859692 0.81668159 1.01098348 1.00177097 1.10349506 1.03765299\n",
      "  1.16634252 1.02746145 1.15024578 1.06863947 0.98749123 1.07052629\n",
      "  0.9863022  1.0748797  1.03687804 1.03830168]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 37616.800052477185\n",
      "[0.20006835 0.1951163  0.19847018 0.19874846 0.20505489 0.20514752\n",
      " 0.20564922 0.20509151 0.19649011 0.19567341 0.19486131 0.19569234\n",
      " 0.1948433  0.20553344 0.2053888  0.20540304]\n",
      "[[1.11885168 0.81729626 1.01133236 1.00205027 1.10278734 1.03674996\n",
      "  1.165646   1.0265447  1.15122454 1.06961921 0.98847277 1.07150465\n",
      "  0.98728403 1.07397158 1.03589439 1.03731801]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.20006835 0.1951163  0.19847018 0.19874846 0.20505489 0.20514752\n",
      " 0.20564922 0.20509151 0.19649011 0.19567341 0.19486131 0.19569234\n",
      " 0.1948433  0.20553344 0.2053888  0.20540304]\n",
      "[[1.11885168 0.81729626 1.01133236 1.00205027 1.10278734 1.03674996\n",
      "  1.165646   1.0265447  1.15122454 1.06961921 0.98847277 1.07150465\n",
      "  0.98728403 1.07397158 1.03589439 1.03731801]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 38608.01887312116\n",
      "[0.30091652 0.29753134 0.29974269 0.29972429 0.30182149 0.30137954\n",
      " 0.30246496 0.30129646 0.30048027 0.29966388 0.29885209 0.2996828\n",
      " 0.29884044 0.3017538  0.30139637 0.3014106 ]\n",
      "[[1.11783985 0.81483527 1.00993915 1.00093635 1.10571665 1.04039593\n",
      "  1.16852607 1.0302337  1.14729251 1.0656818  0.98452871 1.06755833\n",
      "  0.98334169 1.07763587 1.03984129 1.04126493]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "1 loss 38571.305807053584\n",
      "[0.30066235 0.29691659 0.29939398 0.29944422 0.30257969 0.30231657\n",
      " 0.30321368 0.30223881 0.29949514 0.29867849 0.29786644 0.29869742\n",
      " 0.2978543  0.30269403 0.30238598 0.30240022]\n",
      "[[1.11813234 0.81546112 1.01031631 1.001249   1.10505365 1.03949813\n",
      "  1.16787133 1.02932627 1.1482542  1.06664272 0.9854926  1.06850179\n",
      "  0.98430709 1.07673287 1.03886907 1.0402927 ]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "2 loss 38534.629048446215\n",
      "[0.3004129  0.29630382 0.29904925 0.29916877 0.30333077 0.30324964\n",
      " 0.30395506 0.30317762 0.29850954 0.29769264 0.29688035 0.29771158\n",
      " 0.29686751 0.30363053 0.3033759  0.30339014]\n",
      "[[1.11842244 0.81608603 1.01069149 1.00155934 1.10439665 1.03860536\n",
      "  1.16722281 1.02842329 1.14921643 1.06760422 0.98645707 1.069446\n",
      "  0.98527329 1.0758346  1.03789664 1.03932027]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "3 loss 38497.97664249011\n",
      "[0.30016815 0.29569308 0.29870847 0.29889793 0.30407455 0.30417887\n",
      " 0.30468926 0.30411328 0.29752351 0.29670636 0.29589382 0.29672531\n",
      " 0.29588013 0.30456347 0.30436611 0.30438036]\n",
      "[[1.11871021 0.81670994 1.01106471 1.00186737 1.10374573 1.03771732\n",
      "  1.16658023 1.02752412 1.15017915 1.06856626 0.98742206 1.07039093\n",
      "  0.98624017 1.07494079 1.036924   1.03834762]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "4 loss 38461.34925523755\n",
      "[0.29992811 0.29508437 0.29837165 0.2986317  0.30481083 0.30510405\n",
      " 0.30541611 0.30504561 0.29653705 0.29571967 0.29490688 0.29573862\n",
      " 0.29489221 0.30549262 0.30535661 0.30537086]\n",
      "[[1.11899563 0.81733285 1.01143598 1.00217309 1.10310095 1.03683419\n",
      "  1.16594362 1.02662888 1.15114235 1.06952881 0.98838754 1.07133658\n",
      "  0.98720773 1.0740516  1.03595116 1.03737478]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "[0.29992811 0.29508437 0.29837165 0.2986317  0.30481083 0.30510405\n",
      " 0.30541611 0.30504561 0.29653705 0.29571967 0.29490688 0.29573862\n",
      " 0.29489221 0.30549262 0.30535661 0.30537086]\n",
      "[[1.11899563 0.81733285 1.01143598 1.00217309 1.10310095 1.03683419\n",
      "  1.16594362 1.02662888 1.15114235 1.06952881 0.98838754 1.07133658\n",
      "  0.98720773 1.0740516  1.03595116 1.03737478]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "acc 0.12980769230769232\n",
      "[[   3 1629]\n",
      " [   0  240]]\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 39424.950406038006\n",
      "[0.40088967 0.39752834 0.39972576 0.39970276 0.40176359 0.40137436\n",
      " 0.40241242 0.40129648 0.40051023 0.39969348 0.39888133 0.39971242\n",
      " 0.39887073 0.40174893 0.40137451 0.40138876]\n",
      "[[1.11786517 0.81483868 1.00995551 1.0009572  1.10578527 1.04040637\n",
      "  1.16858972 1.03023408 1.14725285 1.06563682 0.98448498 1.06742486\n",
      "  0.98330419 1.07764581 1.0398703  1.04129394]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "1 loss 39395.64802627243\n",
      "[0.4006075  0.39690932 0.3993584  0.39939961 0.40246327 0.40230114\n",
      " 0.40310573 0.40222534 0.3995528  0.39873545 0.3979227  0.39875442\n",
      " 0.3979126  0.4026795  0.40234416 0.40235841]\n",
      "[[1.11818338 0.81546894 1.01035019 1.00129155 1.10519198 1.03952834\n",
      "  1.16800048 1.0293502  1.14817715 1.06655504 0.98540816 1.06823189\n",
      "  0.98423491 1.07676166 1.03892492 1.04034855]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "2 loss 39366.277837569236\n",
      "[0.40032853 0.39629155 0.39899369 0.39909961 0.40315548 0.40322342\n",
      " 0.40379142 0.40315    0.39859414 0.39777619 0.39696286 0.3977952\n",
      " 0.3969529  0.40360584 0.40331466 0.40332892]\n",
      "[[1.11850003 0.81609868 1.0107436  1.00162436 1.1046047  1.03865617\n",
      "  1.16741744 1.02847171 1.14910273 1.06747466 0.98633273 1.06904153\n",
      "  0.98516727 1.07588306 1.0379788  1.03940242]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "3 loss 39336.83329390903\n",
      "[0.40005272 0.3956751  0.39863163 0.39880275 0.40384011 0.40414135\n",
      " 0.40446969 0.40407096 0.3976343  0.39681578 0.39600188 0.39683482\n",
      " 0.39599173 0.40452811 0.40428598 0.40430024]\n",
      "[[1.11881515 0.81672785 1.01113576 1.00195563 1.10402346 1.03778957\n",
      "  1.1668403  1.0275978  1.15002954 1.06839559 0.98725859 1.06985375\n",
      "  0.98610113 1.07500972 1.03703198 1.03845559]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "4 loss 39307.31567697891\n",
      "[0.39978006 0.39505995 0.39827221 0.39850902 0.40451703 0.40505468\n",
      " 0.40514046 0.40498803 0.39667333 0.39585424 0.39503979 0.39587332\n",
      " 0.39502914 0.40544608 0.40525809 0.40527236]\n",
      "[[1.11912874 0.81735643 1.01152666 1.00228536 1.10344828 1.03692873\n",
      "  1.16626905 1.02672862 1.15095753 1.0693178  0.98818571 1.07066854\n",
      "  0.98703646 1.07414184 1.03608448 1.03750809]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "[0.39978006 0.39505995 0.39827221 0.39850902 0.40451703 0.40505468\n",
      " 0.40514046 0.40498803 0.39667333 0.39585424 0.39503979 0.39587332\n",
      " 0.39502914 0.40544608 0.40525809 0.40527236]\n",
      "[[1.11912874 0.81735643 1.01152666 1.00228536 1.10344828 1.03692873\n",
      "  1.16626905 1.02672862 1.15095753 1.0693178  0.98818571 1.07066854\n",
      "  0.98703646 1.07414184 1.03608448 1.03750809]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "acc 0.13141025641025642\n",
      "[[   6 1626]\n",
      " [   0  240]]\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40160.585458222755\n",
      "[0.50086627 0.49753451 0.49971518 0.49968518 0.50170522 0.50136765\n",
      " 0.50236166 0.50129736 0.5006017  0.49978108 0.49896279 0.49979983\n",
      " 0.49895383 0.50174258 0.50130526 0.50131951]\n",
      "[[1.11788503 0.8148325  1.00996406 1.00097227 1.10584972 1.04041907\n",
      "  1.16864767 1.0302315  1.14716225 1.06553073 0.98438072 1.06650398\n",
      "  0.98321777 1.07765799 1.03994263 1.04136625]]\n",
      "{0: 33, 1: 1839}\n",
      "acc 0.14583333333333334\n",
      "(0.13050570962479607, 1.0, 0.23088023088023085, None)\n",
      "\n",
      "1 loss 40140.954914334354\n",
      "[0.50056004 0.49692024 0.49933522 0.49936333 0.50234427 0.50228197\n",
      " 0.5029987  0.50220925 0.49973009 0.49890188 0.49808258 0.49892082\n",
      " 0.49807556 0.50266135 0.50220797 0.50222224]\n",
      "[[1.11822318 0.81545775 1.01036889 1.00132221 1.10532342 1.03956451\n",
      "  1.16812075 1.02937529 1.14799851 1.06634573 0.98520338 1.06641236\n",
      "  0.984066   1.07679635 1.03906617 1.04048977]]\n",
      "{0: 31, 1: 1841}\n",
      "acc 0.14476495726495728\n",
      "(0.13036393264530147, 1.0, 0.23065833733781838, None)\n",
      "\n",
      "2 loss 40121.12716840679\n",
      "[0.50025568 0.49630639 0.49895672 0.49904338 0.5029757  0.50319158\n",
      " 0.50362795 0.50311643 0.49884931 0.49802011 0.49719942 0.49803923\n",
      " 0.49719365 0.50357568 0.50311272 0.50312701]\n",
      "[[1.11856048 0.81608309 1.01077317 1.00167129 1.10480306 1.03871616\n",
      "  1.16759997 1.02852512 1.14883762 1.06716379 0.98602911 1.06632966\n",
      "  0.98491767 1.07594061 1.03818761 1.03961121]]\n",
      "{0: 30, 1: 1842}\n",
      "acc 0.14423076923076922\n",
      "(0.13029315960912052, 1.0, 0.23054755043227665, None)\n",
      "\n",
      "3 loss 40101.141428287876\n",
      "[0.49995317 0.49569301 0.49857967 0.49872532 0.50359943 0.50409663\n",
      " 0.50424972 0.50401956 0.49796605 0.49713548 0.49631344 0.49715476\n",
      " 0.49630826 0.50448574 0.50401944 0.50403374]\n",
      "[[1.11889698 0.81670845 1.01117689 1.00201951 1.10428861 1.03787367\n",
      "  1.16708495 1.02767994 1.14967949 1.06798478 0.9868578  1.06625603\n",
      "  0.98577258 1.07509046 1.03730705 1.03873063]]\n",
      "{0: 26, 1: 1846}\n",
      "acc 0.1420940170940171\n",
      "(0.13001083423618634, 1.0, 0.23010546500479387, None)\n",
      "\n",
      "4 loss 40081.00877109389\n",
      "[0.49965248 0.49508012 0.49820407 0.49840915 0.50421544 0.50499686\n",
      " 0.50486398 0.50491844 0.49707997 0.49624805 0.49542467 0.49626748\n",
      " 0.49541948 0.50539128 0.50492807 0.50494239]\n",
      "[[1.11923268 0.81733381 1.01158005 1.00236687 1.10378005 1.0370373\n",
      "  1.16657567 1.02683989 1.15052406 1.06880866 0.98768939 1.06619163\n",
      "  0.98663065 1.07424612 1.03642453 1.03784809]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "\n",
      "[0.49965248 0.49508012 0.49820407 0.49840915 0.50421544 0.50499686\n",
      " 0.50486398 0.50491844 0.49707997 0.49624805 0.49542467 0.49626748\n",
      " 0.49541948 0.50539128 0.50492807 0.50494239]\n",
      "[[1.11923268 0.81733381 1.01158005 1.00236687 1.10378005 1.0370373\n",
      "  1.16657567 1.02683989 1.15052406 1.06880866 0.98768939 1.06619163\n",
      "  0.98663065 1.07424612 1.03642453 1.03784809]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "acc 0.1388888888888889\n",
      "[[  20 1612]\n",
      " [   0  240]]\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40592.50416536397\n",
      "[0.60084891 0.59755528 0.59971427 0.59967454 0.60164629 0.60135042\n",
      " 0.60231437 0.6012933  0.60071934 0.59990465 0.59909139 0.600852\n",
      " 0.59907895 0.60172615 0.60120261 0.60121688]\n",
      "[[1.11789788 0.81481143 1.00996214 1.00097956 1.10591029 1.04044834\n",
      "  1.16869882 1.03023618 1.14702747 1.06535968 0.98421101 1.06611201\n",
      "  0.98308755 1.07768625 1.04006489 1.0414885 ]]\n",
      "{0: 279, 1: 1593}\n",
      "acc 0.2761752136752137\n",
      "(0.15003138731952292, 0.9958333333333333, 0.2607746863066012, None)\n",
      "\n",
      "1 loss 40582.1715315032\n",
      "[0.60052497 0.59696073 0.59933125 0.59934105 0.60222397 0.60223427\n",
      " 0.60289581 0.60217497 0.59996742 0.5991531  0.59833813 0.60102094\n",
      " 0.59832528 0.60261581 0.60200484 0.60201913]\n",
      "[[1.11824876 0.81541646 1.01036688 1.0013373  1.1054471  1.03964494\n",
      "  1.16822977 1.02942625 1.1477298  1.06600423 0.98486625 1.06565124\n",
      "  0.98380868 1.07687406 1.0393087  1.04073229]]\n",
      "{0: 273, 1: 1599}\n",
      "acc 0.27297008547008544\n",
      "(0.14946841776110067, 0.9958333333333333, 0.25992387166938546, None)\n",
      "\n",
      "2 loss 40571.72518193695\n",
      "[0.60020181 0.59636569 0.59894861 0.59900846 0.60279442 0.6031132\n",
      " 0.60346968 0.60305151 0.59921105 0.59839712 0.59758047 0.60119259\n",
      " 0.59756643 0.60350084 0.60281048 0.6028248 ]\n",
      "[[1.11859941 0.81602229 1.01077174 1.00169472 1.10498938 1.03884776\n",
      "  1.16776655 1.02862268 1.14843628 1.06665312 0.98552596 1.06518811\n",
      "  0.98453482 1.07606785 1.03854895 1.03997251]]\n",
      "{0: 269, 1: 1603}\n",
      "acc 0.2708333333333333\n",
      "(0.14909544603867747, 0.9958333333333333, 0.25935973955507324, None)\n",
      "\n",
      "3 loss 40561.15560197674\n",
      "[0.59987939 0.59577022 0.59856636 0.59867676 0.60335765 0.60398749\n",
      " 0.60403643 0.60392382 0.59845033 0.59763683 0.59681851 0.60136694\n",
      " 0.59680253 0.60438147 0.60361944 0.60363379]\n",
      "[[1.11894988 0.81662884 1.0111767  1.00205182 1.1045371  1.03805632\n",
      "  1.16730873 1.02782406 1.14914685 1.06730626 0.98619004 1.0647225\n",
      "  0.98526574 1.07526716 1.03778571 1.03920925]]\n",
      "{0: 268, 1: 1604}\n",
      "acc 0.2702991452991453\n",
      "(0.14900249376558602, 0.9958333333333333, 0.2592190889370933, None)\n",
      "\n",
      "4 loss 40550.46224679591\n",
      "[0.5995577  0.59517434 0.59818451 0.59834596 0.60391367 0.60485688\n",
      " 0.60459608 0.60479175 0.59768533 0.59687228 0.59605237 0.60154397\n",
      " 0.59603369 0.60525748 0.60443169 0.60444607]\n",
      "[[1.11930016 0.81723609 1.01158175 1.0024086  1.10409017 1.03727088\n",
      "  1.16685623 1.02703052 1.14986144 1.06796363 0.98685845 1.06425444\n",
      "  0.98600138 1.07447222 1.03701905 1.03844255]]\n",
      "{0: 264, 1: 1608}\n",
      "acc 0.26816239316239315\n",
      "(0.1486318407960199, 0.9958333333333333, 0.25865800865800864, None)\n",
      "\n",
      "[0.5995577  0.59517434 0.59818451 0.59834596 0.60391367 0.60485688\n",
      " 0.60459608 0.60479175 0.59768533 0.59687228 0.59605237 0.60154397\n",
      " 0.59603369 0.60525748 0.60443169 0.60444607]\n",
      "[[1.11930016 0.81723609 1.01158175 1.0024086  1.10409017 1.03727088\n",
      "  1.16685623 1.02703052 1.14986144 1.06796363 0.98685845 1.06425444\n",
      "  0.98600138 1.07447222 1.03701905 1.03844255]]\n",
      "{0: 264, 1: 1608}\n",
      "acc 0.26816239316239315\n",
      "acc 0.26816239316239315\n",
      "[[ 263 1369]\n",
      " [   1  239]]\n",
      "(0.1486318407960199, 0.9958333333333333, 0.25865800865800864, None)\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40860.41526304816\n",
      "[0.70085409 0.69763301 0.69974879 0.69968181 0.7015999  0.70130202\n",
      " 0.70229795 0.70127087 0.70101835 0.70018344 0.69939054 0.70104912\n",
      " 0.69937729 0.70167954 0.70093087 0.70094516]\n",
      "[[1.11788935 0.81473432 1.00992565 1.00096938 1.10595746 1.04052016\n",
      "  1.16872035 1.03027451 1.14674963 1.06500527 0.98384767 1.06602004\n",
      "  0.9827947  1.07775639 1.04038441 1.04180801]]\n",
      "{0: 610, 1: 1262}\n",
      "acc 0.44871794871794873\n",
      "(0.1862123613312203, 0.9791666666666666, 0.31291611185086554, None)\n",
      "\n",
      "1 loss 40857.181150737866\n",
      "[0.70053524 0.69711574 0.69939787 0.69935445 0.70212657 0.7021122\n",
      " 0.70284395 0.7021035  0.70056525 0.69971149 0.69893683 0.70140486\n",
      " 0.6989216  0.70249822 0.70146236 0.70147669]\n",
      "[[1.11823149 0.81526261 1.0102962  1.00131781 1.1055459  1.03982537\n",
      "  1.16829021 1.02953801 1.14717332 1.06529164 0.98413658 1.06548094\n",
      "  0.98322404 1.07705041 1.03994884 1.0413724 ]]\n",
      "{0: 606, 1: 1266}\n",
      "acc 0.4465811965811966\n",
      "(0.18562401263823064, 0.9791666666666666, 0.31208499335989376, None)\n",
      "\n",
      "2 loss 40853.89355860564\n",
      "[0.70021631 0.69659671 0.69904632 0.69902723 0.70264672 0.70291803\n",
      " 0.70338256 0.7029311  0.70010555 0.69923319 0.69847691 0.7017628\n",
      " 0.6984585  0.70331273 0.70199903 0.70201342]\n",
      "[[1.11857399 0.81579285 1.01066765 1.0016664  1.10513911 1.03913528\n",
      "  1.16786569 1.02880711 1.14760195 1.06558165 0.98442945 1.06494007\n",
      "  0.98366001 1.07634901 1.03950729 1.0409308 ]]\n",
      "{0: 600, 1: 1272}\n",
      "acc 0.44337606837606836\n",
      "(0.18474842767295596, 0.9791666666666666, 0.3108465608465608, None)\n",
      "\n",
      "3 loss 40850.54605757701\n",
      "[0.69989725 0.69607595 0.69869417 0.69870013 0.70316035 0.70371998\n",
      " 0.70391434 0.70375477 0.69963927 0.69874964 0.69801069 0.70212302\n",
      " 0.69798803 0.70412353 0.70254011 0.70255455]\n",
      "[[1.11891689 0.81632497 1.01103995 1.00201516 1.1047371  1.03844917\n",
      "  1.16744626 1.02808017 1.14803556 1.06587531 0.98472629 1.06439718\n",
      "  0.98410248 1.07565151 1.03905975 1.04048322]]\n",
      "{0: 593, 1: 1279}\n",
      "acc 0.43963675213675213\n",
      "(0.18373729476153244, 0.9791666666666666, 0.3094140882159315, None)\n",
      "\n",
      "4 loss 40847.13517177441\n",
      "[0.69957807 0.69555349 0.69834142 0.69837317 0.70366748 0.70451787\n",
      " 0.70443931 0.70457443 0.6991664  0.69826025 0.6975382  0.70248549\n",
      " 0.69751021 0.70493044 0.70308688 0.70310136]\n",
      "[[1.11926019 0.81685897 1.01141311 1.00236409 1.1043398  1.03776724\n",
      "  1.16703187 1.02735731 1.14847414 1.06617269 0.98502715 1.06385227\n",
      "  0.98455143 1.07495808 1.03860624 1.04002967]]\n",
      "{0: 582, 1: 1290}\n",
      "acc 0.4337606837606838\n",
      "(0.1821705426356589, 0.9791666666666666, 0.30718954248366015, None)\n",
      "\n",
      "[0.69957807 0.69555349 0.69834142 0.69837317 0.70366748 0.70451787\n",
      " 0.70443931 0.70457443 0.6991664  0.69826025 0.6975382  0.70248549\n",
      " 0.69751021 0.70493044 0.70308688 0.70310136]\n",
      "[[1.11926019 0.81685897 1.01141311 1.00236409 1.1043398  1.03776724\n",
      "  1.16703187 1.02735731 1.14847414 1.06617269 0.98502715 1.06385227\n",
      "  0.98455143 1.07495808 1.03860624 1.04002967]]\n",
      "{0: 582, 1: 1290}\n",
      "acc 0.4337606837606838\n",
      "acc 0.4337606837606838\n",
      "[[ 577 1055]\n",
      " [   5  235]]\n",
      "(0.1821705426356589, 0.9791666666666666, 0.30718954248366015, None)\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40948.556322521\n",
      "[0.80090103 0.79782484 0.79983838 0.79971898 0.80150169 0.80118307\n",
      " 0.80226173 0.801187   0.80137794 0.80061777 0.79978222 0.80111646\n",
      " 0.79974741 0.80156277 0.80042026 0.80043455]\n",
      "[[1.11783907 0.81454268 1.0098336  1.00092867 1.10604595 1.04066059\n",
      "  1.1687617  1.03040165 1.14648781 1.06478835 0.98363145 1.0659475\n",
      "  0.98244656 1.07789632 1.04093356 1.04235718]]\n",
      "{0: 962, 1: 910}\n",
      "acc 0.6089743589743589\n",
      "(0.22967032967032966, 0.8708333333333333, 0.3634782608695652, None)\n",
      "\n",
      "1 loss 40948.160804551\n",
      "[0.8006288  0.79750266 0.79957666 0.79942788 0.80192081 0.8018074\n",
      " 0.80274326 0.8019022  0.80128992 0.8005845  0.79972257 0.80149868\n",
      " 0.79966894 0.80219873 0.80043459 0.80044893]\n",
      "[[1.11813108 0.81487598 1.01011244 1.00123713 1.10573237 1.04018164\n",
      "  1.16839798 1.0298204  1.14664543 1.06485685 0.98369808 1.06535407\n",
      "  0.9825177  1.07740583 1.04105179 1.04247537]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "2 loss 40947.770988909964\n",
      "[0.8003564  0.79717954 0.79931449 0.79913678 0.80233688 0.80242798\n",
      " 0.80322028 0.80261296 0.80120049 0.80055151 0.79966313 0.80188088\n",
      " 0.79958863 0.80283097 0.80045053 0.80046492]\n",
      "[[1.11842341 0.81521034 1.01039186 1.00154575 1.10542071 1.0397059\n",
      "  1.1680374  1.02924351 1.14680379 1.06492529 0.98376464 1.06476044\n",
      "  0.98259034 1.07691861 1.04116827 1.04259182]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "3 loss 40947.381391936964\n",
      "[0.8000838  0.79685549 0.7990519  0.79884568 0.80274982 0.80304572\n",
      " 0.80369342 0.80332055 0.80110964 0.8005188  0.7996039  0.80226329\n",
      " 0.79950644 0.80346038 0.80046809 0.80048253]\n",
      "[[1.11871609 0.81554574 1.01067185 1.00185453 1.10511105 1.03923242\n",
      "  1.16767942 1.02866948 1.14696287 1.06499366 0.98383109 1.06416623\n",
      "  0.98266444 1.07643367 1.04128301 1.04270653]]\n",
      "{0: 958, 1: 914}\n",
      "acc 0.6068376068376068\n",
      "(0.2286652078774617, 0.8708333333333333, 0.3622183708838822, None)\n",
      "\n",
      "4 loss 40946.991893642175\n",
      "[0.79981099 0.79653051 0.79878887 0.79855458 0.80315961 0.80366058\n",
      " 0.80416269 0.80402495 0.80101734 0.80048638 0.79954489 0.80264591\n",
      " 0.79942237 0.80408693 0.8004873  0.80050179]\n",
      "[[1.11900913 0.8158822  1.01095241 1.00216345 1.10480337 1.0387612\n",
      "  1.16732403 1.02809832 1.14712269 1.06506198 0.98389744 1.06357141\n",
      "  0.98274001 1.07595105 1.04139599 1.04281947]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "\n",
      "[0.79981099 0.79653051 0.79878887 0.79855458 0.80315961 0.80366058\n",
      " 0.80416269 0.80402495 0.80101734 0.80048638 0.79954489 0.80264591\n",
      " 0.79942237 0.80408693 0.8004873  0.80050179]\n",
      "[[1.11900913 0.8158822  1.01095241 1.00216345 1.10480337 1.0387612\n",
      "  1.16732403 1.02809832 1.14712269 1.06506198 0.98389744 1.06357141\n",
      "  0.98274001 1.07595105 1.04139599 1.04281947]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "acc 0.6052350427350427\n",
      "[[923 709]\n",
      " [ 30 210]]\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 41003.452654651395\n",
      "[0.90090747 0.89784935 0.89984971 0.89972232 0.90130818 0.9010178\n",
      " 0.90203485 0.90104743 0.90135027 0.90057044 0.89977106 0.90122268\n",
      " 0.89983185 0.90139571 0.90019427 0.90020853]\n",
      "[[1.1178228  0.81451301 1.00981405 1.00091706 1.10619529 1.04080488\n",
      "  1.168951   1.03055242 1.14648621 1.06482055 0.98365403 1.06575353\n",
      "  0.98241532 1.07804351 1.04111771 1.04254135]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "1 loss 41003.271755162255\n",
      "[0.90064114 0.8975513  0.8995981  0.89943194 0.90153959 0.90134232\n",
      " 0.90232637 0.90150212 0.90123278 0.90049067 0.89969999 0.90166412\n",
      " 0.89983504 0.90172821 0.89998026 0.89999454]\n",
      "[[1.11809924 0.81481697 1.01007443 1.00121655 1.10602361 1.04057934\n",
      "  1.16873607 1.0302237  1.14664289 1.06492037 0.98374452 1.06491125\n",
      "  0.98245863 1.07781205 1.04142043 1.04284406]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "2 loss 41003.09266293903\n",
      "[0.9003747  0.89725301 0.89934631 0.89914151 0.90176987 0.90166348\n",
      " 0.90261559 0.90195141 0.90111511 0.90041104 0.899629   0.90210291\n",
      " 0.89983805 0.90205724 0.89976619 0.8997805 ]\n",
      "[[1.11837587 0.81512122 1.01033506 1.00151614 1.1058524  1.04035602\n",
      "  1.16852232 1.02989924 1.14679975 1.06502009 0.98383498 1.06406729\n",
      "  0.98250207 1.0775829  1.04172265 1.04314628]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "3 loss 41002.91332541443\n",
      "[0.90010812 0.89695453 0.89909435 0.89885106 0.90199889 0.90198219\n",
      " 0.90290283 0.90239672 0.90099729 0.90033157 0.89955814 0.90253948\n",
      " 0.8998409  0.90238374 0.89955207 0.89956641]\n",
      "[[1.1186527  0.8154257  1.01059591 1.00181582 1.10568175 1.04013424\n",
      "  1.16830951 1.02957782 1.14695676 1.06511973 0.98392537 1.06322147\n",
      "  0.98254556 1.07735538 1.04202441 1.04344802]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "4 loss 41002.73372577054\n",
      "[0.8998414  0.89665586 0.89884224 0.8985606  0.90222667 0.90229848\n",
      " 0.9031881  0.90283807 0.90087931 0.90025224 0.89948741 0.90297384\n",
      " 0.89984358 0.90270772 0.89933791 0.89935227]\n",
      "[[1.11892972 0.81573042 1.01085697 1.00211558 1.10551167 1.039914\n",
      "  1.16809765 1.02925943 1.14711391 1.06521929 0.98401569 1.06237376\n",
      "  0.98258912 1.07712947 1.04232569 1.0437493 ]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "[0.8998414  0.89665586 0.89884224 0.8985606  0.90222667 0.90229848\n",
      " 0.9031881  0.90283807 0.90087931 0.90025224 0.89948741 0.90297384\n",
      " 0.89984358 0.90270772 0.89933791 0.89935227]\n",
      "[[1.11892972 0.81573042 1.01085697 1.00211558 1.10551167 1.039914\n",
      "  1.16809765 1.02925943 1.14711391 1.06521929 0.98401569 1.06237376\n",
      "  0.98258912 1.07712947 1.04232569 1.0437493 ]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "acc 0.6666666666666666\n",
      "[[1051  581]\n",
      " [  43  197]]\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty2\n",
      "loss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 41033.940030842794\n",
      "[1.00236172 0.99893934 1.00099467 1.00000736 1.00226036 1.00167788\n",
      " 1.00283767 1.00156842 1.0026252  1.00189305 0.99995454 1.00190966\n",
      " 0.99995435 1.00201392 1.00167275 1.00168684]\n",
      "[[1.1178414  0.81452396 1.00982897 1.00092989 1.10634091 1.04134644\n",
      "  1.16916216 1.03120894 1.14651537 1.06486336 0.98369221 1.0666534\n",
      "  0.98246879 1.07859219 1.04114762 1.04257126]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "1 loss 41033.93973974388\n",
      "[1.00357081 0.99950044 1.00239797 1.00001087 1.00347694 1.00295751\n",
      " 1.00401366 1.00286357 1.00381509 1.00314679 0.99997466 1.00316155\n",
      " 0.99998016 1.00325371 1.00295324 1.00296542]\n",
      "[[1.11813522 0.81483954 1.01010407 1.00124266 1.10630217 1.04134665\n",
      "  1.16912225 1.0312089  1.14670098 1.06500294 0.9838222  1.06689868\n",
      "  0.98256589 1.07859255 1.04147995 1.04290359]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "2 loss 41033.93935449929\n",
      "[1.00471737 0.99981314 1.00360924 1.00002322 1.00462646 1.00413133\n",
      " 1.00514413 1.00404262 1.00495271 1.00431163 0.99997872 1.00432583\n",
      " 0.99999018 1.00441283 1.00412745 1.00413898]\n",
      "[[1.11842912 0.81515517 1.01037924 1.00155546 1.10626393 1.04134756\n",
      "  1.16908335 1.03120985 1.14688676 1.06514265 0.98395225 1.0674294\n",
      "  0.98266308 1.07859379 1.04181231 1.04323595]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "3 loss 41033.93884616661\n",
      "[1.00582957 0.99991754 1.00475384 1.00014501 1.00574003 1.00525784\n",
      " 1.00624669 1.00517176 1.00605974 1.00543378 0.99997656 1.00544788\n",
      " 0.99999554 1.00553167 1.00525425 1.00526543]\n",
      "[[1.11872317 0.81547081 1.01065449 1.00186827 1.10622664 1.04134982\n",
      "  1.16904605 1.03121267 1.1470728  1.06528262 0.98408229 1.06817893\n",
      "  0.98276026 1.0785966  1.04214478 1.04356843]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "4 loss 41033.93818551422\n",
      "[1.00692084 0.99993961 1.00586483 1.00155106 1.00683203 1.00635794\n",
      " 1.00733144 1.00627346 1.00714762 1.00653136 0.99996899 1.00654556\n",
      " 0.99999836 1.00662702 1.00635458 1.00636557]\n",
      "[[1.11901747 0.81578646 1.01092988 1.00218109 1.1061908  1.0413542\n",
      "  1.16901106 1.03121845 1.14725921 1.065423   0.98421232 1.06905886\n",
      "  0.98285742 1.07860184 1.04247741 1.04390106]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "[1.00692084 0.99993961 1.00586483 1.00155106 1.00683203 1.00635794\n",
      " 1.00733144 1.00627346 1.00714762 1.00653136 0.99996899 1.00654556\n",
      " 0.99999836 1.00662702 1.00635458 1.00636557]\n",
      "[[1.11901747 0.81578646 1.01092988 1.00218109 1.1061908  1.0413542\n",
      "  1.16901106 1.03121845 1.14725921 1.065423   0.98421232 1.06905886\n",
      "  0.98285742 1.07860184 1.04247741 1.04390106]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "acc 0.6741452991452992\n",
      "[[1065  567]\n",
      " [  43  197]]\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "for i in np.linspace(0,1,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 36272.053384859966\n",
      "[ 0.00096819 -0.00246081 -0.00022373 -0.00023594  0.00193544  0.00139114\n",
      "  0.00257035  0.00130007  0.00046464 -0.00035151 -0.00116307 -0.0003326\n",
      " -0.00117648  0.00176462  0.00140698  0.00142122]\n",
      "[[1.1177623  0.81481647 1.00988389 1.00087171 1.1055609  1.04037169\n",
      "  1.16837987 1.03022449 1.14732371 1.06571468 0.98456076 1.06760853\n",
      "  0.98337146 1.07761299 1.03982016 1.0412438 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 36220.222009824654\n",
      "[ 0.00077152 -0.00306429 -0.00053328 -0.00046991  0.00280593  0.00235019\n",
      "  0.00342943  0.00226625 -0.00053377 -0.00134997 -0.00216158 -0.00133106\n",
      " -0.0021776   0.00272547  0.00240545  0.00241968]\n",
      "[[1.11797456 0.81542123 1.01020277 1.00111604 1.10474283 1.03943018\n",
      "  1.16757476 1.02927183 1.14831355 1.0667059  0.9855528  1.06860199\n",
      "  0.98436336 1.07666858 1.03882848 1.04025212]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 36168.73391918599\n",
      "[ 0.00058591 -0.00366311 -0.00083354 -0.00069318  0.00367051  0.00330647\n",
      "  0.00428227  0.00323015 -0.00153218 -0.00234842 -0.00316008 -0.00232951\n",
      " -0.0031788   0.00368373  0.00340389  0.00341812]\n",
      "[[1.11818103 0.81602357 1.0105168  1.0013548  1.10392999 1.03849163\n",
      "  1.16677518 1.02832158 1.14930338 1.06769711 0.98654484 1.06959542\n",
      "  0.98535542 1.07572696 1.03783687 1.03926051]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 36117.56060794781\n",
      "[ 0.00041137 -0.00425722 -0.00112447 -0.00090571  0.00452896  0.00426011\n",
      "  0.00512886  0.00419204 -0.00253059 -0.00334688 -0.00415859 -0.00332797\n",
      " -0.00418002  0.00463952  0.00440234  0.00441658]\n",
      "[[1.11838173 0.81662346 1.010826   1.001588   1.10312251 1.03755584\n",
      "  1.16598095 1.02737338 1.15029319 1.06868832 0.98753688 1.07058884\n",
      "  0.98634759 1.07478791 1.0368453  1.03826894]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 36066.69894879077\n",
      "[ 0.0002479  -0.00484658 -0.00140603 -0.00110747  0.00538094  0.00521093\n",
      "  0.00596889  0.00515177 -0.00352902 -0.00434536 -0.00515712 -0.00432645\n",
      " -0.00518125  0.00559269  0.0054008   0.00541504]\n",
      "[[1.11857667 0.81722089 1.01113035 1.00181562 1.10232054 1.03662293\n",
      "  1.16519222 1.02642734 1.151283   1.06967952 0.98852892 1.07158226\n",
      "  0.98733987 1.07385155 1.03585377 1.03727741]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[ 0.0002479  -0.00484658 -0.00140603 -0.00110747  0.00538094  0.00521093\n",
      "  0.00596889  0.00515177 -0.00352902 -0.00434536 -0.00515712 -0.00432645\n",
      " -0.00518125  0.00559269  0.0054008   0.00541504]\n",
      "[[1.11857667 0.81722089 1.01113035 1.00181562 1.10232054 1.03662293\n",
      "  1.16519222 1.02642734 1.151283   1.06967952 0.98852892 1.07158226\n",
      "  0.98733987 1.07385155 1.03585377 1.03727741]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 37001.80337293428\n",
      "[0.1009594  0.09753896 0.09977131 0.09975826 0.10190758 0.10138807\n",
      " 0.10254439 0.10129883 0.10046577 0.0996496  0.09883802 0.09966851\n",
      " 0.09882508 0.10176176 0.1014063  0.10142053]\n",
      "[[1.11778627 0.81482251 1.00990119 1.00089162 1.10560216 1.04037828\n",
      "  1.16841878 1.03022762 1.14731924 1.06571036 0.98455647 1.06760411\n",
      "  0.98336709 1.07761919 1.03982311 1.04124675]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 36954.36754660866\n",
      "[0.10075163 0.09693414 0.09945476 0.09951616 0.10275094 0.1023414\n",
      " 0.10337612 0.10225939 0.09946814 0.0986519  0.09784025 0.09867081\n",
      " 0.09782537 0.10271729 0.10240432 0.10241855]\n",
      "[[1.11802361 0.81543403 1.01023844 1.00115724 1.10482484 1.03944844\n",
      "  1.16765413 1.02928613 1.14830518 1.06669767 0.98554495 1.06859322\n",
      "  0.98435514 1.0766858  1.03883422 1.04025786]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 36907.17321961758\n",
      "[0.10055253 0.09633298 0.0991455  0.09928245 0.10358802 0.10329167\n",
      " 0.10420127 0.10321739 0.09847046 0.09765416 0.09684245 0.09767307\n",
      " 0.09682553 0.10366994 0.10340235 0.10341659]\n",
      "[[1.11825647 0.81604369 1.01057194 1.00141855 1.1040531  1.03852209\n",
      "  1.16689531 1.02834755 1.14929117 1.06768503 0.9865335  1.06958237\n",
      "  0.98534344 1.07575569 1.03784538 1.03926902]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 36860.19687474014\n",
      "[0.10036209 0.09573553 0.09884354 0.09905717 0.10441861 0.10423902\n",
      " 0.10501989 0.10417311 0.09747274 0.09665638 0.09584459 0.09667529\n",
      " 0.09582561 0.10461986 0.10440042 0.10441465]\n",
      "[[1.11848488 0.81665145 1.01090171 1.00167556 1.10328706 1.03759902\n",
      "  1.16614213 1.02741147 1.15027721 1.06867245 0.9875221  1.07057157\n",
      "  0.98633192 1.07482864 1.03685657 1.0382802 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 36813.43679756721\n",
      "[0.10018032 0.09514181 0.0985489  0.09884032 0.1052424  0.10518325\n",
      " 0.1058317  0.1051264  0.09647497 0.09565854 0.09484669 0.09567745\n",
      " 0.09482562 0.10556689 0.10539852 0.10541276]\n",
      "[[1.11870884 0.81725732 1.01122776 1.00192825 1.10252685 1.03667936\n",
      "  1.16539471 1.02647798 1.1512633  1.06965992 0.98851076 1.07156083\n",
      "  0.98732057 1.07390478 1.03586778 1.03729141]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.10018032 0.09514181 0.0985489  0.09884032 0.1052424  0.10518325\n",
      " 0.1058317  0.1051264  0.09647497 0.09565854 0.09484669 0.09567745\n",
      " 0.09482562 0.10556689 0.10539852 0.10541276]\n",
      "[[1.11870884 0.81725732 1.01122776 1.00192825 1.10252685 1.03667936\n",
      "  1.16539471 1.02647798 1.1512633  1.06965992 0.98851076 1.07156083\n",
      "  0.98732057 1.07390478 1.03586778 1.03729141]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 37786.20914276529\n",
      "[0.20094112 0.19753584 0.19975919 0.19974404 0.20187006 0.20138418\n",
      " 0.20250962 0.20129747 0.20046947 0.19965324 0.1988416  0.19967215\n",
      " 0.19882922 0.20175813 0.20140387 0.2014181 ]\n",
      "[[1.11781275 0.81482912 1.00992018 1.00091373 1.1056542  1.04038641\n",
      "  1.16846767 1.03023099 1.14731056 1.06570142 0.98454774 1.06759253\n",
      "  0.98335879 1.07762686 1.03982885 1.04125249]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss 37743.65974699688\n",
      "[0.20071311 0.19692677 0.19942867 0.1994856  0.20267654 0.20233016\n",
      " 0.20330495 0.20225025 0.19947486 0.19865851 0.19784675 0.19867743\n",
      " 0.1978331  0.20270677 0.20239995 0.20241419]\n",
      "[[1.11807748 0.815448   1.0102774  1.0012027  1.1049286  1.03947117\n",
      "  1.16775328 1.0293044  1.14828878 1.06668056 0.98552869 1.06857023\n",
      "  0.98433954 1.07670729 1.03884527 1.04026891]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss 37701.250863488196\n",
      "[0.20049165 0.19632046 0.19910367 0.19923352 0.20347632 0.20327267\n",
      " 0.20409332 0.20320004 0.1984801  0.19766363 0.19685175 0.19768255\n",
      " 0.19683672 0.20365215 0.20339613 0.20341037]\n",
      "[[1.11833885 0.8160655  1.01063183 1.00148845 1.10420885 1.03856011\n",
      "  1.16704497 1.02838138 1.14926719 1.06765992 0.98650987 1.06954815\n",
      "  0.9853207  1.07579164 1.03786167 1.03928531]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss 37658.96444479593\n",
      "[0.20027673 0.19571697 0.19878417 0.1989878  0.20426919 0.20421186\n",
      " 0.20487481 0.20414718 0.19748518 0.1966686  0.1958566  0.19668752\n",
      " 0.19584011 0.20459444 0.20439242 0.20440666]\n",
      "[[1.11859692 0.81668159 1.01098348 1.00177097 1.10349506 1.03765299\n",
      "  1.16634252 1.02746145 1.15024578 1.06863947 0.98749123 1.07052629\n",
      "  0.9863022  1.0748797  1.03687804 1.03830168]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss 37616.800052477185\n",
      "[0.20006835 0.1951163  0.19847018 0.19874846 0.20505489 0.20514752\n",
      " 0.20564922 0.20509151 0.19649011 0.19567341 0.19486131 0.19569234\n",
      " 0.1948433  0.20553344 0.2053888  0.20540304]\n",
      "[[1.11885168 0.81729626 1.01133236 1.00205027 1.10278734 1.03674996\n",
      "  1.165646   1.0265447  1.15122454 1.06961921 0.98847277 1.07150465\n",
      "  0.98728403 1.07397158 1.03589439 1.03731801]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.20006835 0.1951163  0.19847018 0.19874846 0.20505489 0.20514752\n",
      " 0.20564922 0.20509151 0.19649011 0.19567341 0.19486131 0.19569234\n",
      " 0.1948433  0.20553344 0.2053888  0.20540304]\n",
      "[[1.11885168 0.81729626 1.01133236 1.00205027 1.10278734 1.03674996\n",
      "  1.165646   1.0265447  1.15122454 1.06961921 0.98847277 1.07150465\n",
      "  0.98728403 1.07397158 1.03589439 1.03731801]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 38608.01887312116\n",
      "[0.30091652 0.29753134 0.29974269 0.29972429 0.30182149 0.30137954\n",
      " 0.30246496 0.30129646 0.30048027 0.29966388 0.29885209 0.2996828\n",
      " 0.29884044 0.3017538  0.30139637 0.3014106 ]\n",
      "[[1.11783985 0.81483527 1.00993915 1.00093635 1.10571665 1.04039593\n",
      "  1.16852607 1.0302337  1.14729251 1.0656818  0.98452871 1.06755833\n",
      "  0.98334169 1.07763587 1.03984129 1.04126493]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "1 loss 38571.305807053584\n",
      "[0.30066235 0.29691659 0.29939398 0.29944422 0.30257969 0.30231657\n",
      " 0.30321368 0.30223881 0.29949514 0.29867849 0.29786644 0.29869742\n",
      " 0.2978543  0.30269403 0.30238598 0.30240022]\n",
      "[[1.11813234 0.81546112 1.01031631 1.001249   1.10505365 1.03949813\n",
      "  1.16787133 1.02932627 1.1482542  1.06664272 0.9854926  1.06850179\n",
      "  0.98430709 1.07673287 1.03886907 1.0402927 ]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "2 loss 38534.629048446215\n",
      "[0.3004129  0.29630382 0.29904925 0.29916877 0.30333077 0.30324964\n",
      " 0.30395506 0.30317762 0.29850954 0.29769264 0.29688035 0.29771158\n",
      " 0.29686751 0.30363053 0.3033759  0.30339014]\n",
      "[[1.11842244 0.81608603 1.01069149 1.00155934 1.10439665 1.03860536\n",
      "  1.16722281 1.02842329 1.14921643 1.06760422 0.98645707 1.069446\n",
      "  0.98527329 1.0758346  1.03789664 1.03932027]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "3 loss 38497.97664249011\n",
      "[0.30016815 0.29569308 0.29870847 0.29889793 0.30407455 0.30417887\n",
      " 0.30468926 0.30411328 0.29752351 0.29670636 0.29589382 0.29672531\n",
      " 0.29588013 0.30456347 0.30436611 0.30438036]\n",
      "[[1.11871021 0.81670994 1.01106471 1.00186737 1.10374573 1.03771732\n",
      "  1.16658023 1.02752412 1.15017915 1.06856626 0.98742206 1.07039093\n",
      "  0.98624017 1.07494079 1.036924   1.03834762]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "4 loss 38461.34925523755\n",
      "[0.29992811 0.29508437 0.29837165 0.2986317  0.30481083 0.30510405\n",
      " 0.30541611 0.30504561 0.29653705 0.29571967 0.29490688 0.29573862\n",
      " 0.29489221 0.30549262 0.30535661 0.30537086]\n",
      "[[1.11899563 0.81733285 1.01143598 1.00217309 1.10310095 1.03683419\n",
      "  1.16594362 1.02662888 1.15114235 1.06952881 0.98838754 1.07133658\n",
      "  0.98720773 1.0740516  1.03595116 1.03737478]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "[0.29992811 0.29508437 0.29837165 0.2986317  0.30481083 0.30510405\n",
      " 0.30541611 0.30504561 0.29653705 0.29571967 0.29490688 0.29573862\n",
      " 0.29489221 0.30549262 0.30535661 0.30537086]\n",
      "[[1.11899563 0.81733285 1.01143598 1.00217309 1.10310095 1.03683419\n",
      "  1.16594362 1.02662888 1.15114235 1.06952881 0.98838754 1.07133658\n",
      "  0.98720773 1.0740516  1.03595116 1.03737478]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "acc 0.12980769230769232\n",
      "[[   3 1629]\n",
      " [   0  240]]\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 39424.950406038006\n",
      "[0.40088967 0.39752834 0.39972576 0.39970276 0.40176359 0.40137436\n",
      " 0.40241242 0.40129648 0.40051023 0.39969348 0.39888133 0.39971242\n",
      " 0.39887073 0.40174893 0.40137451 0.40138876]\n",
      "[[1.11786517 0.81483868 1.00995551 1.0009572  1.10578527 1.04040637\n",
      "  1.16858972 1.03023408 1.14725285 1.06563682 0.98448498 1.06742486\n",
      "  0.98330419 1.07764581 1.0398703  1.04129394]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "1 loss 39395.64802627243\n",
      "[0.4006075  0.39690932 0.3993584  0.39939961 0.40246327 0.40230114\n",
      " 0.40310573 0.40222534 0.3995528  0.39873545 0.3979227  0.39875442\n",
      " 0.3979126  0.4026795  0.40234416 0.40235841]\n",
      "[[1.11818338 0.81546894 1.01035019 1.00129155 1.10519198 1.03952834\n",
      "  1.16800048 1.0293502  1.14817715 1.06655504 0.98540816 1.06823189\n",
      "  0.98423491 1.07676166 1.03892492 1.04034855]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "2 loss 39366.277837569236\n",
      "[0.40032853 0.39629155 0.39899369 0.39909961 0.40315548 0.40322342\n",
      " 0.40379142 0.40315    0.39859414 0.39777619 0.39696286 0.3977952\n",
      " 0.3969529  0.40360584 0.40331466 0.40332892]\n",
      "[[1.11850003 0.81609868 1.0107436  1.00162436 1.1046047  1.03865617\n",
      "  1.16741744 1.02847171 1.14910273 1.06747466 0.98633273 1.06904153\n",
      "  0.98516727 1.07588306 1.0379788  1.03940242]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "3 loss 39336.83329390903\n",
      "[0.40005272 0.3956751  0.39863163 0.39880275 0.40384011 0.40414135\n",
      " 0.40446969 0.40407096 0.3976343  0.39681578 0.39600188 0.39683482\n",
      " 0.39599173 0.40452811 0.40428598 0.40430024]\n",
      "[[1.11881515 0.81672785 1.01113576 1.00195563 1.10402346 1.03778957\n",
      "  1.1668403  1.0275978  1.15002954 1.06839559 0.98725859 1.06985375\n",
      "  0.98610113 1.07500972 1.03703198 1.03845559]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "4 loss 39307.31567697891\n",
      "[0.39978006 0.39505995 0.39827221 0.39850902 0.40451703 0.40505468\n",
      " 0.40514046 0.40498803 0.39667333 0.39585424 0.39503979 0.39587332\n",
      " 0.39502914 0.40544608 0.40525809 0.40527236]\n",
      "[[1.11912874 0.81735643 1.01152666 1.00228536 1.10344828 1.03692873\n",
      "  1.16626905 1.02672862 1.15095753 1.0693178  0.98818571 1.07066854\n",
      "  0.98703646 1.07414184 1.03608448 1.03750809]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "[0.39978006 0.39505995 0.39827221 0.39850902 0.40451703 0.40505468\n",
      " 0.40514046 0.40498803 0.39667333 0.39585424 0.39503979 0.39587332\n",
      " 0.39502914 0.40544608 0.40525809 0.40527236]\n",
      "[[1.11912874 0.81735643 1.01152666 1.00228536 1.10344828 1.03692873\n",
      "  1.16626905 1.02672862 1.15095753 1.0693178  0.98818571 1.07066854\n",
      "  0.98703646 1.07414184 1.03608448 1.03750809]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "acc 0.13141025641025642\n",
      "[[   6 1626]\n",
      " [   0  240]]\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40160.585458222755\n",
      "[0.50086627 0.49753451 0.49971518 0.49968518 0.50170522 0.50136765\n",
      " 0.50236166 0.50129736 0.5006017  0.49978108 0.49896279 0.49979983\n",
      " 0.49895383 0.50174258 0.50130526 0.50131951]\n",
      "[[1.11788503 0.8148325  1.00996406 1.00097227 1.10584972 1.04041907\n",
      "  1.16864767 1.0302315  1.14716225 1.06553073 0.98438072 1.06650398\n",
      "  0.98321777 1.07765799 1.03994263 1.04136625]]\n",
      "{0: 33, 1: 1839}\n",
      "acc 0.14583333333333334\n",
      "(0.13050570962479607, 1.0, 0.23088023088023085, None)\n",
      "\n",
      "1 loss 40140.954914334354\n",
      "[0.50056004 0.49692024 0.49933522 0.49936333 0.50234427 0.50228197\n",
      " 0.5029987  0.50220925 0.49973009 0.49890188 0.49808258 0.49892082\n",
      " 0.49807556 0.50266135 0.50220797 0.50222224]\n",
      "[[1.11822318 0.81545775 1.01036889 1.00132221 1.10532342 1.03956451\n",
      "  1.16812075 1.02937529 1.14799851 1.06634573 0.98520338 1.06641236\n",
      "  0.984066   1.07679635 1.03906617 1.04048977]]\n",
      "{0: 31, 1: 1841}\n",
      "acc 0.14476495726495728\n",
      "(0.13036393264530147, 1.0, 0.23065833733781838, None)\n",
      "\n",
      "2 loss 40121.12716840679\n",
      "[0.50025568 0.49630639 0.49895672 0.49904338 0.5029757  0.50319158\n",
      " 0.50362795 0.50311643 0.49884931 0.49802011 0.49719942 0.49803923\n",
      " 0.49719365 0.50357568 0.50311272 0.50312701]\n",
      "[[1.11856048 0.81608309 1.01077317 1.00167129 1.10480306 1.03871616\n",
      "  1.16759997 1.02852512 1.14883762 1.06716379 0.98602911 1.06632966\n",
      "  0.98491767 1.07594061 1.03818761 1.03961121]]\n",
      "{0: 30, 1: 1842}\n",
      "acc 0.14423076923076922\n",
      "(0.13029315960912052, 1.0, 0.23054755043227665, None)\n",
      "\n",
      "3 loss 40101.141428287876\n",
      "[0.49995317 0.49569301 0.49857967 0.49872532 0.50359943 0.50409663\n",
      " 0.50424972 0.50401956 0.49796605 0.49713548 0.49631344 0.49715476\n",
      " 0.49630826 0.50448574 0.50401944 0.50403374]\n",
      "[[1.11889698 0.81670845 1.01117689 1.00201951 1.10428861 1.03787367\n",
      "  1.16708495 1.02767994 1.14967949 1.06798478 0.9868578  1.06625603\n",
      "  0.98577258 1.07509046 1.03730705 1.03873063]]\n",
      "{0: 26, 1: 1846}\n",
      "acc 0.1420940170940171\n",
      "(0.13001083423618634, 1.0, 0.23010546500479387, None)\n",
      "\n",
      "4 loss 40081.00877109389\n",
      "[0.49965248 0.49508012 0.49820407 0.49840915 0.50421544 0.50499686\n",
      " 0.50486398 0.50491844 0.49707997 0.49624805 0.49542467 0.49626748\n",
      " 0.49541948 0.50539128 0.50492807 0.50494239]\n",
      "[[1.11923268 0.81733381 1.01158005 1.00236687 1.10378005 1.0370373\n",
      "  1.16657567 1.02683989 1.15052406 1.06880866 0.98768939 1.06619163\n",
      "  0.98663065 1.07424612 1.03642453 1.03784809]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "\n",
      "[0.49965248 0.49508012 0.49820407 0.49840915 0.50421544 0.50499686\n",
      " 0.50486398 0.50491844 0.49707997 0.49624805 0.49542467 0.49626748\n",
      " 0.49541948 0.50539128 0.50492807 0.50494239]\n",
      "[[1.11923268 0.81733381 1.01158005 1.00236687 1.10378005 1.0370373\n",
      "  1.16657567 1.02683989 1.15052406 1.06880866 0.98768939 1.06619163\n",
      "  0.98663065 1.07424612 1.03642453 1.03784809]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "acc 0.1388888888888889\n",
      "[[  20 1612]\n",
      " [   0  240]]\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40592.50416536397\n",
      "[0.60084891 0.59755528 0.59971427 0.59967454 0.60164629 0.60135042\n",
      " 0.60231437 0.6012933  0.60071934 0.59990465 0.59909139 0.600852\n",
      " 0.59907895 0.60172615 0.60120261 0.60121688]\n",
      "[[1.11789788 0.81481143 1.00996214 1.00097956 1.10591029 1.04044834\n",
      "  1.16869882 1.03023618 1.14702747 1.06535968 0.98421101 1.06611201\n",
      "  0.98308755 1.07768625 1.04006489 1.0414885 ]]\n",
      "{0: 279, 1: 1593}\n",
      "acc 0.2761752136752137\n",
      "(0.15003138731952292, 0.9958333333333333, 0.2607746863066012, None)\n",
      "\n",
      "1 loss 40582.1715315032\n",
      "[0.60052497 0.59696073 0.59933125 0.59934105 0.60222397 0.60223427\n",
      " 0.60289581 0.60217497 0.59996742 0.5991531  0.59833813 0.60102094\n",
      " 0.59832528 0.60261581 0.60200484 0.60201913]\n",
      "[[1.11824876 0.81541646 1.01036688 1.0013373  1.1054471  1.03964494\n",
      "  1.16822977 1.02942625 1.1477298  1.06600423 0.98486625 1.06565124\n",
      "  0.98380868 1.07687406 1.0393087  1.04073229]]\n",
      "{0: 273, 1: 1599}\n",
      "acc 0.27297008547008544\n",
      "(0.14946841776110067, 0.9958333333333333, 0.25992387166938546, None)\n",
      "\n",
      "2 loss 40571.72518193695\n",
      "[0.60020181 0.59636569 0.59894861 0.59900846 0.60279442 0.6031132\n",
      " 0.60346968 0.60305151 0.59921105 0.59839712 0.59758047 0.60119259\n",
      " 0.59756643 0.60350084 0.60281048 0.6028248 ]\n",
      "[[1.11859941 0.81602229 1.01077174 1.00169472 1.10498938 1.03884776\n",
      "  1.16776655 1.02862268 1.14843628 1.06665312 0.98552596 1.06518811\n",
      "  0.98453482 1.07606785 1.03854895 1.03997251]]\n",
      "{0: 269, 1: 1603}\n",
      "acc 0.2708333333333333\n",
      "(0.14909544603867747, 0.9958333333333333, 0.25935973955507324, None)\n",
      "\n",
      "3 loss 40561.15560197674\n",
      "[0.59987939 0.59577022 0.59856636 0.59867676 0.60335765 0.60398749\n",
      " 0.60403643 0.60392382 0.59845033 0.59763683 0.59681851 0.60136694\n",
      " 0.59680253 0.60438147 0.60361944 0.60363379]\n",
      "[[1.11894988 0.81662884 1.0111767  1.00205182 1.1045371  1.03805632\n",
      "  1.16730873 1.02782406 1.14914685 1.06730626 0.98619004 1.0647225\n",
      "  0.98526574 1.07526716 1.03778571 1.03920925]]\n",
      "{0: 268, 1: 1604}\n",
      "acc 0.2702991452991453\n",
      "(0.14900249376558602, 0.9958333333333333, 0.2592190889370933, None)\n",
      "\n",
      "4 loss 40550.46224679591\n",
      "[0.5995577  0.59517434 0.59818451 0.59834596 0.60391367 0.60485688\n",
      " 0.60459608 0.60479175 0.59768533 0.59687228 0.59605237 0.60154397\n",
      " 0.59603369 0.60525748 0.60443169 0.60444607]\n",
      "[[1.11930016 0.81723609 1.01158175 1.0024086  1.10409017 1.03727088\n",
      "  1.16685623 1.02703052 1.14986144 1.06796363 0.98685845 1.06425444\n",
      "  0.98600138 1.07447222 1.03701905 1.03844255]]\n",
      "{0: 264, 1: 1608}\n",
      "acc 0.26816239316239315\n",
      "(0.1486318407960199, 0.9958333333333333, 0.25865800865800864, None)\n",
      "\n",
      "[0.5995577  0.59517434 0.59818451 0.59834596 0.60391367 0.60485688\n",
      " 0.60459608 0.60479175 0.59768533 0.59687228 0.59605237 0.60154397\n",
      " 0.59603369 0.60525748 0.60443169 0.60444607]\n",
      "[[1.11930016 0.81723609 1.01158175 1.0024086  1.10409017 1.03727088\n",
      "  1.16685623 1.02703052 1.14986144 1.06796363 0.98685845 1.06425444\n",
      "  0.98600138 1.07447222 1.03701905 1.03844255]]\n",
      "{0: 264, 1: 1608}\n",
      "acc 0.26816239316239315\n",
      "acc 0.26816239316239315\n",
      "[[ 263 1369]\n",
      " [   1  239]]\n",
      "(0.1486318407960199, 0.9958333333333333, 0.25865800865800864, None)\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40860.41526304816\n",
      "[0.70085409 0.69763301 0.69974879 0.69968181 0.7015999  0.70130202\n",
      " 0.70229795 0.70127087 0.70101835 0.70018344 0.69939054 0.70104912\n",
      " 0.69937729 0.70167954 0.70093087 0.70094516]\n",
      "[[1.11788935 0.81473432 1.00992565 1.00096938 1.10595746 1.04052016\n",
      "  1.16872035 1.03027451 1.14674963 1.06500527 0.98384767 1.06602004\n",
      "  0.9827947  1.07775639 1.04038441 1.04180801]]\n",
      "{0: 610, 1: 1262}\n",
      "acc 0.44871794871794873\n",
      "(0.1862123613312203, 0.9791666666666666, 0.31291611185086554, None)\n",
      "\n",
      "1 loss 40857.181150737866\n",
      "[0.70053524 0.69711574 0.69939787 0.69935445 0.70212657 0.7021122\n",
      " 0.70284395 0.7021035  0.70056525 0.69971149 0.69893683 0.70140486\n",
      " 0.6989216  0.70249822 0.70146236 0.70147669]\n",
      "[[1.11823149 0.81526261 1.0102962  1.00131781 1.1055459  1.03982537\n",
      "  1.16829021 1.02953801 1.14717332 1.06529164 0.98413658 1.06548094\n",
      "  0.98322404 1.07705041 1.03994884 1.0413724 ]]\n",
      "{0: 606, 1: 1266}\n",
      "acc 0.4465811965811966\n",
      "(0.18562401263823064, 0.9791666666666666, 0.31208499335989376, None)\n",
      "\n",
      "2 loss 40853.89355860564\n",
      "[0.70021631 0.69659671 0.69904632 0.69902723 0.70264672 0.70291803\n",
      " 0.70338256 0.7029311  0.70010555 0.69923319 0.69847691 0.7017628\n",
      " 0.6984585  0.70331273 0.70199903 0.70201342]\n",
      "[[1.11857399 0.81579285 1.01066765 1.0016664  1.10513911 1.03913528\n",
      "  1.16786569 1.02880711 1.14760195 1.06558165 0.98442945 1.06494007\n",
      "  0.98366001 1.07634901 1.03950729 1.0409308 ]]\n",
      "{0: 600, 1: 1272}\n",
      "acc 0.44337606837606836\n",
      "(0.18474842767295596, 0.9791666666666666, 0.3108465608465608, None)\n",
      "\n",
      "3 loss 40850.54605757701\n",
      "[0.69989725 0.69607595 0.69869417 0.69870013 0.70316035 0.70371998\n",
      " 0.70391434 0.70375477 0.69963927 0.69874964 0.69801069 0.70212302\n",
      " 0.69798803 0.70412353 0.70254011 0.70255455]\n",
      "[[1.11891689 0.81632497 1.01103995 1.00201516 1.1047371  1.03844917\n",
      "  1.16744626 1.02808017 1.14803556 1.06587531 0.98472629 1.06439718\n",
      "  0.98410248 1.07565151 1.03905975 1.04048322]]\n",
      "{0: 593, 1: 1279}\n",
      "acc 0.43963675213675213\n",
      "(0.18373729476153244, 0.9791666666666666, 0.3094140882159315, None)\n",
      "\n",
      "4 loss 40847.13517177441\n",
      "[0.69957807 0.69555349 0.69834142 0.69837317 0.70366748 0.70451787\n",
      " 0.70443931 0.70457443 0.6991664  0.69826025 0.6975382  0.70248549\n",
      " 0.69751021 0.70493044 0.70308688 0.70310136]\n",
      "[[1.11926019 0.81685897 1.01141311 1.00236409 1.1043398  1.03776724\n",
      "  1.16703187 1.02735731 1.14847414 1.06617269 0.98502715 1.06385227\n",
      "  0.98455143 1.07495808 1.03860624 1.04002967]]\n",
      "{0: 582, 1: 1290}\n",
      "acc 0.4337606837606838\n",
      "(0.1821705426356589, 0.9791666666666666, 0.30718954248366015, None)\n",
      "\n",
      "[0.69957807 0.69555349 0.69834142 0.69837317 0.70366748 0.70451787\n",
      " 0.70443931 0.70457443 0.6991664  0.69826025 0.6975382  0.70248549\n",
      " 0.69751021 0.70493044 0.70308688 0.70310136]\n",
      "[[1.11926019 0.81685897 1.01141311 1.00236409 1.1043398  1.03776724\n",
      "  1.16703187 1.02735731 1.14847414 1.06617269 0.98502715 1.06385227\n",
      "  0.98455143 1.07495808 1.03860624 1.04002967]]\n",
      "{0: 582, 1: 1290}\n",
      "acc 0.4337606837606838\n",
      "acc 0.4337606837606838\n",
      "[[ 577 1055]\n",
      " [   5  235]]\n",
      "(0.1821705426356589, 0.9791666666666666, 0.30718954248366015, None)\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 40948.556322521\n",
      "[0.80090103 0.79782484 0.79983838 0.79971898 0.80150169 0.80118307\n",
      " 0.80226173 0.801187   0.80137794 0.80061777 0.79978222 0.80111646\n",
      " 0.79974741 0.80156277 0.80042026 0.80043455]\n",
      "[[1.11783907 0.81454268 1.0098336  1.00092867 1.10604595 1.04066059\n",
      "  1.1687617  1.03040165 1.14648781 1.06478835 0.98363145 1.0659475\n",
      "  0.98244656 1.07789632 1.04093356 1.04235718]]\n",
      "{0: 962, 1: 910}\n",
      "acc 0.6089743589743589\n",
      "(0.22967032967032966, 0.8708333333333333, 0.3634782608695652, None)\n",
      "\n",
      "1 loss 40948.160804551\n",
      "[0.8006288  0.79750266 0.79957666 0.79942788 0.80192081 0.8018074\n",
      " 0.80274326 0.8019022  0.80128992 0.8005845  0.79972257 0.80149868\n",
      " 0.79966894 0.80219873 0.80043459 0.80044893]\n",
      "[[1.11813108 0.81487598 1.01011244 1.00123713 1.10573237 1.04018164\n",
      "  1.16839798 1.0298204  1.14664543 1.06485685 0.98369808 1.06535407\n",
      "  0.9825177  1.07740583 1.04105179 1.04247537]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "2 loss 40947.770988909964\n",
      "[0.8003564  0.79717954 0.79931449 0.79913678 0.80233688 0.80242798\n",
      " 0.80322028 0.80261296 0.80120049 0.80055151 0.79966313 0.80188088\n",
      " 0.79958863 0.80283097 0.80045053 0.80046492]\n",
      "[[1.11842341 0.81521034 1.01039186 1.00154575 1.10542071 1.0397059\n",
      "  1.1680374  1.02924351 1.14680379 1.06492529 0.98376464 1.06476044\n",
      "  0.98259034 1.07691861 1.04116827 1.04259182]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "3 loss 40947.381391936964\n",
      "[0.8000838  0.79685549 0.7990519  0.79884568 0.80274982 0.80304572\n",
      " 0.80369342 0.80332055 0.80110964 0.8005188  0.7996039  0.80226329\n",
      " 0.79950644 0.80346038 0.80046809 0.80048253]\n",
      "[[1.11871609 0.81554574 1.01067185 1.00185453 1.10511105 1.03923242\n",
      "  1.16767942 1.02866948 1.14696287 1.06499366 0.98383109 1.06416623\n",
      "  0.98266444 1.07643367 1.04128301 1.04270653]]\n",
      "{0: 958, 1: 914}\n",
      "acc 0.6068376068376068\n",
      "(0.2286652078774617, 0.8708333333333333, 0.3622183708838822, None)\n",
      "\n",
      "4 loss 40946.991893642175\n",
      "[0.79981099 0.79653051 0.79878887 0.79855458 0.80315961 0.80366058\n",
      " 0.80416269 0.80402495 0.80101734 0.80048638 0.79954489 0.80264591\n",
      " 0.79942237 0.80408693 0.8004873  0.80050179]\n",
      "[[1.11900913 0.8158822  1.01095241 1.00216345 1.10480337 1.0387612\n",
      "  1.16732403 1.02809832 1.14712269 1.06506198 0.98389744 1.06357141\n",
      "  0.98274001 1.07595105 1.04139599 1.04281947]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "\n",
      "[0.79981099 0.79653051 0.79878887 0.79855458 0.80315961 0.80366058\n",
      " 0.80416269 0.80402495 0.80101734 0.80048638 0.79954489 0.80264591\n",
      " 0.79942237 0.80408693 0.8004873  0.80050179]\n",
      "[[1.11900913 0.8158822  1.01095241 1.00216345 1.10480337 1.0387612\n",
      "  1.16732403 1.02809832 1.14712269 1.06506198 0.98389744 1.06357141\n",
      "  0.98274001 1.07595105 1.04139599 1.04281947]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "acc 0.6052350427350427\n",
      "[[923 709]\n",
      " [ 30 210]]\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 41003.452654651395\n",
      "[0.90090747 0.89784935 0.89984971 0.89972232 0.90130818 0.9010178\n",
      " 0.90203485 0.90104743 0.90135027 0.90057044 0.89977106 0.90122268\n",
      " 0.89983185 0.90139571 0.90019427 0.90020853]\n",
      "[[1.1178228  0.81451301 1.00981405 1.00091706 1.10619529 1.04080488\n",
      "  1.168951   1.03055242 1.14648621 1.06482055 0.98365403 1.06575353\n",
      "  0.98241532 1.07804351 1.04111771 1.04254135]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "1 loss 41003.271755162255\n",
      "[0.90064114 0.8975513  0.8995981  0.89943194 0.90153959 0.90134232\n",
      " 0.90232637 0.90150212 0.90123278 0.90049067 0.89969999 0.90166412\n",
      " 0.89983504 0.90172821 0.89998026 0.89999454]\n",
      "[[1.11809924 0.81481697 1.01007443 1.00121655 1.10602361 1.04057934\n",
      "  1.16873607 1.0302237  1.14664289 1.06492037 0.98374452 1.06491125\n",
      "  0.98245863 1.07781205 1.04142043 1.04284406]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "2 loss 41003.09266293903\n",
      "[0.9003747  0.89725301 0.89934631 0.89914151 0.90176987 0.90166348\n",
      " 0.90261559 0.90195141 0.90111511 0.90041104 0.899629   0.90210291\n",
      " 0.89983805 0.90205724 0.89976619 0.8997805 ]\n",
      "[[1.11837587 0.81512122 1.01033506 1.00151614 1.1058524  1.04035602\n",
      "  1.16852232 1.02989924 1.14679975 1.06502009 0.98383498 1.06406729\n",
      "  0.98250207 1.0775829  1.04172265 1.04314628]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "3 loss 41002.91332541443\n",
      "[0.90010812 0.89695453 0.89909435 0.89885106 0.90199889 0.90198219\n",
      " 0.90290283 0.90239672 0.90099729 0.90033157 0.89955814 0.90253948\n",
      " 0.8998409  0.90238374 0.89955207 0.89956641]\n",
      "[[1.1186527  0.8154257  1.01059591 1.00181582 1.10568175 1.04013424\n",
      "  1.16830951 1.02957782 1.14695676 1.06511973 0.98392537 1.06322147\n",
      "  0.98254556 1.07735538 1.04202441 1.04344802]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "4 loss 41002.73372577054\n",
      "[0.8998414  0.89665586 0.89884224 0.8985606  0.90222667 0.90229848\n",
      " 0.9031881  0.90283807 0.90087931 0.90025224 0.89948741 0.90297384\n",
      " 0.89984358 0.90270772 0.89933791 0.89935227]\n",
      "[[1.11892972 0.81573042 1.01085697 1.00211558 1.10551167 1.039914\n",
      "  1.16809765 1.02925943 1.14711391 1.06521929 0.98401569 1.06237376\n",
      "  0.98258912 1.07712947 1.04232569 1.0437493 ]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "[0.8998414  0.89665586 0.89884224 0.8985606  0.90222667 0.90229848\n",
      " 0.9031881  0.90283807 0.90087931 0.90025224 0.89948741 0.90297384\n",
      " 0.89984358 0.90270772 0.89933791 0.89935227]\n",
      "[[1.11892972 0.81573042 1.01085697 1.00211558 1.10551167 1.039914\n",
      "  1.16809765 1.02925943 1.14711391 1.06521929 0.98401569 1.06237376\n",
      "  0.98258912 1.07712947 1.04232569 1.0437493 ]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "acc 0.6666666666666666\n",
      "[[1051  581]\n",
      " [  43  197]]\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normalized\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 41033.940030842794\n",
      "[1.00236172 0.99893934 1.00099467 1.00000736 1.00226036 1.00167788\n",
      " 1.00283767 1.00156842 1.0026252  1.00189305 0.99995454 1.00190966\n",
      " 0.99995435 1.00201392 1.00167275 1.00168684]\n",
      "[[1.1178414  0.81452396 1.00982897 1.00092989 1.10634091 1.04134644\n",
      "  1.16916216 1.03120894 1.14651537 1.06486336 0.98369221 1.0666534\n",
      "  0.98246879 1.07859219 1.04114762 1.04257126]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "1 loss 41033.93973974388\n",
      "[1.00357081 0.99950044 1.00239797 1.00001087 1.00347694 1.00295751\n",
      " 1.00401366 1.00286357 1.00381509 1.00314679 0.99997466 1.00316155\n",
      " 0.99998016 1.00325371 1.00295324 1.00296542]\n",
      "[[1.11813522 0.81483954 1.01010407 1.00124266 1.10630217 1.04134665\n",
      "  1.16912225 1.0312089  1.14670098 1.06500294 0.9838222  1.06689868\n",
      "  0.98256589 1.07859255 1.04147995 1.04290359]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "2 loss 41033.93935449929\n",
      "[1.00471737 0.99981314 1.00360924 1.00002322 1.00462646 1.00413133\n",
      " 1.00514413 1.00404262 1.00495271 1.00431163 0.99997872 1.00432583\n",
      " 0.99999018 1.00441283 1.00412745 1.00413898]\n",
      "[[1.11842912 0.81515517 1.01037924 1.00155546 1.10626393 1.04134756\n",
      "  1.16908335 1.03120985 1.14688676 1.06514265 0.98395225 1.0674294\n",
      "  0.98266308 1.07859379 1.04181231 1.04323595]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "3 loss 41033.93884616661\n",
      "[1.00582957 0.99991754 1.00475384 1.00014501 1.00574003 1.00525784\n",
      " 1.00624669 1.00517176 1.00605974 1.00543378 0.99997656 1.00544788\n",
      " 0.99999554 1.00553167 1.00525425 1.00526543]\n",
      "[[1.11872317 0.81547081 1.01065449 1.00186827 1.10622664 1.04134982\n",
      "  1.16904605 1.03121267 1.1470728  1.06528262 0.98408229 1.06817893\n",
      "  0.98276026 1.0785966  1.04214478 1.04356843]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "4 loss 41033.93818551422\n",
      "[1.00692084 0.99993961 1.00586483 1.00155106 1.00683203 1.00635794\n",
      " 1.00733144 1.00627346 1.00714762 1.00653136 0.99996899 1.00654556\n",
      " 0.99999836 1.00662702 1.00635458 1.00636557]\n",
      "[[1.11901747 0.81578646 1.01092988 1.00218109 1.1061908  1.0413542\n",
      "  1.16901106 1.03121845 1.14725921 1.065423   0.98421232 1.06905886\n",
      "  0.98285742 1.07860184 1.04247741 1.04390106]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "[1.00692084 0.99993961 1.00586483 1.00155106 1.00683203 1.00635794\n",
      " 1.00733144 1.00627346 1.00714762 1.00653136 0.99996899 1.00654556\n",
      " 0.99999836 1.00662702 1.00635458 1.00636557]\n",
      "[[1.11901747 0.81578646 1.01092988 1.00218109 1.1061908  1.0413542\n",
      "  1.16901106 1.03121845 1.14725921 1.065423   0.98421232 1.06905886\n",
      "  0.98285742 1.07860184 1.04247741 1.04390106]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "acc 0.6741452991452992\n",
      "[[1065  567]\n",
      " [  43  197]]\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "for i in np.linspace(0,1,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -10687.358578682832\n",
      "[ 0.00074551 -0.00252045 -0.00038564 -0.00042053  0.00115683  0.00045892\n",
      "  0.00180252  0.00034441  0.00046368 -0.00035245 -0.00116398 -0.00033354\n",
      " -0.00117587  0.00083135  0.00140751  0.00142175]\n",
      "[[1.11797457 0.81487711 1.01004089 1.00105108 1.10628482 1.04129301\n",
      "  1.16909554 1.03117552 1.14732662 1.06571723 0.9845632  1.06761112\n",
      "  0.9833739  1.07853605 1.03982197 1.04124562]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss -10737.422882367582\n",
      "[ 0.00031859 -0.00318888 -0.00086475 -0.00085034  0.00125394  0.00050282\n",
      "  0.00191249  0.00037857 -0.00053624 -0.00135238 -0.00216394 -0.00133347\n",
      " -0.00217583  0.00087525  0.00240695  0.00242119]\n",
      "[[1.11840149 0.81554551 1.01052    1.00148089 1.10618771 1.04124911\n",
      "  1.16898556 1.03114136 1.14831965 1.06671141 0.98555784 1.06860797\n",
      "  0.98436755 1.07849215 1.03883184 1.04025548]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss -10787.538537185632\n",
      "[-0.00010828 -0.00385735 -0.00134384 -0.00128016  0.0013511   0.00054674\n",
      "  0.00202244  0.00041269 -0.00153618 -0.00235234 -0.00316392 -0.00233344\n",
      " -0.00317581  0.00091917  0.00340641  0.00342065]\n",
      "[[1.11882837 0.81621396 1.01099909 1.00191071 1.10609055 1.04120519\n",
      "  1.16887562 1.03110724 1.14931274 1.06770566 0.98655255 1.06960487\n",
      "  0.98536126 1.07844823 1.03784172 1.03926536]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss -10837.704596885986\n",
      "[-0.00053515 -0.00452583 -0.00182292 -0.00170998  0.00144828  0.0005907\n",
      "  0.00213241  0.00044682 -0.00253614 -0.00335232 -0.00416392 -0.00333342\n",
      " -0.00417581  0.00096313  0.00440589  0.00442013]\n",
      "[[1.11925525 0.81688241 1.01147818 1.00234053 1.10599337 1.04116124\n",
      "  1.16876565 1.03107311 1.15030587 1.06869996 0.98754731 1.07060181\n",
      "  0.98635502 1.07840428 1.0368516  1.03827524]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss -10887.920392491895\n",
      "[-0.00096203 -0.00519432 -0.00230202 -0.0021398   0.00154549  0.00063469\n",
      "  0.0022424   0.00048095 -0.00353612 -0.00435232 -0.00516394 -0.00433341\n",
      " -0.00517583  0.00100712  0.00540538  0.00541962]\n",
      "[[1.11968213 0.81755087 1.01195727 1.00277035 1.10589616 1.04111725\n",
      "  1.16865566 1.03103898 1.15129906 1.0696943  0.98854212 1.0715988\n",
      "  0.98734882 1.07836029 1.03586148 1.03728513]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[-0.00096203 -0.00519432 -0.00230202 -0.0021398   0.00154549  0.00063469\n",
      "  0.0022424   0.00048095 -0.00353612 -0.00435232 -0.00516394 -0.00433341\n",
      " -0.00517583  0.00100712  0.00540538  0.00541962]\n",
      "[[1.11968213 0.81755087 1.01195727 1.00277035 1.10589616 1.04111725\n",
      "  1.16865566 1.03103898 1.15129906 1.0696943  0.98854212 1.0715988\n",
      "  0.98734882 1.07836029 1.03586148 1.03728513]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss -9202.01628208825\n",
      "[0.10074565 0.09747973 0.09961456 0.09957948 0.10115641 0.10045837\n",
      " 0.10180204 0.10034464 0.10046471 0.09964859 0.09883705 0.0996675\n",
      " 0.09882516 0.1008308  0.10140647 0.10142071]\n",
      "[[1.11797443 0.81487694 1.01004069 1.00105107 1.10628524 1.04129357\n",
      "  1.16909601 1.03117529 1.14732285 1.06571368 0.98455957 1.06760814\n",
      "  0.98337033 1.07853661 1.03982545 1.0412491 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss -9248.402994353539\n",
      "[0.10031883 0.09681137 0.09913554 0.09914966 0.10125319 0.10050171\n",
      " 0.10191167 0.10037879 0.09946562 0.09864948 0.09783792 0.09866839\n",
      " 0.09782604 0.10087414 0.1024051  0.10241933]\n",
      "[[1.11840126 0.81554529 1.01051972 1.00148089 1.10618846 1.04125023\n",
      "  1.16898638 1.03114113 1.14831256 1.0667046  0.98555118 1.06860205\n",
      "  0.98436095 1.07849327 1.03883865 1.0402623 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss -9294.860106072198\n",
      "[0.09989205 0.09614295 0.09865654 0.09871984 0.10135004 0.10054509\n",
      " 0.1020213  0.10041292 0.09846647 0.09765031 0.09683873 0.09766921\n",
      " 0.09682684 0.10091752 0.10340377 0.10341801]\n",
      "[[1.11882805 0.81621369 1.01099873 1.00191072 1.10609161 1.04120684\n",
      "  1.16887676 1.03110701 1.14930239 1.06769566 0.98654293 1.06959608\n",
      "  0.98535169 1.07844988 1.03785183 1.03927548]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss -9341.385083359986\n",
      "[0.09946526 0.09547453 0.09817753 0.09829003 0.10144692 0.10058855\n",
      " 0.10213095 0.10044705 0.09746725 0.09665108 0.09583948 0.09666998\n",
      " 0.09582759 0.10096098 0.1044025  0.10441674]\n",
      "[[1.11925485 0.8168821  1.01147775 1.00234054 1.10599473 1.04116339\n",
      "  1.16876711 1.03107288 1.15029232 1.06868682 0.98753478 1.07059021\n",
      "  0.98634252 1.07840642 1.03686499 1.03828864]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss -9387.976886215505\n",
      "[0.09903847 0.0948061  0.09769851 0.09786021 0.10154384 0.10063207\n",
      " 0.10224065 0.10048118 0.096468   0.0956518  0.09484018 0.09567071\n",
      " 0.09482829 0.1010045  0.10540127 0.10541551]\n",
      "[[1.11968166 0.81755052 1.01195677 1.00277037 1.10589781 1.04111986\n",
      "  1.16865741 1.03103874 1.15128234 1.06967808 0.98852672 1.07158445\n",
      "  0.98733343 1.0783629  1.03587813 1.03730178]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.09903847 0.0948061  0.09769851 0.09786021 0.10154384 0.10063207\n",
      " 0.10224065 0.10048118 0.096468   0.0956518  0.09484018 0.09567071\n",
      " 0.09482829 0.1010045  0.10540127 0.10541551]\n",
      "[[1.11968166 0.81755052 1.01195677 1.00277037 1.10589781 1.04111986\n",
      "  1.16865741 1.03103874 1.15128234 1.06967808 0.98852672 1.07158445\n",
      "  0.98733343 1.0783629  1.03587813 1.03730178]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -7750.354155369171\n",
      "[0.20074586 0.19748011 0.19961495 0.19957951 0.20115577 0.20045727\n",
      " 0.20180126 0.20034499 0.20046787 0.19965174 0.1988402  0.19967065\n",
      " 0.19882832 0.2008297  0.20140333 0.20141756]\n",
      "[[1.11797423 0.81487657 1.01004031 1.00105105 1.10628588 1.04129466\n",
      "  1.1690968  1.03117493 1.14731552 1.06570633 0.98455221 1.06760058\n",
      "  0.98336342 1.0785377  1.03983212 1.04125577]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "1 loss -7792.398676717837\n",
      "[0.20031922 0.19681192 0.19913611 0.19914972 0.20125199 0.20049955\n",
      " 0.20191025 0.20037907 0.19947141 0.19865527 0.19784371 0.19867418\n",
      " 0.19783182 0.20087198 0.20239932 0.20241355]\n",
      "[[1.11840089 0.81554476 1.01051916 1.00148085 1.10618966 1.04125239\n",
      "  1.16898781 1.03114086 1.14829871 1.06669053 0.98553751 1.06858709\n",
      "  0.98434807 1.07849543 1.03885151 1.04027515]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "2 loss -7834.543814109119\n",
      "[0.19989261 0.19614367 0.19865729 0.19871992 0.20134831 0.20054193\n",
      " 0.20201925 0.20041311 0.19847478 0.19765862 0.19684704 0.19767753\n",
      " 0.19683515 0.20091436 0.20339547 0.20340971]\n",
      "[[1.11882751 0.81621301 1.010998   1.00191066 1.10609334 1.04121001\n",
      "  1.16887881 1.03110681 1.14928215 1.06767501 0.98652309 1.06957391\n",
      "  0.98533297 1.07845305 1.03787079 1.03929443]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "3 loss -7876.784535836327\n",
      "[0.19946599 0.19547541 0.19817844 0.19829012 0.20144468 0.20058443\n",
      " 0.20212832 0.20044718 0.197478   0.19666182 0.19585022 0.19668073\n",
      " 0.19583833 0.20095686 0.20439177 0.20440601]\n",
      "[[1.11925415 0.81688128 1.01147686 1.00234047 1.10599698 1.04116751\n",
      "  1.16876974 1.03107275 1.15026581 1.06865973 0.98750889 1.07056099\n",
      "  0.98631807 1.07841055 1.03688996 1.0383136 ]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "4 loss -7919.119302966023\n",
      "[0.19903935 0.19480713 0.19769959 0.19786032 0.2015411  0.20062704\n",
      " 0.20223745 0.20048126 0.19648108 0.19566488 0.19485326 0.19568379\n",
      " 0.19484137 0.20099947 0.20538821 0.20540245]\n",
      "[[1.1196808  0.81754956 1.01195574 1.00277028 1.10590055 1.04112489\n",
      "  1.16866061 1.03103867 1.15124968 1.06964467 0.98849491 1.07154832\n",
      "  0.98730337 1.07836793 1.03590903 1.03733267]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "\n",
      "[0.19903935 0.19480713 0.19769959 0.19786032 0.2015411  0.20062704\n",
      " 0.20223745 0.20048126 0.19648108 0.19566488 0.19485326 0.19568379\n",
      " 0.19484137 0.20099947 0.20538821 0.20540245]\n",
      "[[1.1196808  0.81754956 1.01195574 1.00277028 1.10590055 1.04112489\n",
      "  1.16866061 1.03103867 1.15124968 1.06964467 0.98849491 1.07154832\n",
      "  0.98730337 1.07836793 1.03590903 1.03733267]]\n",
      "{1: 1872}\n",
      "acc 0.1282051282051282\n",
      "acc 0.1282051282051282\n",
      "[[   0 1632]\n",
      " [   0  240]]\n",
      "(0.1282051282051282, 1.0, 0.22727272727272727, None)\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -6360.545426098522\n",
      "[0.3007462  0.29748109 0.2996158  0.29957967 0.3011548  0.30045512\n",
      " 0.30179998 0.30034551 0.30047701 0.29966089 0.29884935 0.2996798\n",
      " 0.29883746 0.30082755 0.30139419 0.30140842]\n",
      "[[1.11797389 0.8148756  1.01003946 1.0010509  1.10628685 1.04129682\n",
      "  1.16909808 1.03117442 1.14730014 1.06569007 0.98453611 1.06757954\n",
      "  0.98334895 1.07853986 1.03984636 1.04127   ]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "1 loss -6397.139824145165\n",
      "[0.30031992 0.29681352 0.29913744 0.29915003 0.30125011 0.30049537\n",
      " 0.30190777 0.30037921 0.29948857 0.29867243 0.29786087 0.29869134\n",
      " 0.29784898 0.3008678  0.30238218 0.30239641]\n",
      "[[1.11840019 0.81554319 1.01051785 1.00148055 1.10619154 1.04125657\n",
      "  1.16899029 1.03114072 1.14826935 1.06665927 0.98550712 1.06854519\n",
      "  0.98432075 1.07849961 1.03887876 1.0403024 ]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "2 loss -6433.879150886576\n",
      "[0.29989367 0.29614586 0.29865906 0.29872038 0.30134552 0.30053581\n",
      " 0.30201563 0.3004129  0.29849966 0.29768351 0.29687193 0.29770242\n",
      " 0.29686004 0.30090824 0.30337061 0.30338485]\n",
      "[[1.11882647 0.81621088 1.01099626 1.00191022 1.10609613 1.04121612\n",
      "  1.16888244 1.03110703 1.14923911 1.06762907 0.98647873 1.06951163\n",
      "  0.98529309 1.07845916 1.0379108  1.03933445]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "3 loss -6470.754983813509\n",
      "[0.29946739 0.29547816 0.29818065 0.29829072 0.30144102 0.30057647\n",
      " 0.30212359 0.30044664 0.29751035 0.29669417 0.29588257 0.29671308\n",
      " 0.29587068 0.3009489  0.30435946 0.3043737 ]\n",
      "[[1.11925277 0.8168786  1.01147469 1.0023399  1.10600063 1.04117547\n",
      "  1.16877448 1.03107328 1.15020938 1.06859942 0.98745088 1.07047879\n",
      "  0.98626591 1.07841851 1.0369425  1.03836615]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "4 loss -6507.765250601397\n",
      "[0.29904108 0.29481042 0.29770221 0.29786106 0.3015366  0.30061733\n",
      " 0.30223165 0.30048043 0.29652063 0.29570444 0.29489282 0.29572335\n",
      " 0.29488093 0.30098976 0.30534871 0.30536294]\n",
      "[[1.11967911 0.81754637 1.01195316 1.00276959 1.10590505 1.0411346\n",
      "  1.16866641 1.0310395  1.15118015 1.0695703  0.98842355 1.07144665\n",
      "  0.98723918 1.07837764 1.03597387 1.03739752]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "\n",
      "[0.29904108 0.29481042 0.29770221 0.29786106 0.3015366  0.30061733\n",
      " 0.30223165 0.30048043 0.29652063 0.29570444 0.29489282 0.29572335\n",
      " 0.29488093 0.30098976 0.30534871 0.30536294]\n",
      "[[1.11967911 0.81754637 1.01195316 1.00276959 1.10590505 1.0411346\n",
      "  1.16866641 1.0310395  1.15118015 1.0695703  0.98842355 1.07144665\n",
      "  0.98723918 1.07837764 1.03597387 1.03739752]]\n",
      "{0: 3, 1: 1869}\n",
      "acc 0.12980769230769232\n",
      "acc 0.12980769230769232\n",
      "[[   3 1629]\n",
      " [   0  240]]\n",
      "(0.12841091492776885, 1.0, 0.22759601706970128, None)\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -5080.835258044225\n",
      "[0.40074692 0.39748417 0.39961805 0.39958041 0.40115332 0.40045066\n",
      " 0.40179784 0.40034611 0.40050271 0.39968659 0.39887505 0.3997055\n",
      " 0.39886316 0.40082309 0.4013685  0.40138274]\n",
      "[[1.11797317 0.81487254 1.01003722 1.00105015 1.10628833 1.04130128\n",
      "  1.16910022 1.03117382 1.14726601 1.06565254 0.98449896 1.0675087\n",
      "  0.98331675 1.07854432 1.03987905 1.0413027 ]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "1 loss -5110.263417923494\n",
      "[0.40032155 0.39681908 0.39914123 0.39915158 0.40124703 0.40048677\n",
      " 0.40190336 0.4003784  0.39953788 0.39872174 0.39791018 0.39874066\n",
      " 0.39789829 0.4008592  0.40233291 0.40234714]\n",
      "[[1.11839858 0.81553768 1.01051408 1.00147902 1.10619462 1.04126517\n",
      "  1.16899471 1.03114153 1.14820325 1.06658632 0.98543569 1.06840211\n",
      "  0.98425897 1.07850821 1.03894176 1.04036541]]\n",
      "{0: 7, 1: 1865}\n",
      "acc 0.13194444444444445\n",
      "(0.128686327077748, 1.0, 0.22802850356294535, None)\n",
      "\n",
      "2 loss -5139.891042535368\n",
      "[0.39989617 0.3961538  0.39866434 0.39872271 0.40134089 0.40052326\n",
      " 0.40200901 0.40041077 0.39857186 0.39775571 0.39694413 0.39777464\n",
      " 0.39693224 0.40089569 0.40329848 0.40331272]\n",
      "[[1.118824   0.81620301 1.01099102 1.00190792 1.10610076 1.04122868\n",
      "  1.16888906 1.03110916 1.14914175 1.06752147 0.9863738  1.06929784\n",
      "  0.98520242 1.07847172 1.03800344 1.03942708]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "3 loss -5169.70688100588\n",
      "[0.39947074 0.39548838 0.39818736 0.3982938  0.40143488 0.40056013\n",
      " 0.40211483 0.40044328 0.39760474 0.39678857 0.39597698 0.39680751\n",
      " 0.39596508 0.40093256 0.40426516 0.4042794 ]\n",
      "[[1.11924947 0.81686848 1.01146804 1.00233686 1.10600678 1.04119181\n",
      "  1.16878323 1.03107665 1.15008143 1.06845789 0.98731316 1.0701958\n",
      "  0.98614698 1.07843485 1.03706414 1.03848779]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "4 loss -5199.708669461549\n",
      "[0.39904525 0.39482283 0.39771029 0.39786486 0.40152898 0.40059736\n",
      " 0.40222083 0.40047592 0.39663655 0.39582037 0.39500875 0.39583932\n",
      " 0.39499686 0.40096979 0.4052329  0.40524714]\n",
      "[[1.119675   0.81753408 1.01194514 1.00276583 1.10591267 1.04115457\n",
      "  1.16867724 1.03104401 1.15102225 1.06939555 0.98825375 1.07109595\n",
      "  0.98709262 1.07839761 1.03612391 1.03754756]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "\n",
      "[0.39904525 0.39482283 0.39771029 0.39786486 0.40152898 0.40059736\n",
      " 0.40222083 0.40047592 0.39663655 0.39582037 0.39500875 0.39583932\n",
      " 0.39499686 0.40096979 0.4052329  0.40524714]\n",
      "[[1.119675   0.81753408 1.01194514 1.00276583 1.10591267 1.04115457\n",
      "  1.16867724 1.03104401 1.15102225 1.06939555 0.98825375 1.07109595\n",
      "  0.98709262 1.07839761 1.03612391 1.03754756]]\n",
      "{0: 6, 1: 1866}\n",
      "acc 0.13141025641025642\n",
      "acc 0.13141025641025642\n",
      "[[   6 1626]\n",
      " [   0  240]]\n",
      "(0.12861736334405144, 1.0, 0.2279202279202279, None)\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -3988.283968083965\n",
      "[0.50074931 0.49749519 0.49962509 0.49958382 0.50115065 0.50044069\n",
      " 0.5017939  0.50034589 0.50058233 0.49976354 0.4989476  0.49978223\n",
      " 0.49893571 0.50081312 0.50128893 0.50130316]\n",
      "[[1.1179708  0.81486153 1.0100302  1.00104676 1.106291   1.04131124\n",
      "  1.16910416 1.03117404 1.14718714 1.06556345 0.98441005 1.06703492\n",
      "  0.98324168 1.07855428 1.03995944 1.04138309]]\n",
      "{0: 33, 1: 1839}\n",
      "acc 0.14583333333333334\n",
      "(0.13050570962479607, 1.0, 0.23088023088023085, None)\n",
      "\n",
      "1 loss -4008.1010770720677\n",
      "[0.50032678 0.49684025 0.49915402 0.49915863 0.50124123 0.5004675\n",
      " 0.50189478 0.50037309 0.49969108 0.49886776 0.49805225 0.49888659\n",
      " 0.49804035 0.50083993 0.50217639 0.50219063]\n",
      "[[1.11839338 0.81551656 1.01050132 1.00147199 1.10620042 1.04128443\n",
      "  1.16900328 1.03114684 1.14804816 1.06641105 0.98526172 1.06744792\n",
      "  0.98411267 1.07852747 1.03909887 1.04052252]]\n",
      "{0: 31, 1: 1841}\n",
      "acc 0.14476495726495728\n",
      "(0.13036393264530147, 1.0, 0.23065833733781838, None)\n",
      "\n",
      "2 loss -4028.206075834522\n",
      "[0.49990416 0.49618479 0.49868269 0.49873332 0.50133205 0.50049501\n",
      " 0.50199592 0.50040064 0.49879251 0.49796953 0.49715401 0.49798846\n",
      " 0.49714211 0.50086744 0.50306662 0.50308085]\n",
      "[[1.11881605 0.81617211 1.01097271 1.00189735 1.10610961 1.04125692\n",
      "  1.16890214 1.03111929 1.14891191 1.06726157 0.98611637 1.06787263\n",
      "  0.98498633 1.07849996 1.03823561 1.03965926]]\n",
      "{0: 30, 1: 1842}\n",
      "acc 0.14423076923076922\n",
      "(0.13029315960912052, 1.0, 0.23054755043227665, None)\n",
      "\n",
      "3 loss -4048.54347129609\n",
      "[0.49948141 0.49552887 0.49821112 0.4983079  0.50142306 0.50052321\n",
      " 0.50209736 0.5004286  0.49789153 0.49706855 0.49625302 0.49708757\n",
      " 0.49624111 0.50089564 0.50395948 0.50397372]\n",
      "[[1.11923885 0.81682812 1.01144434 1.00232282 1.10601859 1.04122873\n",
      "  1.16880071 1.03109133 1.14977826 1.06811488 0.98697384 1.06830901\n",
      "  0.98586251 1.07847177 1.03736976 1.03879341]]\n",
      "{0: 25, 1: 1847}\n",
      "acc 0.14155982905982906\n",
      "(0.12994044396318355, 1.0, 0.22999520843315766, None)\n",
      "\n",
      "4 loss -4069.1039060602825\n",
      "[0.49905853 0.49487251 0.4977393  0.49788236 0.50151427 0.50055207\n",
      " 0.50219908 0.50045694 0.49698788 0.49616488 0.49534933 0.49618399\n",
      " 0.49533742 0.5009245  0.50485492 0.50486916]\n",
      "[[1.11966179 0.81748456 1.01191622 1.00274841 1.10592739 1.04119986\n",
      "  1.16869899 1.03106299 1.15064718 1.06897092 0.98783408 1.06875695\n",
      "  0.98674114 1.0784429  1.0365014  1.03792505]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "\n",
      "[0.49905853 0.49487251 0.4977393  0.49788236 0.50151427 0.50055207\n",
      " 0.50219908 0.50045694 0.49698788 0.49616488 0.49534933 0.49618399\n",
      " 0.49533742 0.5009245  0.50485492 0.50486916]\n",
      "[[1.11966179 0.81748456 1.01191622 1.00274841 1.10592739 1.04119986\n",
      "  1.16869899 1.03106299 1.15064718 1.06897092 0.98783408 1.06875695\n",
      "  0.98674114 1.0784429  1.0365014  1.03792505]]\n",
      "{0: 20, 1: 1852}\n",
      "acc 0.1388888888888889\n",
      "acc 0.1388888888888889\n",
      "[[  20 1612]\n",
      " [   0  240]]\n",
      "(0.12958963282937366, 1.0, 0.22944550669216063, None)\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -3300.498607896117\n",
      "[0.60075523 0.59752011 0.5996402  0.59959192 0.60114711 0.60043068\n",
      " 0.60178956 0.60030843 0.60068965 0.59987695 0.59906594 0.60051606\n",
      " 0.59905022 0.60080312 0.60117657 0.60119081]\n",
      "[[1.11796489 0.81483665 1.0100151  1.00103867 1.10629454 1.04132125\n",
      "  1.1691085  1.0312115  1.14706592 1.06541675 0.9842628  1.06673517\n",
      "  0.98312537 1.07856429 1.04009117 1.04151482]]\n",
      "{0: 279, 1: 1593}\n",
      "acc 0.2761752136752137\n",
      "(0.15003138731952292, 0.9958333333333333, 0.2607746863066012, None)\n",
      "\n",
      "1 loss -3310.8562122332505\n",
      "[0.60033913 0.59688947 0.59918262 0.5991749  0.60123358 0.60044729\n",
      " 0.6018848  0.60032612 0.5999079  0.59909762 0.59828712 0.60037036\n",
      " 0.59826856 0.60081972 0.60195383 0.60196806]\n",
      "[[1.11838106 0.81546741 1.01047276 1.00145576 1.10620807 1.04130464\n",
      "  1.16901326 1.03119381 1.14780687 1.0661187  0.98497014 1.06684756\n",
      "  0.98388345 1.07854768 1.0393604  1.04078405]]\n",
      "{0: 273, 1: 1599}\n",
      "acc 0.27297008547008544\n",
      "(0.14946841776110067, 0.9958333333333333, 0.25992387166938546, None)\n",
      "\n",
      "2 loss -3321.3797241896264\n",
      "[0.59992283 0.59625782 0.59872453 0.59875767 0.60132036 0.60046473\n",
      " 0.6019804  0.60034452 0.5991218  0.59831393 0.59750393 0.60022464\n",
      " 0.59748256 0.60083716 0.60273548 0.60274971]\n",
      "[[1.11879743 0.8160992  1.01093093 1.00187306 1.1061213  1.0412872\n",
      "  1.16891767 1.03117541 1.14855183 1.06682487 0.98568185 1.06695999\n",
      "  0.98464561 1.07853024 1.03862522 1.04004887]]\n",
      "{0: 269, 1: 1603}\n",
      "acc 0.2708333333333333\n",
      "(0.14909544603867747, 0.9958333333333333, 0.25935973955507324, None)\n",
      "\n",
      "3 loss -3332.066366562339\n",
      "[0.59950626 0.59562522 0.59826594 0.59834025 0.60140742 0.600483\n",
      " 0.60207642 0.60036362 0.59833148 0.59752601 0.5967165  0.60007888\n",
      " 0.59669234 0.60085543 0.60352141 0.60353564]\n",
      "[[1.11921408 0.81673193 1.01138961 1.00229055 1.10603424 1.04126894\n",
      "  1.16882166 1.03115631 1.1493007  1.06753516 0.98639782 1.06707249\n",
      "  0.98541166 1.07851197 1.03788573 1.03930939]]\n",
      "{0: 268, 1: 1604}\n",
      "acc 0.2702991452991453\n",
      "(0.14900249376558602, 0.9958333333333333, 0.2592190889370933, None)\n",
      "\n",
      "4 loss -3342.9165029902497\n",
      "[0.59908944 0.5949917  0.59780687 0.59792263 0.60149476 0.60050209\n",
      " 0.60217283 0.6003834  0.59753701 0.59673393 0.59592493 0.59992976\n",
      " 0.59589799 0.60087452 0.60431154 0.60432578]\n",
      "[[1.11963097 0.81736559 1.01184876 1.00270823 1.10594691 1.04124985\n",
      "  1.16872525 1.03113653 1.15005345 1.06824952 0.98711798 1.06718505\n",
      "  0.98618154 1.07849289 1.03714202 1.03856568]]\n",
      "{0: 263, 1: 1609}\n",
      "acc 0.2676282051282051\n",
      "(0.1485394655065258, 0.9958333333333333, 0.2585181179015685, None)\n",
      "\n",
      "[0.59908944 0.5949917  0.59780687 0.59792263 0.60149476 0.60050209\n",
      " 0.60217283 0.6003834  0.59753701 0.59673393 0.59592493 0.59992976\n",
      " 0.59589799 0.60087452 0.60431154 0.60432578]\n",
      "[[1.11963097 0.81736559 1.01184876 1.00270823 1.10594691 1.04124985\n",
      "  1.16872525 1.03113653 1.15005345 1.06824952 0.98711798 1.06718505\n",
      "  0.98618154 1.07849289 1.03714202 1.03856568]]\n",
      "{0: 263, 1: 1609}\n",
      "acc 0.2676282051282051\n",
      "acc 0.2676282051282051\n",
      "[[ 262 1370]\n",
      " [   1  239]]\n",
      "(0.1485394655065258, 0.9958333333333333, 0.2585181179015685, None)\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -2867.7007037488856\n",
      "[0.70077615 0.69759778 0.69968503 0.6996132  0.70113616 0.70041712\n",
      " 0.70177686 0.70030642 0.70097146 0.7001402  0.69934963 0.70056438\n",
      " 0.69933324 0.70078955 0.70088827 0.7009025 ]\n",
      "[[1.11794398 0.81475899 1.00997028 1.0010174  1.10630549 1.04133481\n",
      "  1.1691212  1.03121351 1.14680914 1.06510196 0.98393857 1.06670154\n",
      "  0.98286015 1.07857785 1.04042628 1.04184993]]\n",
      "{0: 610, 1: 1262}\n",
      "acc 0.44871794871794873\n",
      "(0.1862123613312203, 0.9791666666666666, 0.31291611185086554, None)\n",
      "\n",
      "1 loss -2870.9130461605596\n",
      "[0.7003815  0.69704455 0.69926999 0.69921692 0.70121061 0.70041724\n",
      " 0.70185719 0.70031026 0.70047081 0.69962427 0.69885429 0.70046428\n",
      " 0.69883366 0.70078967 0.70137804 0.70139228]\n",
      "[[1.11833873 0.81531239 1.01038542 1.00141378 1.10623105 1.04133469\n",
      "  1.16904087 1.03120967 1.14729289 1.06548568 0.98432032 1.06678167\n",
      "  0.98335535 1.07857773 1.04003174 1.0414554 ]]\n",
      "{0: 606, 1: 1266}\n",
      "acc 0.4465811965811966\n",
      "(0.18562401263823064, 0.9791666666666666, 0.31208499335989376, None)\n",
      "\n",
      "2 loss -2874.206988767118\n",
      "[0.69998633 0.69648923 0.69885389 0.69882021 0.70128563 0.70041808\n",
      " 0.7019382  0.70031459 0.69996338 0.69910179 0.69835251 0.70036418\n",
      " 0.69832729 0.70079051 0.70187466 0.7018889 ]\n",
      "[[1.118734   0.81586788 1.01080163 1.00181059 1.10615603 1.04133386\n",
      "  1.16895986 1.03120534 1.14778165 1.06587341 0.98470638 1.06686181\n",
      "  0.98385625 1.0785769  1.03963017 1.04105384]]\n",
      "{0: 599, 1: 1273}\n",
      "acc 0.4428418803418803\n",
      "(0.18460329929300864, 0.9791666666666666, 0.310641110376735, None)\n",
      "\n",
      "3 loss -2877.5810893765506\n",
      "[0.6995906  0.69593189 0.69843676 0.69842308 0.70136118 0.70041961\n",
      " 0.70201995 0.70031935 0.6994492  0.69857413 0.69784419 0.70026407\n",
      " 0.69781416 0.70079204 0.70237616 0.70239039]\n",
      "[[1.11912984 0.81642539 1.01121887 1.00220782 1.10608048 1.04133232\n",
      "  1.16887813 1.03120058 1.14827543 1.06626514 0.98509675 1.06694198\n",
      "  0.98436274 1.07857536 1.03922157 1.04064524]]\n",
      "{0: 592, 1: 1280}\n",
      "acc 0.4391025641025641\n",
      "(0.18359375, 0.9791666666666666, 0.3092105263157895, None)\n",
      "\n",
      "4 loss -2881.0369617723823\n",
      "[0.6991943  0.69537255 0.6980186  0.69802552 0.70143727 0.70042187\n",
      " 0.70210241 0.70032454 0.6989283  0.69804021 0.69732941 0.70016395\n",
      " 0.69729428 0.7007943  0.70288473 0.70289897]\n",
      "[[1.11952623 0.8169849  1.01163714 1.00260547 1.1060044  1.04133006\n",
      "  1.16879567 1.03119538 1.14877423 1.06666092 0.98549148 1.06702218\n",
      "  0.98487481 1.0785731  1.03880594 1.04022962]]\n",
      "{0: 581, 1: 1291}\n",
      "acc 0.43322649572649574\n",
      "(0.1820294345468629, 0.9791666666666666, 0.3069888961463096, None)\n",
      "\n",
      "[0.6991943  0.69537255 0.6980186  0.69802552 0.70143727 0.70042187\n",
      " 0.70210241 0.70032454 0.6989283  0.69804021 0.69732941 0.70016395\n",
      " 0.69729428 0.7007943  0.70288473 0.70289897]\n",
      "[[1.11952623 0.8169849  1.01163714 1.00260547 1.1060044  1.04133006\n",
      "  1.16879567 1.03119538 1.14877423 1.06666092 0.98549148 1.06702218\n",
      "  0.98487481 1.0785731  1.03880594 1.04022962]]\n",
      "{0: 581, 1: 1291}\n",
      "acc 0.43322649572649574\n",
      "acc 0.43322649572649574\n",
      "[[ 576 1056]\n",
      " [   5  235]]\n",
      "(0.1820294345468629, 0.9791666666666666, 0.3069888961463096, None)\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -2690.103459189755\n",
      "[0.8008366  0.79779031 0.79978485 0.79966439 0.80110987 0.80039632\n",
      " 0.80174409 0.80030565 0.80133046 0.80049074 0.79968039 0.80059352\n",
      " 0.79970173 0.80076875 0.80037789 0.80039213]\n",
      "[[1.11788354 0.81456644 1.00987046 1.00096623 1.10633178 1.04135561\n",
      "  1.16915397 1.03121428 1.14654918 1.0648777  0.98371498 1.06667275\n",
      "  0.98253359 1.07859865 1.04096801 1.04239166]]\n",
      "{0: 961, 1: 911}\n",
      "acc 0.6084401709401709\n",
      "(0.22941822173435786, 0.8708333333333333, 0.3631624674196351, None)\n",
      "\n",
      "1 loss -2690.477447476015\n",
      "[0.80050233 0.79743294 0.79946918 0.79931863 0.80115702 0.80038611\n",
      " 0.80179106 0.80030169 0.80119419 0.80033423 0.79951597 0.80052272\n",
      " 0.79957756 0.80075854 0.80035025 0.80036449]\n",
      "[[1.11821793 0.81492393 1.01018623 1.00131211 1.10628463 1.04136582\n",
      "  1.169107   1.03121823 1.14676884 1.06503217 0.98386667 1.06672531\n",
      "  0.98269487 1.07860886 1.04112015 1.04254381]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "2 loss -2690.85409536894\n",
      "[0.80016764 0.79707437 0.79915281 0.79897252 0.80120468 0.80037616\n",
      " 0.8018387  0.8002979  0.80105621 0.80017758 0.79935129 0.80045191\n",
      " 0.79945164 0.80074859 0.80032456 0.8003388 ]\n",
      "[[1.11855275 0.81528263 1.01050271 1.00165835 1.10623698 1.04137577\n",
      "  1.16905937 1.03122203 1.14698929 1.06518677 0.98401854 1.06677788\n",
      "  0.98285732 1.07861881 1.04127027 1.04269394]]\n",
      "{0: 959, 1: 913}\n",
      "acc 0.6073717948717948\n",
      "(0.2289156626506024, 0.8708333333333333, 0.3625325238508239, None)\n",
      "\n",
      "3 loss -2691.2335377063396\n",
      "[0.79983248 0.7967146  0.79883574 0.79862608 0.80125282 0.80036643\n",
      " 0.80188702 0.80029423 0.80091649 0.80002079 0.79918641 0.8003811\n",
      " 0.79932393 0.80073886 0.80030084 0.80031508]\n",
      "[[1.11888803 0.81564252 1.01081989 1.00200492 1.10618884 1.0413855\n",
      "  1.16901104 1.03122569 1.14721056 1.06534151 0.98417056 1.06683049\n",
      "  0.98302088 1.07862854 1.04141835 1.04284203]]\n",
      "{0: 956, 1: 916}\n",
      "acc 0.6068376068376068\n",
      "(0.2292576419213974, 0.875, 0.3633217993079585, None)\n",
      "\n",
      "4 loss -2691.615834538186\n",
      "[0.79949685 0.79635363 0.79851798 0.79827929 0.80130145 0.80035693\n",
      " 0.80193605 0.8002907  0.80077501 0.79986386 0.79902133 0.80031029\n",
      " 0.79919442 0.80072936 0.80027912 0.80029336]\n",
      "[[1.11922379 0.81600363 1.01113776 1.00235184 1.10614021 1.04139501\n",
      "  1.16896202 1.03122922 1.14743266 1.06549638 0.98432272 1.06688312\n",
      "  0.98318555 1.07863805 1.04156438 1.04298807]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "\n",
      "[0.79949685 0.79635363 0.79851798 0.79827929 0.80130145 0.80035693\n",
      " 0.80193605 0.8002907  0.80077501 0.79986386 0.79902133 0.80031029\n",
      " 0.79919442 0.80072936 0.80027912 0.80029336]\n",
      "[[1.11922379 0.81600363 1.01113776 1.00235184 1.10614021 1.04139501\n",
      "  1.16896202 1.03122922 1.14743266 1.06549638 0.98432272 1.06688312\n",
      "  0.98318555 1.07863805 1.04156438 1.04298807]]\n",
      "{0: 953, 1: 919}\n",
      "acc 0.6052350427350427\n",
      "acc 0.6052350427350427\n",
      "[[923 709]\n",
      " [ 30 210]]\n",
      "(0.22850924918389554, 0.875, 0.36238136324417597, None)\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -2600.6231735051992\n",
      "[0.90087523 0.8978322  0.89982332 0.89969499 0.90110341 0.90040492\n",
      " 0.90173881 0.90031137 0.90130793 0.90050534 0.89971274 0.90063307\n",
      " 0.89976881 0.90077736 0.90017172 0.90018596]\n",
      "[[1.11784497 0.81452461 1.00983203 1.0009357  1.10633824 1.04134701\n",
      "  1.16915925 1.03120856 1.14651599 1.06486522 0.98369363 1.0666413\n",
      "  0.98246781 1.07859005 1.04113358 1.04255724]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "1 loss -2600.816636349613\n",
      "[0.90057854 0.89751664 0.89954533 0.8993773  0.90114547 0.90040413\n",
      " 0.90178276 0.9003112  0.90114819 0.90036395 0.89958195 0.90059928\n",
      " 0.89971043 0.90077656 0.89993537 0.89994961]\n",
      "[[1.11814187 0.81484038 1.01011021 1.00125362 1.10629618 1.0413478\n",
      "  1.16911531 1.03120873 1.14670231 1.06500665 0.98382446 1.06666985\n",
      "  0.98256318 1.07859084 1.04145178 1.04287545]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "2 loss -2601.011016786469\n",
      "[0.90028165 0.89720079 0.89926706 0.89905941 0.90118809 0.90040358\n",
      " 0.90182747 0.90031118 0.90098824 0.9002226  0.89945111 0.90056548\n",
      " 0.89965209 0.90077601 0.89969893 0.89971316]\n",
      "[[1.11843898 0.81515646 1.01038866 1.00157175 1.10625356 1.04134835\n",
      "  1.1690706  1.03120875 1.14688875 1.06514805 0.98395533 1.06669844\n",
      "  0.98265851 1.07859139 1.04176948 1.04319316]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "3 loss -2601.2062372365494\n",
      "[0.89998452 0.89688468 0.89898855 0.89874135 0.90123123 0.90040324\n",
      " 0.90187296 0.90031129 0.90082812 0.90008127 0.89932029 0.90053167\n",
      " 0.89959382 0.90077567 0.8994624  0.89947663]\n",
      "[[1.11873632 0.8154728  1.01066735 1.00189005 1.10621044 1.04134869\n",
      "  1.16902511 1.03120863 1.1470753  1.06528943 0.9840862  1.06672705\n",
      "  0.98275369 1.07859173 1.04208671 1.04351039]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "4 loss -2601.4022978469106\n",
      "[0.89968717 0.8965683  0.89870981 0.89842312 0.90127486 0.90040311\n",
      " 0.90191923 0.90031155 0.90066781 0.89993996 0.89918949 0.90049786\n",
      " 0.89953564 0.90077554 0.89922578 0.89924002]\n",
      "[[1.11903389 0.8157894  1.01094628 1.00220852 1.1061668  1.04134882\n",
      "  1.16897884 1.03120837 1.14726194 1.06543079 0.98421704 1.06675568\n",
      "  0.98284873 1.07859186 1.04240347 1.04382716]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "\n",
      "[0.89968717 0.8965683  0.89870981 0.89842312 0.90127486 0.90040311\n",
      " 0.90191923 0.90031155 0.90066781 0.89993996 0.89918949 0.90049786\n",
      " 0.89953564 0.90077554 0.89922578 0.89924002]\n",
      "[[1.11903389 0.8157894  1.01094628 1.00220852 1.1061668  1.04134882\n",
      "  1.16897884 1.03120837 1.14726194 1.06543079 0.98421704 1.06675568\n",
      "  0.98284873 1.07859186 1.04240347 1.04382716]]\n",
      "{0: 1094, 1: 778}\n",
      "acc 0.6666666666666666\n",
      "acc 0.6666666666666666\n",
      "[[1051  581]\n",
      " [  43  197]]\n",
      "(0.2532133676092545, 0.8208333333333333, 0.38703339882121807, None)\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 16), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 16), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 16), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 16), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(16,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(16,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(16,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss -2565.017609458073\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11784139 0.81452396 1.00982897 1.00092989 1.10634082 1.04134571\n",
      "  1.16916184 1.03120815 1.14651534 1.06486335 0.98369221 1.06661405\n",
      "  0.98246879 1.07858875 1.04114761 1.04257126]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "1 loss -2565.017827362595\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11813515 0.81483954 1.01010406 1.00124266 1.10630174 1.04134561\n",
      "  1.16912116 1.0312077  1.14670085 1.06500286 0.9838222  1.06661405\n",
      "  0.98256589 1.07858864 1.04147993 1.04290357]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "2 loss -2565.0180453125695\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11842889 0.81515517 1.01037917 1.00155546 1.10626258 1.04134545\n",
      "  1.16908045 1.03120722 1.14688638 1.06514235 0.98395225 1.06661405\n",
      "  0.98266308 1.07858849 1.04181221 1.04323586]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "3 loss -2565.018263335188\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11872264 0.81547081 1.01065429 1.00186827 1.10622337 1.04134528\n",
      "  1.16903969 1.03120672 1.14707191 1.06528183 0.98408229 1.06661405\n",
      "  0.98276026 1.07858832 1.04214452 1.04356816]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "4 loss -2565.018481431943\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11901639 0.81578646 1.01092941 1.00218108 1.10618413 1.04134509\n",
      "  1.16899886 1.03120622 1.14725744 1.06542131 0.98421232 1.06661405\n",
      "  0.98285742 1.07858813 1.04247684 1.04390049]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n",
      "\n",
      "[1.00117545 0.99814215 1.0000956  1.00000624 1.00106378 1.00041339\n",
      " 1.00169206 1.00031208 1.00146335 1.00064723 0.99983571 1.00066614\n",
      " 0.99982382 1.00078582 1.00040812 1.00042236]\n",
      "[[1.11901639 0.81578646 1.01092941 1.00218108 1.10618413 1.04134509\n",
      "  1.16899886 1.03120622 1.14725744 1.06542131 0.98421232 1.06661405\n",
      "  0.98285742 1.07858813 1.04247684 1.04390049]]\n",
      "{0: 1108, 1: 764}\n",
      "acc 0.6741452991452992\n",
      "acc 0.6741452991452992\n",
      "[[1065  567]\n",
      " [  43  197]]\n",
      "(0.25785340314136124, 0.8208333333333333, 0.3924302788844621, None)\n"
     ]
    }
   ],
   "source": [
    "#16 LFs\n",
    "for i in np.linspace(0,1,11):\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.001/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                              pcl=np.array([-1,1],dtype=np.float64),norm=False,smooth=True,penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 5)\n",
      "(1872, 5)\n",
      "started at: 9-6-2018, 20:22:49\n",
      "trained in  0:00:06.133447\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "import _pickle as pkl\n",
    "# L_train = pkl.load(open(\"train_L_S_discrete.p\",\"rb\"))\n",
    "# L_train = sp.csr_matrix(L_train)\n",
    "\n",
    "# L_gold = pkl.load(open(\"gold_discrete.p\",\"rb\"))\n",
    "# print(np.array(L_gold).shape)\n",
    "# L_gold = sp.csr_matrix(L_gold)\n",
    "\n",
    "L_train = np.load(\"train_L_S_discrete.npy\")\n",
    "L_train = L_train[:,0,:].astype(int)\n",
    "print(np.array(L_train).shape)\n",
    "L_train = sp.csr_matrix(L_train)\n",
    "\n",
    "L_gold = np.load(\"test_L_S_discrete.npy\")\n",
    "L_gold = L_gold[:,0,:].astype(int)\n",
    "print(np.array(L_gold).shape)\n",
    "L_gold = sp.csr_matrix(L_gold)\n",
    "\n",
    "from snorkel.learning import GenerativeModel\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "gen_model.train(L_train, epochs = 100, cardinality=2)\n",
    "# gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)\n",
    "\n",
    "\n",
    "print(\"trained in \",str(datetime.timedelta(seconds=time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1872,)\n",
      "(1872,)\n",
      "(0.18799212598425197, 0.7958333333333333, 0.3041401273885351, None)\n"
     ]
    }
   ],
   "source": [
    "# 5 LFs\n",
    "import numpy as np\n",
    "dev_marginals = gen_model.marginals(L_gold)\n",
    "dev_marginals = np.array(dev_marginals)\n",
    "print(dev_marginals.shape)\n",
    "\n",
    "# GenLabels = np.argmax(dev_marginals,axis=1)\n",
    "GenLabels =  np.array([1 if m > 0.5 else 0 for m in dev_marginals])\n",
    "print(GenLabels.shape)\n",
    "\n",
    "print(precision_recall_fscore_support(np.array(true_labels),GenLabels,average=\"binary\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1872,)\n",
      "(1872,)\n",
      "(0.20664869721473494, 0.9583333333333334, 0.33998521803399856, None)\n"
     ]
    }
   ],
   "source": [
    "# 8 discrete LFs\n",
    "import numpy as np\n",
    "dev_marginals = gen_model.marginals(L_gold)\n",
    "dev_marginals = np.array(dev_marginals)\n",
    "print(dev_marginals.shape)\n",
    "\n",
    "# GenLabels = np.argmax(dev_marginals,axis=1)\n",
    "GenLabels =  np.array([1 if m > 0.5 else 0 for m in dev_marginals])\n",
    "print(GenLabels.shape)\n",
    "\n",
    "print(precision_recall_fscore_support(np.array(true_labels),GenLabels,average=\"binary\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
